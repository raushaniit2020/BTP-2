------------------------------------------------------------
I am inside utils/ops.py
------------------------------------------------------------
I am inside utils/knn_monitor.py
I am inside utils/__init__.py
I am inside utils/model_register.py
I am inside models/__init__.py
I am inside models/__init__.py
I am inside utils/grad_scaler.py
I am inside utils/loggerx.py
------------------------------------------------------------
I am inside models/basic_template.py
------------------------------------------------------------
object -->  <class 'object'>
I am inside network/resnet.py
I am inside network/preact_resnet.py
I am inside network/__init__.py
I am inside models/tcl/tcl_plus.py
I am inside models/tcl/tcl_wrapper.py
I am inside models/tcl/tcl.py
------------------------------------------------------------
I am inside models/tcl/data/create_noise.py
------------------------------------------------------------
I have imported everything required
I am inside __main__function
/content/drive/MyDrive/TCL-master/models/tcl/configs/cifar100_90_prer18.yml
<_io.TextIOWrapper name='/content/drive/MyDrive/TCL-master/models/tcl/configs/cifar100_90_prer18.yml' mode='r' encoding='UTF-8'>
configs -->  {'batch_size': 480, 'num_devices': 1, 'wandb': False, 'project_name': 'noise_label', 'entity': 'zzhuang', 'dataset': 'cifar100', 'resized_crop_scale': 0.2, 'label_file': '/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', 'encoder_name': 'bigresnet18_preact', 'epochs': 1, 'feat_dim': 256, 'img_size': 32, 'learning_rate': 0.03, 'learning_eta_min': 0.01, 'syncbn': True, 'reassign': 1, 'save_freq': 1, 'save_checkpoints': True, 'temp': 0.25, 'use_gaussian_blur': False, 'warmup_epochs': 20, 'weight_decay': 0.001, 'dist': False, 'num_workers': 32, 'model_name': 'tcl', 'sep_gmm': True, 'scale2': 1.0, 'mixup_alpha': 1.0}
480


<class 'models.tcl.tcl.TCL'>


ArgumentParser(prog='Default arguments for training of different methods', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='error', add_help=True)
Namespace(save_freq=50, test_freq=50, wandb=False, project_name='Clustering', entity='Hzzone', run_name=None, num_workers=16, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.0001, momentum=0.9, amp=False, encoder_name='bigresnet18', batch_size=256, epochs=1000, learning_rate=0.05, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=0, dist=False, num_devices=-1, whole_dataset=False, pin_memory=False, dataset='cifar10', data_folder='/content/dataset/', label_file=None, img_size=32, num_cluster=None, test_resized_crop=False, resized_crop_scale=0.08, model_name='supcon', use_gaussian_blur=False, save_checkpoints=False, feat_dim=2048, data_resample=False, reassign=1)
[]
Printing OPT
Namespace(save_freq=50, test_freq=50, wandb=False, project_name='Clustering', entity='Hzzone', run_name=None, num_workers=16, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.0001, momentum=0.9, amp=False, encoder_name='bigresnet18', batch_size=256, epochs=1000, learning_rate=0.05, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=0, dist=False, num_devices=-1, whole_dataset=False, pin_memory=False, dataset='cifar10', data_folder='/content/dataset/', label_file=None, img_size=32, num_cluster=None, test_resized_crop=False, resized_crop_scale=0.08, model_name='supcon', use_gaussian_blur=False, save_checkpoints=False, feat_dim=2048, data_resample=False, reassign=1, sep_gmm=False, temp=None, scale1=None, scale2=None, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr')


opt.run_name--> None
opt.run_name is None
osp.basename(config_path) ---> cifar100_90_prer18
opt.run_name again for logging--> 2024_05_02_19_49_41-cifar100_90_prer18
Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.03, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=None, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt.num_devices -->  1
opt.num_devices -->  1
opt.seed -->  0
torch.backends.cudnn.deterministic  False
self.opt -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.03, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=None, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
 verbose in is_root_worker -->  True
self.verbose -->  True
batch size -->  480
acc_grd_step -->  1
total batch size -->  480
get world size -->  <function get_world_size at 0x7e5663868b80>
learning rate -->  0.03
leaarning rate after update -->  0.056249999999999994
resume epoch -->  0
run name before update -->  2024_05_02_19_49_41-cifar100_90_prer18
run name after update -->  2024_05_02_19_49_41-cifar100_90_prer18
logger after update --> <utils.loggerx.LoggerX object at 0x7e563e13cc70>
calling set loader
opt in set loader --> Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=None, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt after update -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=None, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
set train transform... 
 <function TCL.train_transform.<locals>.ThreeCropTransform at 0x7e56636f9d80>
opt ---->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=None, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
data root ---->  /content/dataset/
data root ---->  /content/dataset/
dataset name ---------------->  cifar100
dataset type ---->  <class 'torchvision.datasets.cifar.CIFAR100'>
has_subfolder ---->  True
Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to /content/dataset/cifar-100-python.tar.gz
100% 169001437/169001437 [00:03<00:00, 47399185.99it/s]
Extracting /content/dataset/cifar-100-python.tar.gz to /content/dataset/
dataset ---->  50000
new labels ---->  50000
noise ratio ---->  0.89062
dataset targets after update ---->  50000
load label file from /content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy, possible noise ratio 0.89062
length of dataset ---->  50000
labels ---->  50000
with_indices ---->  True
dataset with indices ---->  50000
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
set train dataloader with 104 iterations...
set test transform... 
 Compose(
    <function TrainTask.test_transform.<locals>.resize at 0x7e563e10eb00>
    ToTensor()
    Normalize(mean=(0.5071, 0.4867, 0.4408), std=(0.2675, 0.2565, 0.2761))
)
opt ---->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=None, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
data root ---->  /content/dataset/
data root ---->  /content/dataset/
dataset name ---------------->  cifar100
dataset type ---->  <class 'torchvision.datasets.cifar.CIFAR100'>
has_subfolder ---->  True
Files already downloaded and verified
dataset ---->  10000
length of dataset ---->  10000
labels ---->  10000
with_indices ---->  False
set test dataloader with 21 iterations...
opt ---->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=None, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
data root ---->  /content/dataset/
data root ---->  /content/dataset/
dataset name ---------------->  cifar100
dataset type ---->  <class 'torchvision.datasets.cifar.CIFAR100'>
has_subfolder ---->  True
Files already downloaded and verified
dataset ---->  50000
new labels ---->  50000
noise ratio ---->  0.89062
dataset targets after update ---->  50000
load label file from /content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy, possible noise ratio 0.89062
length of dataset ---->  50000
labels ---->  50000
with_indices ---->  False
set memory dataloader with 105 iterations...
load 50000 images...
calling set models
Setting up model...
Encoder type: <function ResNet18 at 0x7e563e6d4ca0>
Dimension in: 512
Model setup kwargs: {'encoder_type': <function ResNet18 at 0x7e563e6d4ca0>, 'in_dim': 512, 'fea_dim': 256, 'T': 0.25, 'num_cluster': 100, 'mixup_alpha': 1.0, 'num_samples': 50000, 'scale1': None, 'scale2': 1.0}
Initializing SimCLRWrapper model...
Temperature (T): 0.25
Number of clusters: 100
Input dimension (in_dim): 512
Feature dimension (fea_dim): 256
Mixup alpha: 1.0
Scale 1: None
Scale 2: 1.0
Number of samples: 50000
Encoder: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (layer1): Sequential(
    (0): PreActBlock(
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (shortcut): Sequential()
    )
    (1): PreActBlock(
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (shortcut): Sequential()
    )
  )
  (layer2): Sequential(
    (0): PreActBlock(
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (shortcut): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
      )
    )
    (1): PreActBlock(
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (shortcut): Sequential()
    )
  )
  (layer3): Sequential(
    (0): PreActBlock(
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (shortcut): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
      )
    )
    (1): PreActBlock(
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (shortcut): Sequential()
    )
  )
  (layer4): Sequential(
    (0): PreActBlock(
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (shortcut): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
      )
    )
    (1): PreActBlock(
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (shortcut): Sequential()
    )
  )
  (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Projector (Q): Sequential(
  (0): Linear(in_features=512, out_features=512, bias=True)
  (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Linear(in_features=512, out_features=256, bias=True)
  (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Classifier (Q): Sequential(
  (0): Linear(in_features=512, out_features=512, bias=True)
  (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Dropout(p=0.25, inplace=False)
  (4): Linear(in_features=512, out_features=100, bias=True)
  (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Prototypes shape: torch.Size([100, 256])
Confidences shape: torch.Size([50000])
I am inside utils/infonce.py
Registered pseudo labels buffer on CPU.
Converting model to SyncBatchNorm...
Initialized optimizer with learning rate: 0.056249999999999994
Model setup completed.
amp----------->  False
scaler after update ---->  <utils.grad_scaler.NativeScalerWithGradNormCount object at 0x7e563e13e050>
logger after append -->  <utils.loggerx.LoggerX object at 0x7e563e13cc70>
model -->  <models.tcl.tcl.TCL object at 0x7e56bbf96dd0>
opt in fit -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
n_iter -->  1
current epoch -->  1
  1% 1/104 [00:00<?, ?it/s]progress bar initialized
Generating the psedo-labels
Obtained ground truth labels: 50000
Extracting features from the model...
Initialized features tensor shape: torch.Size([50000, 256])
Initialized all_labels tensor shape: torch.Size([50000])
Initialized cluster_labels tensor shape: torch.Size([50000, 100])

  0% 0/105 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.
  self.pid = os.fork()
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  1% 1/105 [00:10<18:05, 10.44s/it]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  3% 3/105 [00:10<04:40,  2.75s/it]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  5% 5/105 [00:10<02:19,  1.39s/it]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  7% 7/105 [00:10<01:22,  1.19it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  9% 9/105 [00:11<00:53,  1.78it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 10% 11/105 [00:11<00:37,  2.47it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 12% 13/105 [00:11<00:27,  3.32it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 14% 15/105 [00:11<00:21,  4.24it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 16% 17/105 [00:11<00:17,  5.12it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 18% 19/105 [00:12<00:14,  6.06it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 20% 21/105 [00:12<00:12,  6.81it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 21% 22/105 [00:12<00:11,  7.13it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 23% 24/105 [00:12<00:10,  7.79it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 24% 25/105 [00:12<00:09,  8.06it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 26% 27/105 [00:13<00:08,  8.87it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 27% 28/105 [00:13<00:08,  8.86it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 28% 29/105 [00:13<00:08,  9.00it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 29% 30/105 [00:13<00:08,  9.14it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 30% 31/105 [00:13<00:08,  8.95it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 31% 33/105 [00:13<00:09,  7.36it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 32% 34/105 [00:13<00:09,  7.44it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 34% 36/105 [00:14<00:08,  8.36it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 35% 37/105 [00:14<00:08,  8.32it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 36% 38/105 [00:14<00:07,  8.45it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 37% 39/105 [00:14<00:07,  8.45it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 38% 40/105 [00:14<00:07,  8.14it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 39% 41/105 [00:14<00:07,  8.37it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 40% 42/105 [00:14<00:07,  8.54it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 41% 43/105 [00:14<00:07,  8.34it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 42% 44/105 [00:15<00:06,  8.72it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 43% 45/105 [00:15<00:06,  8.62it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 44% 46/105 [00:15<00:06,  8.58it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 46% 48/105 [00:15<00:05,  9.59it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 47% 49/105 [00:15<00:06,  9.26it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 49% 51/105 [00:15<00:05,  9.96it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 50% 53/105 [00:15<00:05, 10.07it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 52% 55/105 [00:16<00:04, 10.07it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 54% 57/105 [00:16<00:04, 10.11it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 56% 59/105 [00:16<00:04, 10.09it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 58% 61/105 [00:16<00:04, 10.07it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 60% 63/105 [00:16<00:04, 10.15it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 62% 65/105 [00:17<00:03, 10.14it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 64% 67/105 [00:17<00:03, 10.14it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 66% 69/105 [00:17<00:03, 10.13it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 68% 71/105 [00:17<00:03, 10.14it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 70% 73/105 [00:17<00:03, 10.19it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 71% 75/105 [00:18<00:02, 10.18it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 73% 77/105 [00:18<00:02, 10.21it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 75% 79/105 [00:18<00:02, 10.19it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 77% 81/105 [00:18<00:02, 10.20it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 79% 83/105 [00:18<00:02, 10.20it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 81% 85/105 [00:19<00:01, 10.20it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 83% 87/105 [00:19<00:01, 10.18it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 85% 89/105 [00:19<00:01, 10.18it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 87% 91/105 [00:19<00:01, 10.13it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 89% 93/105 [00:19<00:01, 10.17it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 90% 95/105 [00:20<00:00, 10.18it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 92% 97/105 [00:20<00:00, 10.17it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 94% 99/105 [00:20<00:00, 10.14it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 96% 101/105 [00:20<00:00, 10.15it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 98% 103/105 [00:20<00:00, 10.16it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([80, 3, 32, 32]) labels shape: torch.Size([80])
Encoder output shape: torch.Size([80, 512])
Local cluster labels shape: torch.Size([80, 100])
Local features shape: torch.Size([80, 256])

100% 105/105 [00:21<00:00,  4.93it/s]
Updated features tensor shape after indexing: torch.Size([50000, 256])
Updated all_labels tensor shape after indexing: torch.Size([50000])
Updated cluster_labels tensor shape after indexing: torch.Size([50000, 100])
Final labels tensor shape: torch.Size([50000])
Feature extraction completed.
Extracted features and cluster labels from the memory loader.
Parameters of the config file: Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
Computed cluster centers after matrix multiplication: torch.Size([100, 256])
Computed confidence scores: torch.Size([50000])
Computed confidence, context assignments, and centers for noise detection.
Copied centers and context assignments to the model.
Obtained confidence, context assignments, features, and cluster labels from correct_labels function.
Copied confidence values to the tcl model.
data root ---->  /content/dataset/
dataset name ---------------->  cifar100
dataset type ---->  <class 'torchvision.datasets.cifar.CIFAR100'>
has_subfolder ---->  True
Files already downloaded and verified
dataset ---->  50000
Clean labels from the training dataset: 50000
Check if each label is clean: [False False False ... False False False]
save 0000001-0-context_assignments_hist.png to ./ckpt/2024_05_02_19_49_41-cifar100_90_prer18/save_images
Histogram of context assignments computed.
Training accuracy: tensor(0.0100, device='cuda:0')
Extracting features from the model...
Initialized features tensor shape: torch.Size([10000, 256])
Initialized all_labels tensor shape: torch.Size([10000])
Initialized cluster_labels tensor shape: torch.Size([10000, 100])

  0% 0/21 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.
  self.pid = os.fork()
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  5% 1/21 [00:02<00:58,  2.91s/it]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 10% 2/21 [00:03<00:30,  1.61s/it]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 14% 3/21 [00:03<00:17,  1.05it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 19% 4/21 [00:03<00:10,  1.62it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 29% 6/21 [00:04<00:05,  2.92it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 38% 8/21 [00:04<00:03,  4.22it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 48% 10/21 [00:04<00:02,  5.41it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 57% 12/21 [00:04<00:01,  6.49it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 67% 14/21 [00:04<00:00,  7.35it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 76% 16/21 [00:05<00:00,  8.07it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 86% 18/21 [00:05<00:00,  8.63it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 90% 19/21 [00:05<00:00,  8.84it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([400, 3, 32, 32]) labels shape: torch.Size([400])
Encoder output shape: torch.Size([400, 512])
Local cluster labels shape: torch.Size([400, 100])
Local features shape: torch.Size([400, 256])

100% 21/21 [00:06<00:00,  3.13it/s]
Updated features tensor shape after indexing: torch.Size([10000, 256])
Updated all_labels tensor shape after indexing: torch.Size([10000])
Updated cluster_labels tensor shape after indexing: torch.Size([10000, 100])
Final labels tensor shape: torch.Size([10000])
Feature extraction completed.
Extracted features, test cluster labels, and test labels.
Test accuracy: tensor(0.0100, device='cuda:0')
KNN labels predicted for the test features: tensor([12, 80, 30,  ..., 63, 66, 45], device='cuda:0')
(tensor([ 9, 77, 94], device='cuda:0'), tensor([   1,    4, 9995], device='cuda:0'))
KNN accuracy: tensor(0.1164, device='cuda:0')
Estimated noise ratio: 0.4764999747276306
Updated scale1 of tcl model: 0.4764999747276306
Noise accuracy: tensor(0.5162, device='cuda:0')
Context noise AUC: 0.4909500712529655
[2024-05-02 19:50:23] 00001, estimated_noise_ratio 0.47650, noise_accuracy 0.51624, context_noise_auc 0.49095, train_acc 0.01000, test_acc 0.01000, knn_acc 0.11640, 
Evaluated the tcl model using obtained features, confidence, cluster labels, and context assignments.
Before training starts
inside while loop
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.
  self.pid = os.fork()
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.009615384615384616
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  2.704326923076923e-05
learning rate from cosine_annealing_LR -->  2.704326923076923e-05
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:51:04] 00001, lr 0.00003, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 483,  782,  686,  ...,  416,  282, 1332], device='cuda:0')
Generated mixing coefficient (lambda): 0.44912526447830964
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 10.138054847717285
Classification loss 2 during warmup: 4.812106609344482
Alignment loss during warmup: 4.605175018310547
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.359892845153809
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5969133377075195
Contrastive Loss: tensor(6.9310, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(10.1381, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8121, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3599, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5969, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.9310, device='cuda:0', grad_fn=<DivBackward0>), tensor(10.1381, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8121, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3599, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5969, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.9309916496276855 cls_loss1: 10.138054847717285 cls_loss2: 4.812106609344482 ent_loss: 4.359892845153809 ne_loss: -4.5969133377075195 align_loss: 4.605175018310547
Total loss: 26.249309539794922

[------------------------------------------------------------
I am inside utils/ops.py
------------------------------------------------------------
I am inside utils/knn_monitor.py
I am inside utils/__init__.py
I am inside utils/model_register.py
I am inside models/__init__.py
I am inside models/__init__.py
I am inside utils/grad_scaler.py
I am inside utils/loggerx.py
------------------------------------------------------------
I am inside models/basic_template.py
------------------------------------------------------------
object -->  <class 'object'>
I am inside network/resnet.py
I am inside network/preact_resnet.py
I am inside network/__init__.py
I am inside models/tcl/tcl_plus.py
I am inside models/tcl/tcl_wrapper.py
I am inside models/tcl/tcl.py
------------------------------------------------------------
I am inside models/tcl/data/create_noise.py
------------------------------------------------------------
I have imported everything required
I am inside __main__function
/content/drive/MyDrive/TCL-master/models/tcl/configs/cifar100_90_prer18.yml
<_io.TextIOWrapper name='/content/drive/MyDrive/TCL-master/models/tcl/configs/cifar100_90_prer18.yml' mode='r' encoding='UTF-8'>
configs -->  {'batch_size': 480, 'num_devices': 1, 'wandb': False, 'project_name': 'noise_label', 'entity': 'zzhuang', 'dataset': 'cifar100', 'resized_crop_scale': 0.2, 'label_file': '/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', 'encoder_name': 'bigresnet18_preact', 'epochs': 1, 'feat_dim': 256, 'img_size': 32, 'learning_rate': 0.03, 'learning_eta_min': 0.01, 'syncbn': True, 'reassign': 1, 'save_freq': 1, 'save_checkpoints': True, 'temp': 0.25, 'use_gaussian_blur': False, 'warmup_epochs': 20, 'weight_decay': 0.001, 'dist': False, 'num_workers': 32, 'model_name': 'tcl', 'sep_gmm': True, 'scale2': 1.0, 'mixup_alpha': 1.0}
480


<class 'models.tcl.tcl.TCL'>


ArgumentParser(prog='Default arguments for training of different methods', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='error', add_help=True)
Namespace(save_freq=50, test_freq=50, wandb=False, project_name='Clustering', entity='Hzzone', run_name=None, num_workers=16, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.0001, momentum=0.9, amp=False, encoder_name='bigresnet18', batch_size=256, epochs=1000, learning_rate=0.05, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=0, dist=False, num_devices=-1, whole_dataset=False, pin_memory=False, dataset='cifar10', data_folder='/content/dataset/', label_file=None, img_size=32, num_cluster=None, test_resized_crop=False, resized_crop_scale=0.08, model_name='supcon', use_gaussian_blur=False, save_checkpoints=False, feat_dim=2048, data_resample=False, reassign=1)
[]
Printing OPT
Namespace(save_freq=50, test_freq=50, wandb=False, project_name='Clustering', entity='Hzzone', run_name=None, num_workers=16, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.0001, momentum=0.9, amp=False, encoder_name='bigresnet18', batch_size=256, epochs=1000, learning_rate=0.05, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=0, dist=False, num_devices=-1, whole_dataset=False, pin_memory=False, dataset='cifar10', data_folder='/content/dataset/', label_file=None, img_size=32, num_cluster=None, test_resized_crop=False, resized_crop_scale=0.08, model_name='supcon', use_gaussian_blur=False, save_checkpoints=False, feat_dim=2048, data_resample=False, reassign=1, sep_gmm=False, temp=None, scale1=None, scale2=None, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr')


opt.run_name--> None
opt.run_name is None
osp.basename(config_path) ---> cifar100_90_prer18
opt.run_name again for logging--> 2024_05_02_19_49_41-cifar100_90_prer18
Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.03, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=None, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt.num_devices -->  1
opt.num_devices -->  1
opt.seed -->  0
torch.backends.cudnn.deterministic  False
self.opt -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.03, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=None, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
 verbose in is_root_worker -->  True
self.verbose -->  True
batch size -->  480
acc_grd_step -->  1
total batch size -->  480
get world size -->  <function get_world_size at 0x7e5663868b80>
learning rate -->  0.03
leaarning rate after update -->  0.056249999999999994
resume epoch -->  0
run name before update -->  2024_05_02_19_49_41-cifar100_90_prer18
run name after update -->  2024_05_02_19_49_41-cifar100_90_prer18
logger after update --> <utils.loggerx.LoggerX object at 0x7e563e13cc70>
calling set loader
opt in set loader --> Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=None, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt after update -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=None, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
set train transform... 
 <function TCL.train_transform.<locals>.ThreeCropTransform at 0x7e56636f9d80>
opt ---->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=None, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
data root ---->  /content/dataset/
data root ---->  /content/dataset/
dataset name ---------------->  cifar100
dataset type ---->  <class 'torchvision.datasets.cifar.CIFAR100'>
has_subfolder ---->  True
Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to /content/dataset/cifar-100-python.tar.gz
100% 169001437/169001437 [00:03<00:00, 47399185.99it/s]
Extracting /content/dataset/cifar-100-python.tar.gz to /content/dataset/
dataset ---->  50000
new labels ---->  50000
noise ratio ---->  0.89062
dataset targets after update ---->  50000
load label file from /content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy, possible noise ratio 0.89062
length of dataset ---->  50000
labels ---->  50000
with_indices ---->  True
dataset with indices ---->  50000
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
set train dataloader with 104 iterations...
set test transform... 
 Compose(
    <function TrainTask.test_transform.<locals>.resize at 0x7e563e10eb00>
    ToTensor()
    Normalize(mean=(0.5071, 0.4867, 0.4408), std=(0.2675, 0.2565, 0.2761))
)
opt ---->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=None, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
data root ---->  /content/dataset/
data root ---->  /content/dataset/
dataset name ---------------->  cifar100
dataset type ---->  <class 'torchvision.datasets.cifar.CIFAR100'>
has_subfolder ---->  True
Files already downloaded and verified
dataset ---->  10000
length of dataset ---->  10000
labels ---->  10000
with_indices ---->  False
set test dataloader with 21 iterations...
opt ---->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=None, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
data root ---->  /content/dataset/
data root ---->  /content/dataset/
dataset name ---------------->  cifar100
dataset type ---->  <class 'torchvision.datasets.cifar.CIFAR100'>
has_subfolder ---->  True
Files already downloaded and verified
dataset ---->  50000
new labels ---->  50000
noise ratio ---->  0.89062
dataset targets after update ---->  50000
load label file from /content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy, possible noise ratio 0.89062
length of dataset ---->  50000
labels ---->  50000
with_indices ---->  False
set memory dataloader with 105 iterations...
load 50000 images...
calling set models
Setting up model...
Encoder type: <function ResNet18 at 0x7e563e6d4ca0>
Dimension in: 512
Model setup kwargs: {'encoder_type': <function ResNet18 at 0x7e563e6d4ca0>, 'in_dim': 512, 'fea_dim': 256, 'T': 0.25, 'num_cluster': 100, 'mixup_alpha': 1.0, 'num_samples': 50000, 'scale1': None, 'scale2': 1.0}
Initializing SimCLRWrapper model...
Temperature (T): 0.25
Number of clusters: 100
Input dimension (in_dim): 512
Feature dimension (fea_dim): 256
Mixup alpha: 1.0
Scale 1: None
Scale 2: 1.0
Number of samples: 50000
Encoder: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (layer1): Sequential(
    (0): PreActBlock(
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (shortcut): Sequential()
    )
    (1): PreActBlock(
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (shortcut): Sequential()
    )
  )
  (layer2): Sequential(
    (0): PreActBlock(
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (shortcut): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
      )
    )
    (1): PreActBlock(
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (shortcut): Sequential()
    )
  )
  (layer3): Sequential(
    (0): PreActBlock(
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (shortcut): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
      )
    )
    (1): PreActBlock(
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (shortcut): Sequential()
    )
  )
  (layer4): Sequential(
    (0): PreActBlock(
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (shortcut): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
      )
    )
    (1): PreActBlock(
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (shortcut): Sequential()
    )
  )
  (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Projector (Q): Sequential(
  (0): Linear(in_features=512, out_features=512, bias=True)
  (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Linear(in_features=512, out_features=256, bias=True)
  (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Classifier (Q): Sequential(
  (0): Linear(in_features=512, out_features=512, bias=True)
  (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Dropout(p=0.25, inplace=False)
  (4): Linear(in_features=512, out_features=100, bias=True)
  (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Prototypes shape: torch.Size([100, 256])
Confidences shape: torch.Size([50000])
I am inside utils/infonce.py
Registered pseudo labels buffer on CPU.
Converting model to SyncBatchNorm...
Initialized optimizer with learning rate: 0.056249999999999994
Model setup completed.
amp----------->  False
scaler after update ---->  <utils.grad_scaler.NativeScalerWithGradNormCount object at 0x7e563e13e050>
logger after append -->  <utils.loggerx.LoggerX object at 0x7e563e13cc70>
model -->  <models.tcl.tcl.TCL object at 0x7e56bbf96dd0>
opt in fit -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
n_iter -->  1
current epoch -->  1
  1% 1/104 [00:00<?, ?it/s]progress bar initialized
Generating the psedo-labels
Obtained ground truth labels: 50000
Extracting features from the model...
Initialized features tensor shape: torch.Size([50000, 256])
Initialized all_labels tensor shape: torch.Size([50000])
Initialized cluster_labels tensor shape: torch.Size([50000, 100])

  0% 0/105 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.
  self.pid = os.fork()
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  1% 1/105 [00:10<18:05, 10.44s/it]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  3% 3/105 [00:10<04:40,  2.75s/it]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  5% 5/105 [00:10<02:19,  1.39s/it]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  7% 7/105 [00:10<01:22,  1.19it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  9% 9/105 [00:11<00:53,  1.78it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 10% 11/105 [00:11<00:37,  2.47it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 12% 13/105 [00:11<00:27,  3.32it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 14% 15/105 [00:11<00:21,  4.24it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 16% 17/105 [00:11<00:17,  5.12it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 18% 19/105 [00:12<00:14,  6.06it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 20% 21/105 [00:12<00:12,  6.81it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 21% 22/105 [00:12<00:11,  7.13it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 23% 24/105 [00:12<00:10,  7.79it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 24% 25/105 [00:12<00:09,  8.06it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 26% 27/105 [00:13<00:08,  8.87it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 27% 28/105 [00:13<00:08,  8.86it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 28% 29/105 [00:13<00:08,  9.00it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 29% 30/105 [00:13<00:08,  9.14it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 30% 31/105 [00:13<00:08,  8.95it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 31% 33/105 [00:13<00:09,  7.36it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 32% 34/105 [00:13<00:09,  7.44it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 34% 36/105 [00:14<00:08,  8.36it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 35% 37/105 [00:14<00:08,  8.32it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 36% 38/105 [00:14<00:07,  8.45it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 37% 39/105 [00:14<00:07,  8.45it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 38% 40/105 [00:14<00:07,  8.14it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 39% 41/105 [00:14<00:07,  8.37it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 40% 42/105 [00:14<00:07,  8.54it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 41% 43/105 [00:14<00:07,  8.34it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 42% 44/105 [00:15<00:06,  8.72it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 43% 45/105 [00:15<00:06,  8.62it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 44% 46/105 [00:15<00:06,  8.58it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 46% 48/105 [00:15<00:05,  9.59it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 47% 49/105 [00:15<00:06,  9.26it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 49% 51/105 [00:15<00:05,  9.96it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 50% 53/105 [00:15<00:05, 10.07it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 52% 55/105 [00:16<00:04, 10.07it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 54% 57/105 [00:16<00:04, 10.11it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 56% 59/105 [00:16<00:04, 10.09it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 58% 61/105 [00:16<00:04, 10.07it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 60% 63/105 [00:16<00:04, 10.15it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 62% 65/105 [00:17<00:03, 10.14it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 64% 67/105 [00:17<00:03, 10.14it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 66% 69/105 [00:17<00:03, 10.13it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 68% 71/105 [00:17<00:03, 10.14it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 70% 73/105 [00:17<00:03, 10.19it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 71% 75/105 [00:18<00:02, 10.18it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 73% 77/105 [00:18<00:02, 10.21it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 75% 79/105 [00:18<00:02, 10.19it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 77% 81/105 [00:18<00:02, 10.20it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 79% 83/105 [00:18<00:02, 10.20it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 81% 85/105 [00:19<00:01, 10.20it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 83% 87/105 [00:19<00:01, 10.18it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 85% 89/105 [00:19<00:01, 10.18it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 87% 91/105 [00:19<00:01, 10.13it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 89% 93/105 [00:19<00:01, 10.17it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 90% 95/105 [00:20<00:00, 10.18it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 92% 97/105 [00:20<00:00, 10.17it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 94% 99/105 [00:20<00:00, 10.14it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 96% 101/105 [00:20<00:00, 10.15it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 98% 103/105 [00:20<00:00, 10.16it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([80, 3, 32, 32]) labels shape: torch.Size([80])
Encoder output shape: torch.Size([80, 512])
Local cluster labels shape: torch.Size([80, 100])
Local features shape: torch.Size([80, 256])

100% 105/105 [00:21<00:00,  4.93it/s]
Updated features tensor shape after indexing: torch.Size([50000, 256])
Updated all_labels tensor shape after indexing: torch.Size([50000])
Updated cluster_labels tensor shape after indexing: torch.Size([50000, 100])
Final labels tensor shape: torch.Size([50000])
Feature extraction completed.
Extracted features and cluster labels from the memory loader.
Parameters of the config file: Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
Computed cluster centers after matrix multiplication: torch.Size([100, 256])
Computed confidence scores: torch.Size([50000])
Computed confidence, context assignments, and centers for noise detection.
Copied centers and context assignments to the model.
Obtained confidence, context assignments, features, and cluster labels from correct_labels function.
Copied confidence values to the tcl model.
data root ---->  /content/dataset/
dataset name ---------------->  cifar100
dataset type ---->  <class 'torchvision.datasets.cifar.CIFAR100'>
has_subfolder ---->  True
Files already downloaded and verified
dataset ---->  50000
Clean labels from the training dataset: 50000
Check if each label is clean: [False False False ... False False False]
save 0000001-0-context_assignments_hist.png to ./ckpt/2024_05_02_19_49_41-cifar100_90_prer18/save_images
Histogram of context assignments computed.
Training accuracy: tensor(0.0100, device='cuda:0')
Extracting features from the model...
Initialized features tensor shape: torch.Size([10000, 256])
Initialized all_labels tensor shape: torch.Size([10000])
Initialized cluster_labels tensor shape: torch.Size([10000, 100])

  0% 0/21 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.
  self.pid = os.fork()
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  5% 1/21 [00:02<00:58,  2.91s/it]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 10% 2/21 [00:03<00:30,  1.61s/it]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 14% 3/21 [00:03<00:17,  1.05it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 19% 4/21 [00:03<00:10,  1.62it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 29% 6/21 [00:04<00:05,  2.92it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 38% 8/21 [00:04<00:03,  4.22it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 48% 10/21 [00:04<00:02,  5.41it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 57% 12/21 [00:04<00:01,  6.49it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 67% 14/21 [00:04<00:00,  7.35it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 76% 16/21 [00:05<00:00,  8.07it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 86% 18/21 [00:05<00:00,  8.63it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 90% 19/21 [00:05<00:00,  8.84it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([400, 3, 32, 32]) labels shape: torch.Size([400])
Encoder output shape: torch.Size([400, 512])
Local cluster labels shape: torch.Size([400, 100])
Local features shape: torch.Size([400, 256])

100% 21/21 [00:06<00:00,  3.13it/s]
Updated features tensor shape after indexing: torch.Size([10000, 256])
Updated all_labels tensor shape after indexing: torch.Size([10000])
Updated cluster_labels tensor shape after indexing: torch.Size([10000, 100])
Final labels tensor shape: torch.Size([10000])
Feature extraction completed.
Extracted features, test cluster labels, and test labels.
Test accuracy: tensor(0.0100, device='cuda:0')
KNN labels predicted for the test features: tensor([12, 80, 30,  ..., 63, 66, 45], device='cuda:0')
(tensor([ 9, 77, 94], device='cuda:0'), tensor([   1,    4, 9995], device='cuda:0'))
KNN accuracy: tensor(0.1164, device='cuda:0')
Estimated noise ratio: 0.4764999747276306
Updated scale1 of tcl model: 0.4764999747276306
Noise accuracy: tensor(0.5162, device='cuda:0')
Context noise AUC: 0.4909500712529655
[2024-05-02 19:50:23] 00001, estimated_noise_ratio 0.47650, noise_accuracy 0.51624, context_noise_auc 0.49095, train_acc 0.01000, test_acc 0.01000, knn_acc 0.11640, 
Evaluated the tcl model using obtained features, confidence, cluster labels, and context assignments.
Before training starts
inside while loop
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.
  self.pid = os.fork()
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.009615384615384616
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  2.704326923076923e-05
learning rate from cosine_annealing_LR -->  2.704326923076923e-05
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:51:04] 00001, lr 0.00003, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 483,  782,  686,  ...,  416,  282, 1332], device='cuda:0')
Generated mixing coefficient (lambda): 0.44912526447830964
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 10.138054847717285
Classification loss 2 during warmup: 4.812106609344482
Alignment loss during warmup: 4.605175018310547
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.359892845153809
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5969133377075195
Contrastive Loss: tensor(6.9310, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(10.1381, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8121, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3599, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5969, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.9310, device='cuda:0', grad_fn=<DivBackward0>), tensor(10.1381, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8121, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3599, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5969, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.9309916496276855 cls_loss1: 10.138054847717285 cls_loss2: 4.812106609344482 ent_loss: 4.359892845153809 ne_loss: -4.5969133377075195 align_loss: 4.605175018310547
Total loss: 26.249309539794922
[2024-05-02 19:52:59] 00001, contrastive_loss 6.93099, cls_loss1 10.13805, cls_loss2 4.81211, ent_loss 4.35989, ne_loss -4.59691, align_loss 4.60518, 
Training completed.
training step completed
  2% 2/104 [03:07<5:19:08, 187.73s/it]iteration number -->  2
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.019230769230769232
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  5.408653846153846e-05
learning rate from cosine_annealing_LR -->  5.408653846153846e-05
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:52:59] 00002, lr 0.00005, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1083,  999,  924,  ...,  743,  411,  971], device='cuda:0')
Generated mixing coefficient (lambda): 0.023707189032098733
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.823692321777344
Classification loss 2 during warmup: 5.04218053817749
Alignment loss during warmup: 4.605167865753174
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.280387878417969
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.600931167602539
Contrastive Loss: tensor(6.9424, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.8237, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(5.0422, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.2804, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.6009, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.9424, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.8237, device='cuda:0', grad_fn=<AddBackward0>), tensor(5.0422, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.2804, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.6009, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.942377090454102 cls_loss1: 9.823692321777344 cls_loss2: 5.04218053817749 ent_loss: 4.280387878417969 ne_loss: -4.600931167602539 align_loss: 4.605167865753174
Total loss: 26.09287452697754
[2024-05-02 19:53:14] 00002, contrastive_loss 6.94238, cls_loss1 9.82369, cls_loss2 5.04218, ent_loss 4.28039, ne_loss -4.60093, align_loss 4.60517, 
Training completed.
training step completed
  3% 3/104 [03:23<2:25:38, 86.52s/it] iteration number -->  3
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.028846153846153848
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  8.112980769230769e-05
learning rate from cosine_annealing_LR -->  8.112980769230769e-05
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:14] 00003, lr 0.00008, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([  56,  425, 1161,  ...,  821,  279,  284], device='cuda:0')
Generated mixing coefficient (lambda): 0.15599464004468885
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 10.150880813598633
Classification loss 2 during warmup: 4.977916717529297
Alignment loss during warmup: 4.605169773101807
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.321966171264648
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.599339962005615
Contrastive Loss: tensor(6.9610, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(10.1509, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9779, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3220, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5993, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.9610, device='cuda:0', grad_fn=<DivBackward0>), tensor(10.1509, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9779, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3220, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5993, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.9610114097595215 cls_loss1: 10.150880813598633 cls_loss2: 4.977916717529297 ent_loss: 4.321966171264648 ne_loss: -4.599339962005615 align_loss: 4.605169773101807
Total loss: 26.417604446411133
[2024-05-02 19:53:15] 00003, contrastive_loss 6.96101, cls_loss1 10.15088, cls_loss2 4.97792, ent_loss 4.32197, ne_loss -4.59934, align_loss 4.60517, 
Training completed.
training step completed
  4% 4/104 [03:26<1:20:20, 48.20s/it]iteration number -->  4
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.038461538461538464
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.00010817307692307693
learning rate from cosine_annealing_LR -->  0.00010817307692307693
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:17] 00004, lr 0.00011, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1315,  898,  838,  ...,  818, 1361, 1377], device='cuda:0')
Generated mixing coefficient (lambda): 0.5572264854633476
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 10.11421012878418
Classification loss 2 during warmup: 4.794486045837402
Alignment loss during warmup: 4.605165004730225
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.354094982147217
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596653938293457
Contrastive Loss: tensor(6.9310, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(10.1142, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7945, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3541, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5967, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.9310, device='cuda:0', grad_fn=<DivBackward0>), tensor(10.1142, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7945, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3541, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5967, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.9310197830200195 cls_loss1: 10.11421012878418 cls_loss2: 4.794486045837402 ent_loss: 4.354094982147217 ne_loss: -4.596653938293457 align_loss: 4.605165004730225
Total loss: 26.202322006225586
[2024-05-02 19:53:18] 00004, contrastive_loss 6.93102, cls_loss1 10.11421, cls_loss2 4.79449, ent_loss 4.35409, ne_loss -4.59665, align_loss 4.60517, 
Training completed.
training step completed
  5% 5/104 [03:28<49:49, 30.19s/it]  iteration number -->  5
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.04807692307692308
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.00013521634615384616
learning rate from cosine_annealing_LR -->  0.00013521634615384616
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:20] 00005, lr 0.00014, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 260,  589,   77,  ...,  816,  832, 1082], device='cuda:0')
Generated mixing coefficient (lambda): 0.029523968500001368
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.837117195129395
Classification loss 2 during warmup: 4.9652791023254395
Alignment loss during warmup: 4.605168342590332
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.300097942352295
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.600799560546875
Contrastive Loss: tensor(6.9927, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.8371, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9653, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3001, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.6008, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.9927, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.8371, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9653, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3001, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.6008, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.992705345153809 cls_loss1: 9.837117195129395 cls_loss2: 4.9652791023254395 ent_loss: 4.300097942352295 ne_loss: -4.600799560546875 align_loss: 4.605168342590332
Total loss: 26.099567413330078
[2024-05-02 19:53:20] 00005, contrastive_loss 6.99271, cls_loss1 9.83712, cls_loss2 4.96528, ent_loss 4.30010, ne_loss -4.60080, align_loss 4.60517, 
Training completed.
training step completed
  6% 6/104 [03:31<33:02, 20.23s/it]iteration number -->  6
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.057692307692307696
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.00016225961538461538
learning rate from cosine_annealing_LR -->  0.00016225961538461538
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:22] 00006, lr 0.00016, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1315,  266,  515,  ..., 1255, 1292,  495], device='cuda:0')
Generated mixing coefficient (lambda): 0.45133699035902825
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 10.146890640258789
Classification loss 2 during warmup: 4.8062567710876465
Alignment loss during warmup: 4.605167865753174
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.362261772155762
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596324443817139
Contrastive Loss: tensor(6.9608, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(10.1469, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8063, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3623, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5963, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.9608, device='cuda:0', grad_fn=<DivBackward0>), tensor(10.1469, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8063, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3623, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5963, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.960809230804443 cls_loss1: 10.146890640258789 cls_loss2: 4.8062567710876465 ent_loss: 4.362261772155762 ne_loss: -4.596324443817139 align_loss: 4.605167865753174
Total loss: 26.28506088256836
[2024-05-02 19:53:23] 00006, contrastive_loss 6.96081, cls_loss1 10.14689, cls_loss2 4.80626, ent_loss 4.36226, ne_loss -4.59632, align_loss 4.60517, 
Training completed.
training step completed
  7% 7/104 [03:33<22:57, 14.20s/it]iteration number -->  7
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.0673076923076923
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0001893028846153846
learning rate from cosine_annealing_LR -->  0.0001893028846153846
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:25] 00007, lr 0.00019, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([  30,  357, 1060,  ...,  674,  273, 1427], device='cuda:0')
Generated mixing coefficient (lambda): 0.9205318443348298
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.866230010986328
Classification loss 2 during warmup: 4.988160610198975
Alignment loss during warmup: 4.605161666870117
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.291213512420654
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.600477695465088
Contrastive Loss: tensor(6.9676, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.8662, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9882, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.2912, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.6005, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.9676, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.8662, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9882, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.2912, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.6005, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.967554092407227 cls_loss1: 9.866230010986328 cls_loss2: 4.988160610198975 ent_loss: 4.291213512420654 ne_loss: -4.600477695465088 align_loss: 4.605161666870117
Total loss: 26.117843627929688
[2024-05-02 19:53:25] 00007, contrastive_loss 6.96755, cls_loss1 9.86623, cls_loss2 4.98816, ent_loss 4.29121, ne_loss -4.60048, align_loss 4.60516, 
Training completed.
training step completed
  8% 8/104 [03:36<16:36, 10.38s/it]iteration number -->  8
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.07692307692307693
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.00021634615384615385
learning rate from cosine_annealing_LR -->  0.00021634615384615385
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:27] 00008, lr 0.00022, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 327,  838, 1306,  ...,  559,   20,  569], device='cuda:0')
Generated mixing coefficient (lambda): 0.6200326211997292
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.87826156616211
Classification loss 2 during warmup: 4.766782760620117
Alignment loss during warmup: 4.605171203613281
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.3725152015686035
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596757411956787
Contrastive Loss: tensor(6.9829, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.8783, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7668, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3725, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5968, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.9829, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.8783, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7668, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3725, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5968, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.982922554016113 cls_loss1: 9.87826156616211 cls_loss2: 4.766782760620117 ent_loss: 4.3725152015686035 ne_loss: -4.596757411956787 align_loss: 4.605171203613281
Total loss: 26.008895874023438

[2024-05-02 19:53:28] 00008, contrastive_loss 6.98292, cls_loss1 9.87826, cls_loss2 4.76678, ent_loss 4.37252, ne_loss -4.59676, align_loss 4.60517, 
Training completed.
training step completed
  9% 9/104 [03:38<12:27,  7.87s/it]iteration number -->  9
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.08653846153846154
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.00024338942307692305
learning rate from cosine_annealing_LR -->  0.00024338942307692305
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:30] 00009, lr 0.00024, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1057,  629,  201,  ..., 1391,  579,  464], device='cuda:0')
Generated mixing coefficient (lambda): 0.46445321844681775
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 10.012557983398438
Classification loss 2 during warmup: 4.77290153503418
Alignment loss during warmup: 4.605165958404541
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.361077785491943
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596325874328613
Contrastive Loss: tensor(6.8184, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(10.0126, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7729, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3611, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5963, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.8184, device='cuda:0', grad_fn=<DivBackward0>), tensor(10.0126, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7729, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3611, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5963, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.818404674530029 cls_loss1: 10.012557983398438 cls_loss2: 4.77290153503418 ent_loss: 4.361077785491943 ne_loss: -4.596325874328613 align_loss: 4.605165958404541
Total loss: 25.973783493041992
[2024-05-02 19:53:30] 00009, contrastive_loss 6.81840, cls_loss1 10.01256, cls_loss2 4.77290, ent_loss 4.36108, ne_loss -4.59633, align_loss 4.60517, 
Training completed.
training step completed
 10% 10/104 [03:41<09:43,  6.21s/it]iteration number -->  10
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.09615384615384616
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0002704326923076923
learning rate from cosine_annealing_LR -->  0.0002704326923076923
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:32] 00010, lr 0.00027, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 424, 1168,  153,  ...,  723, 1187,  327], device='cuda:0')
Generated mixing coefficient (lambda): 0.5642477065646552
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 10.202207565307617
Classification loss 2 during warmup: 4.768336772918701
Alignment loss during warmup: 4.605165958404541
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.370946407318115
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596320152282715
Contrastive Loss: tensor(6.8514, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(10.2022, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7683, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3709, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5963, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.8514, device='cuda:0', grad_fn=<DivBackward0>), tensor(10.2022, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7683, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3709, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5963, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.851439952850342 cls_loss1: 10.202207565307617 cls_loss2: 4.768336772918701 ent_loss: 4.370946407318115 ne_loss: -4.596320152282715 align_loss: 4.605165958404541
Total loss: 26.2017765045166
[2024-05-02 19:53:33] 00010, contrastive_loss 6.85144, cls_loss1 10.20221, cls_loss2 4.76834, ent_loss 4.37095, ne_loss -4.59632, align_loss 4.60517, 
Training completed.
training step completed
 11% 11/104 [03:43<07:53,  5.09s/it]iteration number -->  11
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.10576923076923077
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.00029747596153846154
learning rate from cosine_annealing_LR -->  0.00029747596153846154
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:35] 00011, lr 0.00030, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 333,  796,  882,  ...,  955,  819, 1372], device='cuda:0')
Generated mixing coefficient (lambda): 0.7205520491410118
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.929697036743164
Classification loss 2 during warmup: 4.846858024597168
Alignment loss during warmup: 4.605172157287598
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.342606544494629
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.597158432006836
Contrastive Loss: tensor(6.8202, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.9297, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8469, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3426, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5972, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.8202, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.9297, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8469, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3426, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5972, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.820225715637207 cls_loss1: 9.929697036743164 cls_loss2: 4.846858024597168 ent_loss: 4.342606544494629 ne_loss: -4.597158432006836 align_loss: 4.605172157287598
Total loss: 25.947402954101562
[2024-05-02 19:53:36] 00011, contrastive_loss 6.82023, cls_loss1 9.92970, cls_loss2 4.84686, ent_loss 4.34261, ne_loss -4.59716, align_loss 4.60517, 
Training completed.
training step completed
 12% 12/104 [03:46<06:37,  4.32s/it]iteration number -->  12
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.11538461538461539
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.00032451923076923077
learning rate from cosine_annealing_LR -->  0.00032451923076923077
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:37] 00012, lr 0.00032, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1091,  774,   97,  ...,  146,   69,  602], device='cuda:0')
Generated mixing coefficient (lambda): 0.6560952779579972
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 10.001388549804688
Classification loss 2 during warmup: 4.796293258666992
Alignment loss during warmup: 4.605177402496338
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.364978313446045
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.597014904022217
Contrastive Loss: tensor(6.8326, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(10.0014, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7963, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3650, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5970, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.8326, device='cuda:0', grad_fn=<DivBackward0>), tensor(10.0014, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7963, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3650, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5970, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.832620620727539 cls_loss1: 10.001388549804688 cls_loss2: 4.796293258666992 ent_loss: 4.364978313446045 ne_loss: -4.597014904022217 align_loss: 4.605177402496338
Total loss: 26.003442764282227
[2024-05-02 19:53:38] 00012, contrastive_loss 6.83262, cls_loss1 10.00139, cls_loss2 4.79629, ent_loss 4.36498, ne_loss -4.59701, align_loss 4.60518, 
Training completed.
training step completed
 12% 13/104 [03:48<05:42,  3.76s/it]iteration number -->  13
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.125
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0003515625
learning rate from cosine_annealing_LR -->  0.0003515625
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:40] 00013, lr 0.00035, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1301,  257, 1382,  ...,  119,  662, 1147], device='cuda:0')
Generated mixing coefficient (lambda): 0.5902086397190913
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 10.005080223083496
Classification loss 2 during warmup: 4.815357208251953
Alignment loss during warmup: 4.605169773101807
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.376402378082275
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596035003662109
Contrastive Loss: tensor(6.8547, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(10.0051, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8154, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3764, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5960, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.8547, device='cuda:0', grad_fn=<DivBackward0>), tensor(10.0051, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8154, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3764, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5960, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.854679107666016 cls_loss1: 10.005080223083496 cls_loss2: 4.815357208251953 ent_loss: 4.376402378082275 ne_loss: -4.596035003662109 align_loss: 4.605169773101807
Total loss: 26.060653686523438
[2024-05-02 19:53:41] 00013, contrastive_loss 6.85468, cls_loss1 10.00508, cls_loss2 4.81536, ent_loss 4.37640, ne_loss -4.59604, align_loss 4.60517, 
Training completed.
training step completed
 13% 14/104 [03:51<05:03,  3.37s/it]iteration number -->  14
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.1346153846153846
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0003786057692307692
learning rate from cosine_annealing_LR -->  0.0003786057692307692
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:42] 00014, lr 0.00038, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1310,   77, 1244,  ...,  575, 1430,  577], device='cuda:0')
Generated mixing coefficient (lambda): 0.8260783257026236
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.998954772949219
Classification loss 2 during warmup: 4.875621795654297
Alignment loss during warmup: 4.605177879333496
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.346097469329834
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.597951889038086
Contrastive Loss: tensor(6.7357, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.9990, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8756, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3461, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5980, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.7357, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.9990, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8756, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3461, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5980, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.735678672790527 cls_loss1: 9.998954772949219 cls_loss2: 4.875621795654297 ent_loss: 4.346097469329834 ne_loss: -4.597951889038086 align_loss: 4.605177879333496
Total loss: 25.963581085205078


Streaming output truncated to the last 5000 lines.
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 70% 73/105 [00:17<00:03, 10.19it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 71% 75/105 [00:18<00:02, 10.18it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 73% 77/105 [00:18<00:02, 10.21it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 75% 79/105 [00:18<00:02, 10.19it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 77% 81/105 [00:18<00:02, 10.20it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 79% 83/105 [00:18<00:02, 10.20it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 81% 85/105 [00:19<00:01, 10.20it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 83% 87/105 [00:19<00:01, 10.18it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 85% 89/105 [00:19<00:01, 10.18it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 87% 91/105 [00:19<00:01, 10.13it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 89% 93/105 [00:19<00:01, 10.17it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 90% 95/105 [00:20<00:00, 10.18it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 92% 97/105 [00:20<00:00, 10.17it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 94% 99/105 [00:20<00:00, 10.14it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 96% 101/105 [00:20<00:00, 10.15it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 98% 103/105 [00:20<00:00, 10.16it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([80, 3, 32, 32]) labels shape: torch.Size([80])
Encoder output shape: torch.Size([80, 512])
Local cluster labels shape: torch.Size([80, 100])
Local features shape: torch.Size([80, 256])

100% 105/105 [00:21<00:00,  4.93it/s]
Updated features tensor shape after indexing: torch.Size([50000, 256])
Updated all_labels tensor shape after indexing: torch.Size([50000])
Updated cluster_labels tensor shape after indexing: torch.Size([50000, 100])
Final labels tensor shape: torch.Size([50000])
Feature extraction completed.
Extracted features and cluster labels from the memory loader.
Parameters of the config file: Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
Computed cluster centers after matrix multiplication: torch.Size([100, 256])
Computed confidence scores: torch.Size([50000])
Computed confidence, context assignments, and centers for noise detection.
Copied centers and context assignments to the model.
Obtained confidence, context assignments, features, and cluster labels from correct_labels function.
Copied confidence values to the tcl model.
data root ---->  /content/dataset/
dataset name ---------------->  cifar100
dataset type ---->  <class 'torchvision.datasets.cifar.CIFAR100'>
has_subfolder ---->  True
Files already downloaded and verified
dataset ---->  50000
Clean labels from the training dataset: 50000
Check if each label is clean: [False False False ... False False False]
save 0000001-0-context_assignments_hist.png to ./ckpt/2024_05_02_19_49_41-cifar100_90_prer18/save_images
Histogram of context assignments computed.
Training accuracy: tensor(0.0100, device='cuda:0')
Extracting features from the model...
Initialized features tensor shape: torch.Size([10000, 256])
Initialized all_labels tensor shape: torch.Size([10000])
Initialized cluster_labels tensor shape: torch.Size([10000, 100])

  0% 0/21 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.
  self.pid = os.fork()
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  5% 1/21 [00:02<00:58,  2.91s/it]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 10% 2/21 [00:03<00:30,  1.61s/it]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 14% 3/21 [00:03<00:17,  1.05it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 19% 4/21 [00:03<00:10,  1.62it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 29% 6/21 [00:04<00:05,  2.92it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 38% 8/21 [00:04<00:03,  4.22it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 48% 10/21 [00:04<00:02,  5.41it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 57% 12/21 [00:04<00:01,  6.49it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 67% 14/21 [00:04<00:00,  7.35it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 76% 16/21 [00:05<00:00,  8.07it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 86% 18/21 [00:05<00:00,  8.63it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 90% 19/21 [00:05<00:00,  8.84it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([400, 3, 32, 32]) labels shape: torch.Size([400])
Encoder output shape: torch.Size([400, 512])
Local cluster labels shape: torch.Size([400, 100])
Local features shape: torch.Size([400, 256])

100% 21/21 [00:06<00:00,  3.13it/s]
Updated features tensor shape after indexing: torch.Size([10000, 256])
Updated all_labels tensor shape after indexing: torch.Size([10000])
Updated cluster_labels tensor shape after indexing: torch.Size([10000, 100])
Final labels tensor shape: torch.Size([10000])
Feature extraction completed.
Extracted features, test cluster labels, and test labels.
Test accuracy: tensor(0.0100, device='cuda:0')
KNN labels predicted for the test features: tensor([12, 80, 30,  ..., 63, 66, 45], device='cuda:0')
(tensor([ 9, 77, 94], device='cuda:0'), tensor([   1,    4, 9995], device='cuda:0'))
KNN accuracy: tensor(0.1164, device='cuda:0')
Estimated noise ratio: 0.4764999747276306
Updated scale1 of tcl model: 0.4764999747276306
Noise accuracy: tensor(0.5162, device='cuda:0')
Context noise AUC: 0.4909500712529655
[2024-05-02 19:50:23] 00001, estimated_noise_ratio 0.47650, noise_accuracy 0.51624, context_noise_auc 0.49095, train_acc 0.01000, test_acc 0.01000, knn_acc 0.11640, 
Evaluated the tcl model using obtained features, confidence, cluster labels, and context assignments.
Before training starts
inside while loop
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.
  self.pid = os.fork()
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.009615384615384616
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  2.704326923076923e-05
learning rate from cosine_annealing_LR -->  2.704326923076923e-05
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:51:04] 00001, lr 0.00003, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 483,  782,  686,  ...,  416,  282, 1332], device='cuda:0')
Generated mixing coefficient (lambda): 0.44912526447830964
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 10.138054847717285
Classification loss 2 during warmup: 4.812106609344482
Alignment loss during warmup: 4.605175018310547
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.359892845153809
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5969133377075195
Contrastive Loss: tensor(6.9310, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(10.1381, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8121, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3599, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5969, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.9310, device='cuda:0', grad_fn=<DivBackward0>), tensor(10.1381, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8121, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3599, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5969, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.9309916496276855 cls_loss1: 10.138054847717285 cls_loss2: 4.812106609344482 ent_loss: 4.359892845153809 ne_loss: -4.5969133377075195 align_loss: 4.605175018310547
Total loss: 26.249309539794922
[2024-05-02 19:52:59] 00001, contrastive_loss 6.93099, cls_loss1 10.13805, cls_loss2 4.81211, ent_loss 4.35989, ne_loss -4.59691, align_loss 4.60518, 
Training completed.
training step completed
  2% 2/104 [03:07<5:19:08, 187.73s/it]iteration number -->  2
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.019230769230769232
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  5.408653846153846e-05
learning rate from cosine_annealing_LR -->  5.408653846153846e-05
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:52:59] 00002, lr 0.00005, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1083,  999,  924,  ...,  743,  411,  971], device='cuda:0')
Generated mixing coefficient (lambda): 0.023707189032098733
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.823692321777344
Classification loss 2 during warmup: 5.04218053817749
Alignment loss during warmup: 4.605167865753174
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.280387878417969
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.600931167602539
Contrastive Loss: tensor(6.9424, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.8237, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(5.0422, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.2804, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.6009, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.9424, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.8237, device='cuda:0', grad_fn=<AddBackward0>), tensor(5.0422, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.2804, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.6009, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.942377090454102 cls_loss1: 9.823692321777344 cls_loss2: 5.04218053817749 ent_loss: 4.280387878417969 ne_loss: -4.600931167602539 align_loss: 4.605167865753174
Total loss: 26.09287452697754
[2024-05-02 19:53:14] 00002, contrastive_loss 6.94238, cls_loss1 9.82369, cls_loss2 5.04218, ent_loss 4.28039, ne_loss -4.60093, align_loss 4.60517, 
Training completed.
training step completed
  3% 3/104 [03:23<2:25:38, 86.52s/it] iteration number -->  3
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.028846153846153848
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  8.112980769230769e-05
learning rate from cosine_annealing_LR -->  8.112980769230769e-05
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:14] 00003, lr 0.00008, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([  56,  425, 1161,  ...,  821,  279,  284], device='cuda:0')
Generated mixing coefficient (lambda): 0.15599464004468885
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 10.150880813598633
Classification loss 2 during warmup: 4.977916717529297
Alignment loss during warmup: 4.605169773101807
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.321966171264648
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.599339962005615
Contrastive Loss: tensor(6.9610, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(10.1509, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9779, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3220, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5993, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.9610, device='cuda:0', grad_fn=<DivBackward0>), tensor(10.1509, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9779, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3220, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5993, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.9610114097595215 cls_loss1: 10.150880813598633 cls_loss2: 4.977916717529297 ent_loss: 4.321966171264648 ne_loss: -4.599339962005615 align_loss: 4.605169773101807
Total loss: 26.417604446411133
[2024-05-02 19:53:15] 00003, contrastive_loss 6.96101, cls_loss1 10.15088, cls_loss2 4.97792, ent_loss 4.32197, ne_loss -4.59934, align_loss 4.60517, 
Training completed.
training step completed
  4% 4/104 [03:26<1:20:20, 48.20s/it]iteration number -->  4
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.038461538461538464
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.00010817307692307693
learning rate from cosine_annealing_LR -->  0.00010817307692307693
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:17] 00004, lr 0.00011, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1315,  898,  838,  ...,  818, 1361, 1377], device='cuda:0')
Generated mixing coefficient (lambda): 0.5572264854633476
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 10.11421012878418
Classification loss 2 during warmup: 4.794486045837402
Alignment loss during warmup: 4.605165004730225
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.354094982147217
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596653938293457
Contrastive Loss: tensor(6.9310, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(10.1142, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7945, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3541, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5967, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.9310, device='cuda:0', grad_fn=<DivBackward0>), tensor(10.1142, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7945, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3541, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5967, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.9310197830200195 cls_loss1: 10.11421012878418 cls_loss2: 4.794486045837402 ent_loss: 4.354094982147217 ne_loss: -4.596653938293457 align_loss: 4.605165004730225
Total loss: 26.202322006225586
[2024-05-02 19:53:18] 00004, contrastive_loss 6.93102, cls_loss1 10.11421, cls_loss2 4.79449, ent_loss 4.35409, ne_loss -4.59665, align_loss 4.60517, 
Training completed.
training step completed
  5% 5/104 [03:28<49:49, 30.19s/it]  iteration number -->  5
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.04807692307692308
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.00013521634615384616
learning rate from cosine_annealing_LR -->  0.00013521634615384616
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:20] 00005, lr 0.00014, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 260,  589,   77,  ...,  816,  832, 1082], device='cuda:0')
Generated mixing coefficient (lambda): 0.029523968500001368
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.837117195129395
Classification loss 2 during warmup: 4.9652791023254395
Alignment loss during warmup: 4.605168342590332
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.300097942352295
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.600799560546875
Contrastive Loss: tensor(6.9927, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.8371, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9653, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3001, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.6008, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.9927, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.8371, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9653, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3001, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.6008, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.992705345153809 cls_loss1: 9.837117195129395 cls_loss2: 4.9652791023254395 ent_loss: 4.300097942352295 ne_loss: -4.600799560546875 align_loss: 4.605168342590332
Total loss: 26.099567413330078
[2024-05-02 19:53:20] 00005, contrastive_loss 6.99271, cls_loss1 9.83712, cls_loss2 4.96528, ent_loss 4.30010, ne_loss -4.60080, align_loss 4.60517, 
Training completed.
training step completed
  6% 6/104 [03:31<33:02, 20.23s/it]iteration number -->  6
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.057692307692307696
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.00016225961538461538
learning rate from cosine_annealing_LR -->  0.00016225961538461538
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:22] 00006, lr 0.00016, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1315,  266,  515,  ..., 1255, 1292,  495], device='cuda:0')
Generated mixing coefficient (lambda): 0.45133699035902825
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 10.146890640258789
Classification loss 2 during warmup: 4.8062567710876465
Alignment loss during warmup: 4.605167865753174
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.362261772155762
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596324443817139
Contrastive Loss: tensor(6.9608, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(10.1469, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8063, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3623, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5963, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.9608, device='cuda:0', grad_fn=<DivBackward0>), tensor(10.1469, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8063, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3623, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5963, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.960809230804443 cls_loss1: 10.146890640258789 cls_loss2: 4.8062567710876465 ent_loss: 4.362261772155762 ne_loss: -4.596324443817139 align_loss: 4.605167865753174
Total loss: 26.28506088256836
[2024-05-02 19:53:23] 00006, contrastive_loss 6.96081, cls_loss1 10.14689, cls_loss2 4.80626, ent_loss 4.36226, ne_loss -4.59632, align_loss 4.60517, 
Training completed.
training step completed
  7% 7/104 [03:33<22:57, 14.20s/it]iteration number -->  7
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.0673076923076923
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0001893028846153846
learning rate from cosine_annealing_LR -->  0.0001893028846153846
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:25] 00007, lr 0.00019, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([  30,  357, 1060,  ...,  674,  273, 1427], device='cuda:0')
Generated mixing coefficient (lambda): 0.9205318443348298
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.866230010986328
Classification loss 2 during warmup: 4.988160610198975
Alignment loss during warmup: 4.605161666870117
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.291213512420654
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.600477695465088
Contrastive Loss: tensor(6.9676, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.8662, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9882, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.2912, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.6005, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.9676, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.8662, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9882, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.2912, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.6005, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.967554092407227 cls_loss1: 9.866230010986328 cls_loss2: 4.988160610198975 ent_loss: 4.291213512420654 ne_loss: -4.600477695465088 align_loss: 4.605161666870117
Total loss: 26.117843627929688
[2024-05-02 19:53:25] 00007, contrastive_loss 6.96755, cls_loss1 9.86623, cls_loss2 4.98816, ent_loss 4.29121, ne_loss -4.60048, align_loss 4.60516, 
Training completed.
training step completed
  8% 8/104 [03:36<16:36, 10.38s/it]iteration number -->  8
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.07692307692307693
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.00021634615384615385
learning rate from cosine_annealing_LR -->  0.00021634615384615385
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:27] 00008, lr 0.00022, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 327,  838, 1306,  ...,  559,   20,  569], device='cuda:0')
Generated mixing coefficient (lambda): 0.6200326211997292
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.87826156616211
Classification loss 2 during warmup: 4.766782760620117
Alignment loss during warmup: 4.605171203613281
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.3725152015686035
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596757411956787
Contrastive Loss: tensor(6.9829, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.8783, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7668, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3725, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5968, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.9829, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.8783, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7668, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3725, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5968, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.982922554016113 cls_loss1: 9.87826156616211 cls_loss2: 4.766782760620117 ent_loss: 4.3725152015686035 ne_loss: -4.596757411956787 align_loss: 4.605171203613281
Total loss: 26.008895874023438
[2024-05-02 19:53:28] 00008, contrastive_loss 6.98292, cls_loss1 9.87826, cls_loss2 4.76678, ent_loss 4.37252, ne_loss -4.59676, align_loss 4.60517, 
Training completed.
training step completed
  9% 9/104 [03:38<12:27,  7.87s/it]iteration number -->  9
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.08653846153846154
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.00024338942307692305
learning rate from cosine_annealing_LR -->  0.00024338942307692305
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:30] 00009, lr 0.00024, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1057,  629,  201,  ..., 1391,  579,  464], device='cuda:0')
Generated mixing coefficient (lambda): 0.46445321844681775
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 10.012557983398438
Classification loss 2 during warmup: 4.77290153503418
Alignment loss during warmup: 4.605165958404541
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.361077785491943
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596325874328613
Contrastive Loss: tensor(6.8184, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(10.0126, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7729, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3611, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5963, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.8184, device='cuda:0', grad_fn=<DivBackward0>), tensor(10.0126, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7729, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3611, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5963, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.818404674530029 cls_loss1: 10.012557983398438 cls_loss2: 4.77290153503418 ent_loss: 4.361077785491943 ne_loss: -4.596325874328613 align_loss: 4.605165958404541
Total loss: 25.973783493041992
[2024-05-02 19:53:30] 00009, contrastive_loss 6.81840, cls_loss1 10.01256, cls_loss2 4.77290, ent_loss 4.36108, ne_loss -4.59633, align_loss 4.60517, 
Training completed.
training step completed
 10% 10/104 [03:41<09:43,  6.21s/it]iteration number -->  10
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.09615384615384616
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0002704326923076923
learning rate from cosine_annealing_LR -->  0.0002704326923076923
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:32] 00010, lr 0.00027, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 424, 1168,  153,  ...,  723, 1187,  327], device='cuda:0')
Generated mixing coefficient (lambda): 0.5642477065646552
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 10.202207565307617
Classification loss 2 during warmup: 4.768336772918701
Alignment loss during warmup: 4.605165958404541
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.370946407318115
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596320152282715
Contrastive Loss: tensor(6.8514, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(10.2022, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7683, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3709, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5963, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.8514, device='cuda:0', grad_fn=<DivBackward0>), tensor(10.2022, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7683, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3709, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5963, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.851439952850342 cls_loss1: 10.202207565307617 cls_loss2: 4.768336772918701 ent_loss: 4.370946407318115 ne_loss: -4.596320152282715 align_loss: 4.605165958404541
Total loss: 26.2017765045166
[2024-05-02 19:53:33] 00010, contrastive_loss 6.85144, cls_loss1 10.20221, cls_loss2 4.76834, ent_loss 4.37095, ne_loss -4.59632, align_loss 4.60517, 
Training completed.
training step completed
 11% 11/104 [03:43<07:53,  5.09s/it]iteration number -->  11
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.10576923076923077
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.00029747596153846154
learning rate from cosine_annealing_LR -->  0.00029747596153846154
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:35] 00011, lr 0.00030, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 333,  796,  882,  ...,  955,  819, 1372], device='cuda:0')
Generated mixing coefficient (lambda): 0.7205520491410118
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.929697036743164
Classification loss 2 during warmup: 4.846858024597168
Alignment loss during warmup: 4.605172157287598
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.342606544494629
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.597158432006836
Contrastive Loss: tensor(6.8202, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.9297, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8469, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3426, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5972, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.8202, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.9297, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8469, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3426, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5972, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.820225715637207 cls_loss1: 9.929697036743164 cls_loss2: 4.846858024597168 ent_loss: 4.342606544494629 ne_loss: -4.597158432006836 align_loss: 4.605172157287598
Total loss: 25.947402954101562
[2024-05-02 19:53:36] 00011, contrastive_loss 6.82023, cls_loss1 9.92970, cls_loss2 4.84686, ent_loss 4.34261, ne_loss -4.59716, align_loss 4.60517, 
Training completed.
training step completed
 12% 12/104 [03:46<06:37,  4.32s/it]iteration number -->  12
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.11538461538461539
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.00032451923076923077
learning rate from cosine_annealing_LR -->  0.00032451923076923077
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:37] 00012, lr 0.00032, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1091,  774,   97,  ...,  146,   69,  602], device='cuda:0')
Generated mixing coefficient (lambda): 0.6560952779579972
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 10.001388549804688
Classification loss 2 during warmup: 4.796293258666992
Alignment loss during warmup: 4.605177402496338
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.364978313446045
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.597014904022217
Contrastive Loss: tensor(6.8326, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(10.0014, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7963, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3650, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5970, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.8326, device='cuda:0', grad_fn=<DivBackward0>), tensor(10.0014, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7963, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3650, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5970, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.832620620727539 cls_loss1: 10.001388549804688 cls_loss2: 4.796293258666992 ent_loss: 4.364978313446045 ne_loss: -4.597014904022217 align_loss: 4.605177402496338
Total loss: 26.003442764282227
[2024-05-02 19:53:38] 00012, contrastive_loss 6.83262, cls_loss1 10.00139, cls_loss2 4.79629, ent_loss 4.36498, ne_loss -4.59701, align_loss 4.60518, 
Training completed.
training step completed
 12% 13/104 [03:48<05:42,  3.76s/it]iteration number -->  13
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.125
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0003515625
learning rate from cosine_annealing_LR -->  0.0003515625
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:40] 00013, lr 0.00035, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1301,  257, 1382,  ...,  119,  662, 1147], device='cuda:0')
Generated mixing coefficient (lambda): 0.5902086397190913
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 10.005080223083496
Classification loss 2 during warmup: 4.815357208251953
Alignment loss during warmup: 4.605169773101807
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.376402378082275
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596035003662109
Contrastive Loss: tensor(6.8547, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(10.0051, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8154, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3764, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5960, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.8547, device='cuda:0', grad_fn=<DivBackward0>), tensor(10.0051, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8154, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3764, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5960, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.854679107666016 cls_loss1: 10.005080223083496 cls_loss2: 4.815357208251953 ent_loss: 4.376402378082275 ne_loss: -4.596035003662109 align_loss: 4.605169773101807
Total loss: 26.060653686523438
[2024-05-02 19:53:41] 00013, contrastive_loss 6.85468, cls_loss1 10.00508, cls_loss2 4.81536, ent_loss 4.37640, ne_loss -4.59604, align_loss 4.60517, 
Training completed.
training step completed
 13% 14/104 [03:51<05:03,  3.37s/it]iteration number -->  14
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.1346153846153846
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0003786057692307692
learning rate from cosine_annealing_LR -->  0.0003786057692307692
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:42] 00014, lr 0.00038, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1310,   77, 1244,  ...,  575, 1430,  577], device='cuda:0')
Generated mixing coefficient (lambda): 0.8260783257026236
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.998954772949219
Classification loss 2 during warmup: 4.875621795654297
Alignment loss during warmup: 4.605177879333496
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.346097469329834
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.597951889038086
Contrastive Loss: tensor(6.7357, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.9990, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8756, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3461, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5980, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.7357, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.9990, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8756, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3461, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5980, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.735678672790527 cls_loss1: 9.998954772949219 cls_loss2: 4.875621795654297 ent_loss: 4.346097469329834 ne_loss: -4.597951889038086 align_loss: 4.605177879333496
Total loss: 25.963581085205078
[2024-05-02 19:53:43] 00014, contrastive_loss 6.73568, cls_loss1 9.99895, cls_loss2 4.87562, ent_loss 4.34610, ne_loss -4.59795, align_loss 4.60518, 
Training completed.
training step completed
 14% 15/104 [03:53<04:36,  3.10s/it]iteration number -->  15
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.14423076923076922
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.00040564903846153843
learning rate from cosine_annealing_LR -->  0.00040564903846153843
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:45] 00015, lr 0.00041, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 955, 1395, 1150,  ...,   86, 1045,  290], device='cuda:0')
Generated mixing coefficient (lambda): 0.34774410463978506
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.907892227172852
Classification loss 2 during warmup: 4.776327133178711
Alignment loss during warmup: 4.605173110961914
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.3733415603637695
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596141338348389
Contrastive Loss: tensor(6.7458, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.9079, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7763, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3733, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5961, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.7458, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.9079, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7763, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3733, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5961, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.745786190032959 cls_loss1: 9.907892227172852 cls_loss2: 4.776327133178711 ent_loss: 4.3733415603637695 ne_loss: -4.596141338348389 align_loss: 4.605173110961914
Total loss: 25.812379837036133
[2024-05-02 19:53:45] 00015, contrastive_loss 6.74579, cls_loss1 9.90789, cls_loss2 4.77633, ent_loss 4.37334, ne_loss -4.59614, align_loss 4.60517, 
Training completed.
training step completed
 15% 16/104 [03:56<04:16,  2.91s/it]iteration number -->  16
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.15384615384615385
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0004326923076923077
learning rate from cosine_annealing_LR -->  0.0004326923076923077
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:47] 00016, lr 0.00043, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([464, 195, 885,  ..., 993, 888, 187], device='cuda:0')
Generated mixing coefficient (lambda): 0.8942360778803845
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.715736389160156
Classification loss 2 during warmup: 4.951152801513672
Alignment loss during warmup: 4.605167865753174
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.341770172119141
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.598780632019043
Contrastive Loss: tensor(6.7556, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.7157, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9512, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3418, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5988, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.7556, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.7157, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9512, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3418, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5988, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.755568504333496 cls_loss1: 9.715736389160156 cls_loss2: 4.951152801513672 ent_loss: 4.341770172119141 ne_loss: -4.598780632019043 align_loss: 4.605167865753174
Total loss: 25.770614624023438
[2024-05-02 19:53:48] 00016, contrastive_loss 6.75557, cls_loss1 9.71574, cls_loss2 4.95115, ent_loss 4.34177, ne_loss -4.59878, align_loss 4.60517, 
Training completed.
training step completed
 16% 17/104 [03:58<04:03,  2.80s/it]iteration number -->  17
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.16346153846153846
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.00045973557692307687
learning rate from cosine_annealing_LR -->  0.00045973557692307687
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:50] 00017, lr 0.00046, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1076,  719,  957,  ...,    7,  600,  183], device='cuda:0')
Generated mixing coefficient (lambda): 0.8971156837051465
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.753928184509277
Classification loss 2 during warmup: 4.899257659912109
Alignment loss during warmup: 4.605175018310547
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.348363399505615
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.598930358886719
Contrastive Loss: tensor(6.7120, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.7539, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8993, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3484, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5989, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.7120, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.7539, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8993, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3484, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5989, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.712009429931641 cls_loss1: 9.753928184509277 cls_loss2: 4.899257659912109 ent_loss: 4.348363399505615 ne_loss: -4.598930358886719 align_loss: 4.605175018310547
Total loss: 25.719804763793945
[2024-05-02 19:53:51] 00017, contrastive_loss 6.71201, cls_loss1 9.75393, cls_loss2 4.89926, ent_loss 4.34836, ne_loss -4.59893, align_loss 4.60518, 
Training completed.
training step completed
 17% 18/104 [04:01<03:54,  2.73s/it]iteration number -->  18
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.17307692307692307
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0004867788461538461
learning rate from cosine_annealing_LR -->  0.0004867788461538461
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:52] 00018, lr 0.00049, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 384,  527,  456,  ...,  311,  490, 1411], device='cuda:0')
Generated mixing coefficient (lambda): 0.9496592932052692
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.732040405273438
Classification loss 2 during warmup: 4.929216384887695
Alignment loss during warmup: 4.605166912078857
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.344701290130615
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.599305629730225
Contrastive Loss: tensor(6.6422, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.7320, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9292, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3447, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5993, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.6422, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.7320, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9292, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3447, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5993, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.6421966552734375 cls_loss1: 9.732040405273438 cls_loss2: 4.929216384887695 ent_loss: 4.344701290130615 ne_loss: -4.599305629730225 align_loss: 4.605166912078857
Total loss: 25.654016494750977
[2024-05-02 19:53:53] 00018, contrastive_loss 6.64220, cls_loss1 9.73204, cls_loss2 4.92922, ent_loss 4.34470, ne_loss -4.59931, align_loss 4.60517, 
Training completed.
training step completed
 18% 19/104 [04:03<03:47,  2.68s/it]iteration number -->  19
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.18269230769230768
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0005138221153846153
learning rate from cosine_annealing_LR -->  0.0005138221153846153
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:55] 00019, lr 0.00051, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 135,  277, 1293,  ...,  386,  103,  551], device='cuda:0')
Generated mixing coefficient (lambda): 0.7017481135576186
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.981897354125977
Classification loss 2 during warmup: 4.8102850914001465
Alignment loss during warmup: 4.6051716804504395
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.392019271850586
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.595961093902588
Contrastive Loss: tensor(6.6836, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.9819, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8103, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3920, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5960, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.6836, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.9819, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8103, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3920, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5960, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.683591842651367 cls_loss1: 9.981897354125977 cls_loss2: 4.8102850914001465 ent_loss: 4.392019271850586 ne_loss: -4.595961093902588 align_loss: 4.6051716804504395
Total loss: 25.877004623413086
[2024-05-02 19:53:56] 00019, contrastive_loss 6.68359, cls_loss1 9.98190, cls_loss2 4.81029, ent_loss 4.39202, ne_loss -4.59596, align_loss 4.60517, 
Training completed.
training step completed
 19% 20/104 [04:06<03:39,  2.61s/it]iteration number -->  20
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.19230769230769232
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0005408653846153846
learning rate from cosine_annealing_LR -->  0.0005408653846153846
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:53:57] 00020, lr 0.00054, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1201,  357,  205,  ...,  451,  319,  934], device='cuda:0')
Generated mixing coefficient (lambda): 0.7138180268008237
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.859444618225098
Classification loss 2 during warmup: 4.771574020385742
Alignment loss during warmup: 4.605164051055908
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.379884243011475
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.595162868499756
Contrastive Loss: tensor(6.6377, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.8594, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7716, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3799, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5952, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.6377, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.8594, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7716, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3799, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5952, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.637685298919678 cls_loss1: 9.859444618225098 cls_loss2: 4.771574020385742 ent_loss: 4.379884243011475 ne_loss: -4.595162868499756 align_loss: 4.605164051055908
Total loss: 25.658588409423828
[2024-05-02 19:53:58] 00020, contrastive_loss 6.63769, cls_loss1 9.85944, cls_loss2 4.77157, ent_loss 4.37988, ne_loss -4.59516, align_loss 4.60516, 
Training completed.
training step completed
 20% 21/104 [04:08<03:33,  2.57s/it]iteration number -->  21
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.20192307692307693
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0005679086538461539
learning rate from cosine_annealing_LR -->  0.0005679086538461539
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:54:00] 00021, lr 0.00057, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([  60,  619,  153,  ..., 1412,  319, 1012], device='cuda:0')
Generated mixing coefficient (lambda): 0.43425720846941174
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.813547134399414
Classification loss 2 during warmup: 4.773649215698242
Alignment loss during warmup: 4.605171203613281
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.41091251373291
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.593672752380371
Contrastive Loss: tensor(6.6479, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.8135, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7736, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4109, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5937, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.6479, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.8135, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7736, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4109, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5937, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.6478590965271 cls_loss1: 9.813547134399414 cls_loss2: 4.773649215698242 ent_loss: 4.41091251373291 ne_loss: -4.593672752380371 align_loss: 4.605171203613281
Total loss: 25.657466888427734
[2024-05-02 19:54:01] 00021, contrastive_loss 6.64786, cls_loss1 9.81355, cls_loss2 4.77365, ent_loss 4.41091, ne_loss -4.59367, align_loss 4.60517, 
Training completed.
training step completed
 21% 22/104 [04:11<03:28,  2.54s/it]iteration number -->  22
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.21153846153846154
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0005949519230769231
learning rate from cosine_annealing_LR -->  0.0005949519230769231
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:54:02] 00022, lr 0.00059, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1230,   85, 1360,  ...,  272,  215,  780], device='cuda:0')
Generated mixing coefficient (lambda): 0.08478169877718927
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.668282508850098
Classification loss 2 during warmup: 4.9280009269714355
Alignment loss during warmup: 4.6051716804504395
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.360316753387451
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596997261047363
Contrastive Loss: tensor(6.7100, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.6683, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9280, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3603, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5970, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.7100, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.6683, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9280, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3603, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5970, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.710000991821289 cls_loss1: 9.668282508850098 cls_loss2: 4.9280009269714355 ent_loss: 4.360316753387451 ne_loss: -4.596997261047363 align_loss: 4.6051716804504395
Total loss: 25.674774169921875
[2024-05-02 19:54:03] 00022, contrastive_loss 6.71000, cls_loss1 9.66828, cls_loss2 4.92800, ent_loss 4.36032, ne_loss -4.59700, align_loss 4.60517, 
Training completed.
training step completed
 22% 23/104 [04:13<03:23,  2.51s/it]iteration number -->  23
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.22115384615384615
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0006219951923076922
learning rate from cosine_annealing_LR -->  0.0006219951923076922
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:54:05] 00023, lr 0.00062, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([159, 108, 249,  ..., 109, 774,  79], device='cuda:0')
Generated mixing coefficient (lambda): 0.6810187749184367
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.812026977539062
Classification loss 2 during warmup: 4.793875694274902
Alignment loss during warmup: 4.605171203613281
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.397053241729736
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.593618392944336
Contrastive Loss: tensor(6.6663, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.8120, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7939, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3971, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5936, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.6663, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.8120, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7939, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3971, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5936, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.666316032409668 cls_loss1: 9.812026977539062 cls_loss2: 4.793875694274902 ent_loss: 4.397053241729736 ne_loss: -4.593618392944336 align_loss: 4.605171203613281
Total loss: 25.680824279785156
[2024-05-02 19:54:06] 00023, contrastive_loss 6.66632, cls_loss1 9.81203, cls_loss2 4.79388, ent_loss 4.39705, ne_loss -4.59362, align_loss 4.60517, 
Training completed.
training step completed
 23% 24/104 [04:16<03:21,  2.51s/it]iteration number -->  24
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.23076923076923078
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0006490384615384615
learning rate from cosine_annealing_LR -->  0.0006490384615384615
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:54:07] 00024, lr 0.00065, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1073,  951,  567,  ..., 1061,   67,  706], device='cuda:0')
Generated mixing coefficient (lambda): 0.847792852616476
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.699512481689453
Classification loss 2 during warmup: 4.898918628692627
Alignment loss during warmup: 4.605168342590332
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.375558376312256
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.595273017883301
Contrastive Loss: tensor(6.6699, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.6995, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8989, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3756, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5953, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.6699, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.6995, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8989, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3756, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5953, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.669850826263428 cls_loss1: 9.699512481689453 cls_loss2: 4.898918628692627 ent_loss: 4.375558376312256 ne_loss: -4.595273017883301 align_loss: 4.605168342590332
Total loss: 25.653736114501953
[2024-05-02 19:54:08] 00024, contrastive_loss 6.66985, cls_loss1 9.69951, cls_loss2 4.89892, ent_loss 4.37556, ne_loss -4.59527, align_loss 4.60517, 
Training completed.
training step completed
 24% 25/104 [04:18<03:19,  2.53s/it]iteration number -->  25
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.2403846153846154
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0006760817307692308
learning rate from cosine_annealing_LR -->  0.0006760817307692308
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:54:10] 00025, lr 0.00068, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 441, 1138, 1166,  ...,  209,  274,   53], device='cuda:0')
Generated mixing coefficient (lambda): 0.3230990163566997
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.862503051757812
Classification loss 2 during warmup: 4.776951313018799
Alignment loss during warmup: 4.605166435241699
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.416754245758057
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.59318733215332
Contrastive Loss: tensor(6.5610, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.8625, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7770, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4168, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5932, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.5610, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.8625, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7770, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4168, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5932, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.561023235321045 cls_loss1: 9.862503051757812 cls_loss2: 4.776951313018799 ent_loss: 4.416754245758057 ne_loss: -4.59318733215332 align_loss: 4.605166435241699
Total loss: 25.62921142578125
[2024-05-02 19:54:11] 00025, contrastive_loss 6.56102, cls_loss1 9.86250, cls_loss2 4.77695, ent_loss 4.41675, ne_loss -4.59319, align_loss 4.60517, 
Training completed.
training step completed
 25% 26/104 [04:21<03:16,  2.52s/it]iteration number -->  26
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.25
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.000703125
learning rate from cosine_annealing_LR -->  0.000703125
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:54:12] 00026, lr 0.00070, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1297,  945, 1191,  ...,  777,  482,  332], device='cuda:0')
Generated mixing coefficient (lambda): 0.15539908865648952
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.77762508392334
Classification loss 2 during warmup: 4.908077239990234
Alignment loss during warmup: 4.60517692565918
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.39987325668335
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.594970703125
Contrastive Loss: tensor(6.5732, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.7776, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9081, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3999, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5950, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.5732, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.7776, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9081, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3999, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5950, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.573196887969971 cls_loss1: 9.77762508392334 cls_loss2: 4.908077239990234 ent_loss: 4.39987325668335 ne_loss: -4.594970703125 align_loss: 4.60517692565918
Total loss: 25.66897964477539
[2024-05-02 19:54:13] 00026, contrastive_loss 6.57320, cls_loss1 9.77763, cls_loss2 4.90808, ent_loss 4.39987, ne_loss -4.59497, align_loss 4.60518, 
Training completed.
training step completed
 26% 27/104 [04:23<03:12,  2.50s/it]iteration number -->  27
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.25961538461538464
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0007301682692307692
learning rate from cosine_annealing_LR -->  0.0007301682692307692
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:54:15] 00027, lr 0.00073, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([  25,  865,  875,  ...,  818, 1061, 1348], device='cuda:0')
Generated mixing coefficient (lambda): 0.6123733632361088
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.783134460449219
Classification loss 2 during warmup: 4.772831439971924
Alignment loss during warmup: 4.605175971984863
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.42838716506958
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591039657592773
Contrastive Loss: tensor(6.5968, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.7831, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7728, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4284, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5910, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.5968, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.7831, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7728, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4284, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5910, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.5967912673950195 cls_loss1: 9.783134460449219 cls_loss2: 4.772831439971924 ent_loss: 4.42838716506958 ne_loss: -4.591039657592773 align_loss: 4.605175971984863
Total loss: 25.595279693603516
[2024-05-02 19:54:16] 00027, contrastive_loss 6.59679, cls_loss1 9.78313, cls_loss2 4.77283, ent_loss 4.42839, ne_loss -4.59104, align_loss 4.60518, 
Training completed.
training step completed
 27% 28/104 [04:26<03:09,  2.50s/it]iteration number -->  28
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.2692307692307692
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0007572115384615384
learning rate from cosine_annealing_LR -->  0.0007572115384615384
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:54:17] 00028, lr 0.00076, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 259,  712, 1012,  ...,  640,  507,  888], device='cuda:0')
Generated mixing coefficient (lambda): 0.966853169621924
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.600409507751465
Classification loss 2 during warmup: 4.913118839263916
Alignment loss during warmup: 4.6051716804504395
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.383155345916748
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596548557281494
Contrastive Loss: tensor(6.6179, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.6004, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9131, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3832, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5965, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.6179, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.6004, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9131, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3832, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5965, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.61790657043457 cls_loss1: 9.600409507751465 cls_loss2: 4.913118839263916 ent_loss: 4.383155345916748 ne_loss: -4.596548557281494 align_loss: 4.6051716804504395
Total loss: 25.52321434020996
[2024-05-02 19:54:18] 00028, contrastive_loss 6.61791, cls_loss1 9.60041, cls_loss2 4.91312, ent_loss 4.38316, ne_loss -4.59655, align_loss 4.60517, 
Training completed.
training step completed
 28% 29/104 [04:28<03:06,  2.49s/it]iteration number -->  29
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.27884615384615385
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0007842548076923076
learning rate from cosine_annealing_LR -->  0.0007842548076923076
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:54:20] 00029, lr 0.00078, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([119, 136, 703,  ..., 456,  88, 751], device='cuda:0')
Generated mixing coefficient (lambda): 0.994367470960539
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.620136260986328
Classification loss 2 during warmup: 4.963539123535156
Alignment loss during warmup: 4.605166912078857
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.370198726654053
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596353530883789
Contrastive Loss: tensor(6.5894, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.6201, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9635, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3702, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5964, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.5894, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.6201, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9635, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3702, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5964, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.589373588562012 cls_loss1: 9.620136260986328 cls_loss2: 4.963539123535156 ent_loss: 4.370198726654053 ne_loss: -4.596353530883789 align_loss: 4.605166912078857
Total loss: 25.55206298828125
[2024-05-02 19:54:21] 00029, contrastive_loss 6.58937, cls_loss1 9.62014, cls_loss2 4.96354, ent_loss 4.37020, ne_loss -4.59635, align_loss 4.60517, 
Training completed.
training step completed
 29% 30/104 [04:31<03:05,  2.51s/it]iteration number -->  30
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.28846153846153844
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0008112980769230769
learning rate from cosine_annealing_LR -->  0.0008112980769230769
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:54:22] 00030, lr 0.00081, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([  33,  825,  712,  ...,  311, 1132,  901], device='cuda:0')
Generated mixing coefficient (lambda): 0.7151287286340898
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.719966888427734
Classification loss 2 during warmup: 4.769436359405518
Alignment loss during warmup: 4.6051764488220215
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.444619178771973
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.593358039855957
Contrastive Loss: tensor(6.5179, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.7200, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7694, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4446, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5934, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.5179, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.7200, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7694, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4446, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5934, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.5178542137146 cls_loss1: 9.719966888427734 cls_loss2: 4.769436359405518 ent_loss: 4.444619178771973 ne_loss: -4.593358039855957 align_loss: 4.6051764488220215
Total loss: 25.46369743347168
[2024-05-02 19:54:23] 00030, contrastive_loss 6.51785, cls_loss1 9.71997, cls_loss2 4.76944, ent_loss 4.44462, ne_loss -4.59336, align_loss 4.60518, 
Training completed.
training step completed
 30% 31/104 [04:33<03:05,  2.54s/it]iteration number -->  31
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.2980769230769231
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0008383413461538461
learning rate from cosine_annealing_LR -->  0.0008383413461538461
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:54:25] 00031, lr 0.00084, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 146,  716,  498,  ..., 1174, 1432, 1321], device='cuda:0')
Generated mixing coefficient (lambda): 0.30155168369520463
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.721797943115234
Classification loss 2 during warmup: 4.7588348388671875
Alignment loss during warmup: 4.605167865753174
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.445368766784668
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592160224914551
Contrastive Loss: tensor(6.5601, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.7218, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7588, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4454, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5922, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.5601, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.7218, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7588, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4454, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5922, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.560099124908447 cls_loss1: 9.721797943115234 cls_loss2: 4.7588348388671875 ent_loss: 4.445368766784668 ne_loss: -4.592160224914551 align_loss: 4.605167865753174
Total loss: 25.499107360839844
[2024-05-02 19:54:26] 00031, contrastive_loss 6.56010, cls_loss1 9.72180, cls_loss2 4.75883, ent_loss 4.44537, ne_loss -4.59216, align_loss 4.60517, 
Training completed.
training step completed
 31% 32/104 [04:36<03:04,  2.57s/it]iteration number -->  32
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.3076923076923077
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0008653846153846154
learning rate from cosine_annealing_LR -->  0.0008653846153846154
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:54:28] 00032, lr 0.00087, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 674,   49,  601,  ...,  183, 1181, 1028], device='cuda:0')
Generated mixing coefficient (lambda): 0.7016429835864404
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.621047973632812
Classification loss 2 during warmup: 4.784285545349121
Alignment loss during warmup: 4.6051716804504395
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.449672222137451
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591598987579346
Contrastive Loss: tensor(6.5305, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.6210, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7843, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4497, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5916, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.5305, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.6210, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7843, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4497, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5916, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.530545711517334 cls_loss1: 9.621047973632812 cls_loss2: 4.784285545349121 ent_loss: 4.449672222137451 ne_loss: -4.591598987579346 align_loss: 4.6051716804504395
Total loss: 25.399124145507812
[2024-05-02 19:54:28] 00032, contrastive_loss 6.53055, cls_loss1 9.62105, cls_loss2 4.78429, ent_loss 4.44967, ne_loss -4.59160, align_loss 4.60517, 
Training completed.
training step completed
 32% 33/104 [04:39<03:00,  2.55s/it]iteration number -->  33
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.3173076923076923
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0008924278846153844
learning rate from cosine_annealing_LR -->  0.0008924278846153844
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:54:30] 00033, lr 0.00089, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 389,  522, 1173,  ...,  544,   25,  128], device='cuda:0')
Generated mixing coefficient (lambda): 0.05983513758470556
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.578994750976562
Classification loss 2 during warmup: 4.94113826751709
Alignment loss during warmup: 4.605169773101807
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.412550449371338
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.595578670501709
Contrastive Loss: tensor(6.5177, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.5790, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9411, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4126, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5956, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.5177, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.5790, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9411, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4126, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5956, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.517709732055664 cls_loss1: 9.578994750976562 cls_loss2: 4.94113826751709 ent_loss: 4.412550449371338 ne_loss: -4.595578670501709 align_loss: 4.605169773101807
Total loss: 25.45998191833496


[2024-05-02 19:54:31] 00033, contrastive_loss 6.51771, cls_loss1 9.57899, cls_loss2 4.94114, ent_loss 4.41255, ne_loss -4.59558, align_loss 4.60517, 
Training completed.
training step completed
 33% 34/104 [04:41<02:57,  2.54s/it]iteration number -->  34
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.3269230769230769
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0009194711538461537
learning rate from cosine_annealing_LR -->  0.0009194711538461537
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:54:33] 00034, lr 0.00092, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1420,  284, 1424,  ..., 1045,   62,  807], device='cuda:0')
Generated mixing coefficient (lambda): 0.6947358508038756
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.577519416809082
Classification loss 2 during warmup: 4.763825416564941
Alignment loss during warmup: 4.6051716804504395
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.453988552093506
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591278076171875
Contrastive Loss: tensor(6.4822, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.5775, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7638, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4540, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5913, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4822, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.5775, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7638, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4540, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5913, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.482233047485352 cls_loss1: 9.577519416809082 cls_loss2: 4.763825416564941 ent_loss: 4.453988552093506 ne_loss: -4.591278076171875 align_loss: 4.6051716804504395
Total loss: 25.291460037231445
[2024-05-02 19:54:33] 00034, contrastive_loss 6.48223, cls_loss1 9.57752, cls_loss2 4.76383, ent_loss 4.45399, ne_loss -4.59128, align_loss 4.60517, 
Training completed.
training step completed
 34% 35/104 [04:44<02:54,  2.53s/it]iteration number -->  35
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.33653846153846156
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.000946514423076923
learning rate from cosine_annealing_LR -->  0.000946514423076923
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:54:35] 00035, lr 0.00095, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 819,  621, 1404,  ...,  464,  752, 1270], device='cuda:0')
Generated mixing coefficient (lambda): 0.3123275352017424
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.612197875976562
Classification loss 2 during warmup: 4.749396324157715
Alignment loss during warmup: 4.605169296264648
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.4597883224487305
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591073513031006
Contrastive Loss: tensor(6.6038, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.6122, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7494, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4598, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5911, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.6038, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.6122, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7494, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4598, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5911, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.6038079261779785 cls_loss1: 9.612197875976562 cls_loss2: 4.749396324157715 ent_loss: 4.4597883224487305 ne_loss: -4.591073513031006 align_loss: 4.605169296264648
Total loss: 25.439285278320312
[2024-05-02 19:54:36] 00035, contrastive_loss 6.60381, cls_loss1 9.61220, cls_loss2 4.74940, ent_loss 4.45979, ne_loss -4.59107, align_loss 4.60517, 
Training completed.
training step completed
 35% 36/104 [04:46<02:51,  2.52s/it]iteration number -->  36
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.34615384615384615
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0009735576923076922
learning rate from cosine_annealing_LR -->  0.0009735576923076922
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:54:38] 00036, lr 0.00097, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 287,  147, 1340,  ...,  199,  171, 1274], device='cuda:0')
Generated mixing coefficient (lambda): 0.20884572042828553
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.529096603393555
Classification loss 2 during warmup: 4.796717643737793
Alignment loss during warmup: 4.605167865753174
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.447994232177734
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592979431152344
Contrastive Loss: tensor(6.5445, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.5291, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7967, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4480, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5930, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.5445, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.5291, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7967, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4480, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5930, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.544486999511719 cls_loss1: 9.529096603393555 cls_loss2: 4.796717643737793 ent_loss: 4.447994232177734 ne_loss: -4.592979431152344 align_loss: 4.605167865753174
Total loss: 25.330482482910156
[2024-05-02 19:54:38] 00036, contrastive_loss 6.54449, cls_loss1 9.52910, cls_loss2 4.79672, ent_loss 4.44799, ne_loss -4.59298, align_loss 4.60517, 
Training completed.
training step completed
 36% 37/104 [04:49<02:49,  2.53s/it]iteration number -->  37
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.3557692307692308
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0010006009615384614
learning rate from cosine_annealing_LR -->  0.0010006009615384614
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:54:40] 00037, lr 0.00100, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1125, 1245,  214,  ..., 1430,  999,  222], device='cuda:0')
Generated mixing coefficient (lambda): 0.1274093315039185
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.526288032531738
Classification loss 2 during warmup: 4.895879745483398
Alignment loss during warmup: 4.60516881942749
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.4370222091674805
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.593865394592285
Contrastive Loss: tensor(6.4179, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.5263, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8959, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4370, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5939, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4179, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.5263, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8959, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4370, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5939, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.417895317077637 cls_loss1: 9.526288032531738 cls_loss2: 4.895879745483398 ent_loss: 4.4370222091674805 ne_loss: -4.593865394592285 align_loss: 4.60516881942749
Total loss: 25.288389205932617
[2024-05-02 19:54:41] 00037, contrastive_loss 6.41790, cls_loss1 9.52629, cls_loss2 4.89588, ent_loss 4.43702, ne_loss -4.59387, align_loss 4.60517, 
Training completed.
training step completed
 37% 38/104 [04:51<02:47,  2.54s/it]iteration number -->  38
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.36538461538461536
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0010276442307692306
learning rate from cosine_annealing_LR -->  0.0010276442307692306
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:54:43] 00038, lr 0.00103, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1251, 1171, 1387,  ...,  760,  701,  210], device='cuda:0')
Generated mixing coefficient (lambda): 0.854798041445862
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.518058776855469
Classification loss 2 during warmup: 4.843657970428467
Alignment loss during warmup: 4.605173110961914
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.439978122711182
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592713356018066
Contrastive Loss: tensor(6.4972, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.5181, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8437, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4400, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5927, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4972, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.5181, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8437, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4400, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5927, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.497195243835449 cls_loss1: 9.518058776855469 cls_loss2: 4.843657970428467 ent_loss: 4.439978122711182 ne_loss: -4.592713356018066 align_loss: 4.605173110961914
Total loss: 25.311349868774414
[2024-05-02 19:54:43] 00038, contrastive_loss 6.49720, cls_loss1 9.51806, cls_loss2 4.84366, ent_loss 4.43998, ne_loss -4.59271, align_loss 4.60517, 
Training completed.
training step completed
 38% 39/104 [04:54<02:44,  2.54s/it]iteration number -->  39
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.375
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0010546874999999999
learning rate from cosine_annealing_LR -->  0.0010546874999999999
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:54:45] 00039, lr 0.00105, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 520, 1114,  217,  ...,  715,  846,  997], device='cuda:0')
Generated mixing coefficient (lambda): 0.03151555454460563
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.496006965637207
Classification loss 2 during warmup: 4.915372848510742
Alignment loss during warmup: 4.605169296264648
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.425492286682129
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.594552993774414
Contrastive Loss: tensor(6.5319, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.4960, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9154, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4255, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5946, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.5319, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.4960, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9154, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4255, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5946, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.531854629516602 cls_loss1: 9.496006965637207 cls_loss2: 4.915372848510742 ent_loss: 4.425492286682129 ne_loss: -4.594552993774414 align_loss: 4.605169296264648
Total loss: 25.37934112548828
[2024-05-02 19:54:46] 00039, contrastive_loss 6.53185, cls_loss1 9.49601, cls_loss2 4.91537, ent_loss 4.42549, ne_loss -4.59455, align_loss 4.60517, 
Training completed.
training step completed
 38% 40/104 [04:56<02:41,  2.52s/it]iteration number -->  40
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.38461538461538464
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0010817307692307693
learning rate from cosine_annealing_LR -->  0.0010817307692307693
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:54:48] 00040, lr 0.00108, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 272, 1340,   30,  ..., 1030,  654,  897], device='cuda:0')
Generated mixing coefficient (lambda): 0.8096430518004919
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.563655853271484
Classification loss 2 during warmup: 4.834773540496826
Alignment loss during warmup: 4.605177402496338
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.452247619628906
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.59085750579834
Contrastive Loss: tensor(6.5371, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.5637, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8348, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4522, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5909, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.5371, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.5637, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8348, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4522, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5909, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.537095069885254 cls_loss1: 9.563655853271484 cls_loss2: 4.834773540496826 ent_loss: 4.452247619628906 ne_loss: -4.59085750579834 align_loss: 4.605177402496338
Total loss: 25.402090072631836
[2024-05-02 19:54:48] 00040, contrastive_loss 6.53710, cls_loss1 9.56366, cls_loss2 4.83477, ent_loss 4.45225, ne_loss -4.59086, align_loss 4.60518, 
Training completed.
training step completed
 39% 41/104 [04:59<02:38,  2.51s/it]iteration number -->  41
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.3942307692307692
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0011087740384615383
learning rate from cosine_annealing_LR -->  0.0011087740384615383
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:54:50] 00041, lr 0.00111, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1229,  804,  258,  ...,  801,  308,  968], device='cuda:0')
Generated mixing coefficient (lambda): 0.9055606749853524
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.476463317871094
Classification loss 2 during warmup: 4.883862495422363
Alignment loss during warmup: 4.605171203613281
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.429980754852295
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.59246301651001
Contrastive Loss: tensor(6.4971, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.4765, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8839, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4300, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5925, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4971, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.4765, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8839, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4300, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5925, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.497140407562256 cls_loss1: 9.476463317871094 cls_loss2: 4.883862495422363 ent_loss: 4.429980754852295 ne_loss: -4.59246301651001 align_loss: 4.605171203613281
Total loss: 25.300155639648438
[2024-05-02 19:54:51] 00041, contrastive_loss 6.49714, cls_loss1 9.47646, cls_loss2 4.88386, ent_loss 4.42998, ne_loss -4.59246, align_loss 4.60517, 
Training completed.
training step completed
 40% 42/104 [05:01<02:35,  2.51s/it]iteration number -->  42
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.40384615384615385
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0011358173076923077
learning rate from cosine_annealing_LR -->  0.0011358173076923077
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:54:53] 00042, lr 0.00114, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 659,  786,  192,  ...,  621, 1018,  814], device='cuda:0')
Generated mixing coefficient (lambda): 0.9152391684362483
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.423694610595703
Classification loss 2 during warmup: 4.842667579650879
Alignment loss during warmup: 4.605170249938965
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.437685012817383
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592807292938232
Contrastive Loss: tensor(6.4199, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.4237, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8427, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4377, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5928, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4199, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.4237, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8427, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4377, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5928, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.419891357421875 cls_loss1: 9.423694610595703 cls_loss2: 4.842667579650879 ent_loss: 4.437685012817383 ne_loss: -4.592807292938232 align_loss: 4.605170249938965
Total loss: 25.13629913330078
[2024-05-02 19:54:53] 00042, contrastive_loss 6.41989, cls_loss1 9.42369, cls_loss2 4.84267, ent_loss 4.43769, ne_loss -4.59281, align_loss 4.60517, 
Training completed.
training step completed
 41% 43/104 [05:04<02:32,  2.51s/it]iteration number -->  43
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.41346153846153844
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0011628605769230767
learning rate from cosine_annealing_LR -->  0.0011628605769230767
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:54:55] 00043, lr 0.00116, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 226, 1420,  722,  ...,  156,  572,  909], device='cuda:0')
Generated mixing coefficient (lambda): 0.9567554696582575
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.361934661865234
Classification loss 2 during warmup: 4.916895866394043
Alignment loss during warmup: 4.60516881942749
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.431939601898193
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.593026161193848
Contrastive Loss: tensor(6.4782, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3619, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9169, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4319, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5930, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4782, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3619, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9169, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4319, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5930, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.47820520401001 cls_loss1: 9.361934661865234 cls_loss2: 4.916895866394043 ent_loss: 4.431939601898193 ne_loss: -4.593026161193848 align_loss: 4.60516881942749
Total loss: 25.201120376586914
[2024-05-02 19:54:56] 00043, contrastive_loss 6.47821, cls_loss1 9.36193, cls_loss2 4.91690, ent_loss 4.43194, ne_loss -4.59303, align_loss 4.60517, 
Training completed.
training step completed
 42% 44/104 [05:06<02:30,  2.50s/it]iteration number -->  44
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.4230769230769231
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0011899038461538462
learning rate from cosine_annealing_LR -->  0.0011899038461538462
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:54:58] 00044, lr 0.00119, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1418,  754,  971,  ...,  718, 1242,  543], device='cuda:0')
Generated mixing coefficient (lambda): 0.20948855925138962
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.456378936767578
Classification loss 2 during warmup: 4.768767833709717
Alignment loss during warmup: 4.605165958404541
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.470645904541016
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5909199714660645
Contrastive Loss: tensor(6.4459, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.4564, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7688, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4706, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5909, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4459, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.4564, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7688, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4706, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5909, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.445889949798584 cls_loss1: 9.456378936767578 cls_loss2: 4.768767833709717 ent_loss: 4.470645904541016 ne_loss: -4.5909199714660645 align_loss: 4.605165958404541
Total loss: 25.155929565429688
[2024-05-02 19:54:58] 00044, contrastive_loss 6.44589, cls_loss1 9.45638, cls_loss2 4.76877, ent_loss 4.47065, ne_loss -4.59092, align_loss 4.60517, 
Training completed.
training step completed
 43% 45/104 [05:09<02:27,  2.50s/it]iteration number -->  45
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.4326923076923077
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0012169471153846154
learning rate from cosine_annealing_LR -->  0.0012169471153846154
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:55:00] 00045, lr 0.00122, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 106,  982, 1011,  ...,   55,  296,  206], device='cuda:0')
Generated mixing coefficient (lambda): 0.7081504442623683
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.385627746582031
Classification loss 2 during warmup: 4.715445518493652
Alignment loss during warmup: 4.605170726776123
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.494816303253174
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591399192810059
Contrastive Loss: tensor(6.5085, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3856, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7154, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4948, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5914, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.5085, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3856, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7154, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4948, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5914, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.508536338806152 cls_loss1: 9.385627746582031 cls_loss2: 4.715445518493652 ent_loss: 4.494816303253174 ne_loss: -4.591399192810059 align_loss: 4.605170726776123
Total loss: 25.11819839477539
[2024-05-02 19:55:01] 00045, contrastive_loss 6.50854, cls_loss1 9.38563, cls_loss2 4.71545, ent_loss 4.49482, ne_loss -4.59140, align_loss 4.60517, 
Training completed.
training step completed
 44% 46/104 [05:11<02:24,  2.48s/it]iteration number -->  46
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.4423076923076923
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0012439903846153844
learning rate from cosine_annealing_LR -->  0.0012439903846153844
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:55:03] 00046, lr 0.00124, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 960,  514,  681,  ...,  396, 1279, 1057], device='cuda:0')
Generated mixing coefficient (lambda): 0.6548990418241223
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.4336519241333
Classification loss 2 during warmup: 4.731293678283691
Alignment loss during warmup: 4.60516881942749
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.499328136444092
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.589512348175049
Contrastive Loss: tensor(6.4568, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.4337, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7313, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4993, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5895, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4568, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.4337, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7313, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4993, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5895, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.4568047523498535 cls_loss1: 9.4336519241333 cls_loss2: 4.731293678283691 ent_loss: 4.499328136444092 ne_loss: -4.589512348175049 align_loss: 4.60516881942749
Total loss: 25.136735916137695
[2024-05-02 19:55:03] 00046, contrastive_loss 6.45680, cls_loss1 9.43365, cls_loss2 4.73129, ent_loss 4.49933, ne_loss -4.58951, align_loss 4.60517, 
Training completed.
training step completed
 45% 47/104 [05:14<02:21,  2.47s/it]iteration number -->  47
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.4519230769230769
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0012710336538461536
learning rate from cosine_annealing_LR -->  0.0012710336538461536
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:55:05] 00047, lr 0.00127, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 568, 1180,  177,  ..., 1247,  921,  502], device='cuda:0')
Generated mixing coefficient (lambda): 0.4720387407503516
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.501489639282227
Classification loss 2 during warmup: 4.6904425621032715
Alignment loss during warmup: 4.605170249938965
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.519610404968262
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5926313400268555
Contrastive Loss: tensor(6.3439, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.5015, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6904, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5196, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5926, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3439, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.5015, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6904, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5196, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5926, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.343851089477539 cls_loss1: 9.501489639282227 cls_loss2: 4.6904425621032715 ent_loss: 4.519610404968262 ne_loss: -4.5926313400268555 align_loss: 4.605170249938965
Total loss: 25.06793212890625
[2024-05-02 19:55:06] 00047, contrastive_loss 6.34385, cls_loss1 9.50149, cls_loss2 4.69044, ent_loss 4.51961, ne_loss -4.59263, align_loss 4.60517, 
Training completed.
training step completed
 46% 48/104 [05:16<02:18,  2.47s/it]iteration number -->  48
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.46153846153846156
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.001298076923076923
learning rate from cosine_annealing_LR -->  0.001298076923076923
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:55:08] 00048, lr 0.00130, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([176, 405, 996,  ..., 413, 253, 927], device='cuda:0')
Generated mixing coefficient (lambda): 0.1178386778555579
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.397106170654297
Classification loss 2 during warmup: 4.855543613433838
Alignment loss during warmup: 4.605173110961914
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.472331523895264
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.593212127685547
Contrastive Loss: tensor(6.4496, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3971, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8555, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4723, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5932, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4496, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3971, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8555, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4723, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5932, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.449647903442383 cls_loss1: 9.397106170654297 cls_loss2: 4.855543613433838 ent_loss: 4.472331523895264 ne_loss: -4.593212127685547 align_loss: 4.605173110961914
Total loss: 25.18659019470215
[2024-05-02 19:55:08] 00048, contrastive_loss 6.44965, cls_loss1 9.39711, cls_loss2 4.85554, ent_loss 4.47233, ne_loss -4.59321, align_loss 4.60517, 
Training completed.
training step completed
 47% 49/104 [05:19<02:15,  2.46s/it]iteration number -->  49
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.47115384615384615
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.001325120192307692
learning rate from cosine_annealing_LR -->  0.001325120192307692
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:55:10] 00049, lr 0.00133, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 551,  291, 1274,  ...,  972, 1357,   65], device='cuda:0')
Generated mixing coefficient (lambda): 0.6777574238898412
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.378005027770996
Classification loss 2 during warmup: 4.700229167938232
Alignment loss during warmup: 4.6051716804504395
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.514970302581787
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592459678649902
Contrastive Loss: tensor(6.4522, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3780, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7002, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5150, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5925, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4522, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3780, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7002, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5150, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5925, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.452236652374268 cls_loss1: 9.378005027770996 cls_loss2: 4.700229167938232 ent_loss: 4.514970302581787 ne_loss: -4.592459678649902 align_loss: 4.6051716804504395
Total loss: 25.058151245117188
[2024-05-02 19:55:11] 00049, contrastive_loss 6.45224, cls_loss1 9.37801, cls_loss2 4.70023, ent_loss 4.51497, ne_loss -4.59246, align_loss 4.60517, 
Training completed.
training step completed
 48% 50/104 [05:21<02:13,  2.47s/it]iteration number -->  50
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.4807692307692308
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0013521634615384615
learning rate from cosine_annealing_LR -->  0.0013521634615384615
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:55:12] 00050, lr 0.00135, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1296,  168,  710,  ...,  845,  682,  788], device='cuda:0')
Generated mixing coefficient (lambda): 0.26845609374919954
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.359954833984375
Classification loss 2 during warmup: 4.73436164855957
Alignment loss during warmup: 4.605166912078857
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.505537986755371
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5921173095703125
Contrastive Loss: tensor(6.4064, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3600, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7344, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5055, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5921, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4064, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3600, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7344, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5055, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5921, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.406398296356201 cls_loss1: 9.359954833984375 cls_loss2: 4.73436164855957 ent_loss: 4.505537986755371 ne_loss: -4.5921173095703125 align_loss: 4.605166912078857
Total loss: 25.019302368164062
[2024-05-02 19:55:13] 00050, contrastive_loss 6.40640, cls_loss1 9.35995, cls_loss2 4.73436, ent_loss 4.50554, ne_loss -4.59212, align_loss 4.60517, 
Training completed.
training step completed
 49% 51/104 [05:23<02:10,  2.47s/it]iteration number -->  51
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.49038461538461536
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0013792067307692305
learning rate from cosine_annealing_LR -->  0.0013792067307692305
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:55:15] 00051, lr 0.00138, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 632, 1438,    0,  ..., 1302, 1288,  133], device='cuda:0')
Generated mixing coefficient (lambda): 0.7157575432612416
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.437143325805664
Classification loss 2 during warmup: 4.709994316101074
Alignment loss during warmup: 4.605169296264648
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.505733013153076
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591486930847168
Contrastive Loss: tensor(6.4999, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.4371, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7100, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5057, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5915, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4999, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.4371, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7100, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5057, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5915, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.499870300292969 cls_loss1: 9.437143325805664 cls_loss2: 4.709994316101074 ent_loss: 4.505733013153076 ne_loss: -4.591486930847168 align_loss: 4.605169296264648
Total loss: 25.16642189025879
[2024-05-02 19:55:16] 00051, contrastive_loss 6.49987, cls_loss1 9.43714, cls_loss2 4.70999, ent_loss 4.50573, ne_loss -4.59149, align_loss 4.60517, 
Training completed.
training step completed
 50% 52/104 [05:26<02:08,  2.46s/it]iteration number -->  52
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.5
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.00140625
learning rate from cosine_annealing_LR -->  0.00140625
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:55:17] 00052, lr 0.00141, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([  14,  277, 1365,  ...,  904, 1162,  695], device='cuda:0')
Generated mixing coefficient (lambda): 0.026508617968777782
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.424413681030273
Classification loss 2 during warmup: 4.866837024688721
Alignment loss during warmup: 4.605168342590332
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.453388690948486
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592679977416992
Contrastive Loss: tensor(6.4576, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.4244, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8668, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4534, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5927, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4576, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.4244, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8668, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4534, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5927, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.457583904266357 cls_loss1: 9.424413681030273 cls_loss2: 4.866837024688721 ent_loss: 4.453388690948486 ne_loss: -4.592679977416992 align_loss: 4.605168342590332
Total loss: 25.214710235595703
[2024-05-02 19:55:18] 00052, contrastive_loss 6.45758, cls_loss1 9.42441, cls_loss2 4.86684, ent_loss 4.45339, ne_loss -4.59268, align_loss 4.60517, 
Training completed.
training step completed
 51% 53/104 [05:28<02:05,  2.46s/it]iteration number -->  53
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.5096153846153846
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.001433293269230769
learning rate from cosine_annealing_LR -->  0.001433293269230769
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:55:20] 00053, lr 0.00143, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 287,    1,  272,  ..., 1409,  152,  273], device='cuda:0')
Generated mixing coefficient (lambda): 0.45490515074934373
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.590309143066406
Classification loss 2 during warmup: 4.700329780578613
Alignment loss during warmup: 4.605169773101807
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.527904510498047
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591590881347656
Contrastive Loss: tensor(6.3761, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.5903, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7003, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5279, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5916, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3761, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.5903, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7003, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5279, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5916, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.376075267791748 cls_loss1: 9.590309143066406 cls_loss2: 4.700329780578613 ent_loss: 4.527904510498047 ne_loss: -4.591590881347656 align_loss: 4.605169773101807
Total loss: 25.20819664001465
[2024-05-02 19:55:21] 00053, contrastive_loss 6.37608, cls_loss1 9.59031, cls_loss2 4.70033, ent_loss 4.52790, ne_loss -4.59159, align_loss 4.60517, 
Training completed.
training step completed
 52% 54/104 [05:31<02:02,  2.45s/it]iteration number -->  54
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.5192307692307693
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0014603365384615384
learning rate from cosine_annealing_LR -->  0.0014603365384615384
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:55:22] 00054, lr 0.00146, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1334,  614,  681,  ...,  451,  732,  748], device='cuda:0')
Generated mixing coefficient (lambda): 0.7613414807918842
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.429737091064453
Classification loss 2 during warmup: 4.7325921058654785
Alignment loss during warmup: 4.605170249938965
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.506185054779053
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592257022857666
Contrastive Loss: tensor(6.4159, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.4297, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7326, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5062, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5923, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4159, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.4297, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7326, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5062, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5923, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.415855407714844 cls_loss1: 9.429737091064453 cls_loss2: 4.7325921058654785 ent_loss: 4.506185054779053 ne_loss: -4.592257022857666 align_loss: 4.605170249938965
Total loss: 25.09728240966797
[2024-05-02 19:55:23] 00054, contrastive_loss 6.41586, cls_loss1 9.42974, cls_loss2 4.73259, ent_loss 4.50619, ne_loss -4.59226, align_loss 4.60517, 
Training completed.
training step completed
 53% 55/104 [05:33<02:00,  2.46s/it]iteration number -->  55
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.5288461538461539
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0014873798076923076
learning rate from cosine_annealing_LR -->  0.0014873798076923076
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:55:25] 00055, lr 0.00149, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1289,  764, 1366,  ...,  512,   58, 1189], device='cuda:0')
Generated mixing coefficient (lambda): 0.5434771005421039
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.39013957977295
Classification loss 2 during warmup: 4.698639392852783
Alignment loss during warmup: 4.605170249938965
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.5216474533081055
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.589171886444092
Contrastive Loss: tensor(6.4280, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3901, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6986, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5216, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5892, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4280, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3901, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6986, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5216, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5892, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.428040027618408 cls_loss1: 9.39013957977295 cls_loss2: 4.698639392852783 ent_loss: 4.5216474533081055 ne_loss: -4.589171886444092 align_loss: 4.605170249938965
Total loss: 25.054466247558594
[2024-05-02 19:55:25] 00055, contrastive_loss 6.42804, cls_loss1 9.39014, cls_loss2 4.69864, ent_loss 4.52165, ne_loss -4.58917, align_loss 4.60517, 
Training completed.
training step completed
 54% 56/104 [05:36<01:58,  2.46s/it]iteration number -->  56
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.5384615384615384
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0015144230769230768
learning rate from cosine_annealing_LR -->  0.0015144230769230768
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:55:27] 00056, lr 0.00151, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 337,  964,  688,  ...,  143, 1373, 1413], device='cuda:0')
Generated mixing coefficient (lambda): 0.952814040170235
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.348225593566895
Classification loss 2 during warmup: 4.875415802001953
Alignment loss during warmup: 4.605170249938965
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.464447498321533
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591960906982422
Contrastive Loss: tensor(6.3875, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3482, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8754, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4644, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5920, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3875, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3482, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8754, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4644, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5920, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.387486457824707 cls_loss1: 9.348225593566895 cls_loss2: 4.875415802001953 ent_loss: 4.464447498321533 ne_loss: -4.591960906982422 align_loss: 4.605170249938965
Total loss: 25.088783264160156
[2024-05-02 19:55:28] 00056, contrastive_loss 6.38749, cls_loss1 9.34823, cls_loss2 4.87542, ent_loss 4.46445, ne_loss -4.59196, align_loss 4.60517, 
Training completed.
training step completed
 55% 57/104 [05:38<01:55,  2.47s/it]iteration number -->  57
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.5480769230769231
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0015414663461538463
learning rate from cosine_annealing_LR -->  0.0015414663461538463
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:55:30] 00057, lr 0.00154, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1010, 1087,  335,  ..., 1299, 1217, 1393], device='cuda:0')
Generated mixing coefficient (lambda): 0.32819466244828815
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.424629211425781
Classification loss 2 during warmup: 4.700747966766357
Alignment loss during warmup: 4.605170249938965
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.522930145263672
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5921783447265625
Contrastive Loss: tensor(6.4043, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.4246, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7007, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5229, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5922, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4043, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.4246, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7007, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5229, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5922, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.4042792320251465 cls_loss1: 9.424629211425781 cls_loss2: 4.700747966766357 ent_loss: 4.522930145263672 ne_loss: -4.5921783447265625 align_loss: 4.605170249938965
Total loss: 25.06557846069336
[2024-05-02 19:55:30] 00057, contrastive_loss 6.40428, cls_loss1 9.42463, cls_loss2 4.70075, ent_loss 4.52293, ne_loss -4.59218, align_loss 4.60517, 
Training completed.
training step completed
 56% 58/104 [05:41<01:53,  2.47s/it]iteration number -->  58
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.5576923076923077
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0015685096153846153
learning rate from cosine_annealing_LR -->  0.0015685096153846153
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:55:32] 00058, lr 0.00157, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1233,  209,  251,  ..., 1352,  672,  102], device='cuda:0')
Generated mixing coefficient (lambda): 0.4466392512993226
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.388874053955078
Classification loss 2 during warmup: 4.6777801513671875
Alignment loss during warmup: 4.605172634124756
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.536094665527344
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.59248685836792
Contrastive Loss: tensor(6.3726, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3889, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6778, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5361, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5925, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3726, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3889, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6778, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5361, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5925, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.372551441192627 cls_loss1: 9.388874053955078 cls_loss2: 4.6777801513671875 ent_loss: 4.536094665527344 ne_loss: -4.59248685836792 align_loss: 4.605172634124756
Total loss: 24.987985610961914
[2024-05-02 19:55:33] 00058, contrastive_loss 6.37255, cls_loss1 9.38887, cls_loss2 4.67778, ent_loss 4.53609, ne_loss -4.59249, align_loss 4.60517, 
Training completed.
training step completed
 57% 59/104 [05:43<01:50,  2.46s/it]iteration number -->  59
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.5673076923076923
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0015955528846153845
learning rate from cosine_annealing_LR -->  0.0015955528846153845
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:55:35] 00059, lr 0.00160, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1345,   94,   82,  ..., 1417,  185,  433], device='cuda:0')
Generated mixing coefficient (lambda): 0.3211759764739396
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.323939323425293
Classification loss 2 during warmup: 4.709095001220703
Alignment loss during warmup: 4.605169773101807
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.523148536682129
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.590883255004883
Contrastive Loss: tensor(6.3879, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3239, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7091, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5231, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5909, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3879, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3239, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7091, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5231, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5909, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.387905120849609 cls_loss1: 9.323939323425293 cls_loss2: 4.709095001220703 ent_loss: 4.523148536682129 ne_loss: -4.590883255004883 align_loss: 4.605169773101807
Total loss: 24.9583740234375
[2024-05-02 19:55:35] 00059, contrastive_loss 6.38791, cls_loss1 9.32394, cls_loss2 4.70910, ent_loss 4.52315, ne_loss -4.59088, align_loss 4.60517, 
Training completed.
training step completed
 58% 60/104 [05:46<01:48,  2.46s/it]iteration number -->  60
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.5769230769230769
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0016225961538461537
learning rate from cosine_annealing_LR -->  0.0016225961538461537
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:55:37] 00060, lr 0.00162, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1132, 1404,  182,  ..., 1239,  356, 1031], device='cuda:0')
Generated mixing coefficient (lambda): 0.8802390895271353
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.344717025756836
Classification loss 2 during warmup: 4.832391262054443
Alignment loss during warmup: 4.605172634124756
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.481468200683594
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.590440273284912
Contrastive Loss: tensor(6.3641, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3447, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8324, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4815, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5904, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3641, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3447, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8324, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4815, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5904, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.364139080047607 cls_loss1: 9.344717025756836 cls_loss2: 4.832391262054443 ent_loss: 4.481468200683594 ne_loss: -4.590440273284912 align_loss: 4.605172634124756
Total loss: 25.03744888305664
[2024-05-02 19:55:38] 00060, contrastive_loss 6.36414, cls_loss1 9.34472, cls_loss2 4.83239, ent_loss 4.48147, ne_loss -4.59044, align_loss 4.60517, 
Training completed.
training step completed
 59% 61/104 [05:48<01:45,  2.46s/it]iteration number -->  61
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.5865384615384616
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0016496394230769232
learning rate from cosine_annealing_LR -->  0.0016496394230769232
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:55:39] 00061, lr 0.00165, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 755, 1200,  228,  ..., 1319, 1404, 1184], device='cuda:0')
Generated mixing coefficient (lambda): 0.796645356851158
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.366592407226562
Classification loss 2 during warmup: 4.7982025146484375
Alignment loss during warmup: 4.605173110961914
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.499765872955322
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591546058654785
Contrastive Loss: tensor(6.2995, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3666, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7982, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4998, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5915, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2995, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3666, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7982, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4998, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5915, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.299537658691406 cls_loss1: 9.366592407226562 cls_loss2: 4.7982025146484375 ent_loss: 4.499765872955322 ne_loss: -4.591546058654785 align_loss: 4.605173110961914
Total loss: 24.977724075317383
[2024-05-02 19:55:40] 00061, contrastive_loss 6.29954, cls_loss1 9.36659, cls_loss2 4.79820, ent_loss 4.49977, ne_loss -4.59155, align_loss 4.60517, 
Training completed.
training step completed
 60% 62/104 [05:50<01:43,  2.45s/it]iteration number -->  62
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.5961538461538461
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0016766826923076922
learning rate from cosine_annealing_LR -->  0.0016766826923076922
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:55:42] 00062, lr 0.00168, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1172, 1392, 1109,  ...,  185,  821,  419], device='cuda:0')
Generated mixing coefficient (lambda): 0.7552029448787363
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.49018383026123
Classification loss 2 during warmup: 4.742636680603027
Alignment loss during warmup: 4.605170249938965
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.511568069458008
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591677188873291
Contrastive Loss: tensor(6.3122, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.4902, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7426, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5116, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5917, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3122, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.4902, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7426, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5116, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5917, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.312234878540039 cls_loss1: 9.49018383026123 cls_loss2: 4.742636680603027 ent_loss: 4.511568069458008 ne_loss: -4.591677188873291 align_loss: 4.605170249938965
Total loss: 25.070117950439453
[2024-05-02 19:55:43] 00062, contrastive_loss 6.31223, cls_loss1 9.49018, cls_loss2 4.74264, ent_loss 4.51157, ne_loss -4.59168, align_loss 4.60517, 
Training completed.
training step completed
 61% 63/104 [05:53<01:40,  2.46s/it]iteration number -->  63
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.6057692307692307
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0017037259615384614
learning rate from cosine_annealing_LR -->  0.0017037259615384614
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:55:44] 00063, lr 0.00170, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([751, 924, 323,  ...,  98, 169,  62], device='cuda:0')
Generated mixing coefficient (lambda): 0.22886211841076604
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.337991714477539
Classification loss 2 during warmup: 4.752597332000732
Alignment loss during warmup: 4.605171203613281
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.507568359375
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.589906692504883
Contrastive Loss: tensor(6.3329, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3380, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7526, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5076, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5899, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3329, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3380, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7526, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5076, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5899, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.332892894744873 cls_loss1: 9.337991714477539 cls_loss2: 4.752597332000732 ent_loss: 4.507568359375 ne_loss: -4.589906692504883 align_loss: 4.605171203613281
Total loss: 24.94631576538086
[2024-05-02 19:55:45] 00063, contrastive_loss 6.33289, cls_loss1 9.33799, cls_loss2 4.75260, ent_loss 4.50757, ne_loss -4.58991, align_loss 4.60517, 
Training completed.
training step completed
 62% 64/104 [05:55<01:38,  2.47s/it]iteration number -->  64
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.6153846153846154
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0017307692307692308
learning rate from cosine_annealing_LR -->  0.0017307692307692308
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:55:47] 00064, lr 0.00173, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([732, 553, 793,  ..., 745, 519, 642], device='cuda:0')
Generated mixing coefficient (lambda): 0.915490962154327
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.35487174987793
Classification loss 2 during warmup: 4.845052719116211
Alignment loss during warmup: 4.605169773101807
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.4798712730407715
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5913238525390625
Contrastive Loss: tensor(6.3525, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3549, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8451, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4799, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5913, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3525, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3549, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8451, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4799, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5913, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.352512836456299 cls_loss1: 9.35487174987793 cls_loss2: 4.845052719116211 ent_loss: 4.4798712730407715 ne_loss: -4.5913238525390625 align_loss: 4.605169773101807
Total loss: 25.046154022216797
[2024-05-02 19:55:48] 00064, contrastive_loss 6.35251, cls_loss1 9.35487, cls_loss2 4.84505, ent_loss 4.47987, ne_loss -4.59132, align_loss 4.60517, 
Training completed.
training step completed
 62% 65/104 [05:58<01:36,  2.46s/it]iteration number -->  65
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.625
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0017578125
learning rate from cosine_annealing_LR -->  0.0017578125
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:55:49] 00065, lr 0.00176, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1334, 1256, 1279,  ..., 1367,  945,  780], device='cuda:0')
Generated mixing coefficient (lambda): 0.70598867872455
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.32016372680664
Classification loss 2 during warmup: 4.725096702575684
Alignment loss during warmup: 4.605170726776123
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.519961833953857
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.589803695678711
Contrastive Loss: tensor(6.3517, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3202, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7251, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5200, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5898, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3517, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3202, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7251, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5200, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5898, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.351701736450195 cls_loss1: 9.32016372680664 cls_loss2: 4.725096702575684 ent_loss: 4.519961833953857 ne_loss: -4.589803695678711 align_loss: 4.605170726776123
Total loss: 24.93229103088379
[2024-05-02 19:55:50] 00065, contrastive_loss 6.35170, cls_loss1 9.32016, cls_loss2 4.72510, ent_loss 4.51996, ne_loss -4.58980, align_loss 4.60517, 
Training completed.
training step completed
 63% 66/104 [06:00<01:33,  2.46s/it]iteration number -->  66
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.6346153846153846
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0017848557692307688
learning rate from cosine_annealing_LR -->  0.0017848557692307688
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:55:52] 00066, lr 0.00178, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1246, 1011,  901,  ..., 1211,  228, 1428], device='cuda:0')
Generated mixing coefficient (lambda): 0.017368226871032337
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.267797470092773
Classification loss 2 during warmup: 4.824526309967041
Alignment loss during warmup: 4.605170249938965
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.473098278045654
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591934680938721
Contrastive Loss: tensor(6.3209, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2678, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8245, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4731, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5919, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3209, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2678, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8245, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4731, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5919, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.320850372314453 cls_loss1: 9.267797470092773 cls_loss2: 4.824526309967041 ent_loss: 4.473098278045654 ne_loss: -4.591934680938721 align_loss: 4.605170249938965
Total loss: 24.89950942993164
[2024-05-02 19:55:53] 00066, contrastive_loss 6.32085, cls_loss1 9.26780, cls_loss2 4.82453, ent_loss 4.47310, ne_loss -4.59193, align_loss 4.60517, 
Training completed.
training step completed
 64% 67/104 [06:03<01:30,  2.46s/it]iteration number -->  67
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.6442307692307693
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0018118990384615383
learning rate from cosine_annealing_LR -->  0.0018118990384615383
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:55:54] 00067, lr 0.00181, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1266,  409, 1113,  ...,   73,  610,  652], device='cuda:0')
Generated mixing coefficient (lambda): 0.3245214107212053
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.274225234985352
Classification loss 2 during warmup: 4.735126972198486
Alignment loss during warmup: 4.605166912078857
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.5212273597717285
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.589644432067871
Contrastive Loss: tensor(6.3392, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2742, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7351, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5212, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5896, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3392, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2742, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7351, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5212, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5896, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.339184284210205 cls_loss1: 9.274225234985352 cls_loss2: 4.735126972198486 ent_loss: 4.5212273597717285 ne_loss: -4.589644432067871 align_loss: 4.605166912078857
Total loss: 24.885284423828125
[2024-05-02 19:55:55] 00067, contrastive_loss 6.33918, cls_loss1 9.27423, cls_loss2 4.73513, ent_loss 4.52123, ne_loss -4.58964, align_loss 4.60517, 
Training completed.
training step completed
 65% 68/104 [06:05<01:28,  2.46s/it]iteration number -->  68
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.6538461538461539
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0018389423076923075
learning rate from cosine_annealing_LR -->  0.0018389423076923075
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:55:57] 00068, lr 0.00184, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 234,  331,  826,  ..., 1353,  408, 1285], device='cuda:0')
Generated mixing coefficient (lambda): 0.1163705612685398
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.347591400146484
Classification loss 2 during warmup: 4.809396266937256
Alignment loss during warmup: 4.605169773101807
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.491380214691162
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5914177894592285
Contrastive Loss: tensor(6.3828, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3476, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8094, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4914, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5914, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3828, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3476, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8094, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4914, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5914, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.3828253746032715 cls_loss1: 9.347591400146484 cls_loss2: 4.809396266937256 ent_loss: 4.491380214691162 ne_loss: -4.5914177894592285 align_loss: 4.605169773101807
Total loss: 25.044946670532227
[2024-05-02 19:55:57] 00068, contrastive_loss 6.38283, cls_loss1 9.34759, cls_loss2 4.80940, ent_loss 4.49138, ne_loss -4.59142, align_loss 4.60517, 
Training completed.
training step completed
 66% 69/104 [06:08<01:26,  2.46s/it]iteration number -->  69
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.6634615384615384
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0018659855769230765
learning rate from cosine_annealing_LR -->  0.0018659855769230765
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:55:59] 00069, lr 0.00187, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1380,  293, 1034,  ...,   58,  244,   71], device='cuda:0')
Generated mixing coefficient (lambda): 0.9790261867049342
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.288299560546875
Classification loss 2 during warmup: 4.90798807144165
Alignment loss during warmup: 4.605171203613281
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.469396114349365
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.590363502502441
Contrastive Loss: tensor(6.3669, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2883, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9080, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4694, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5904, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3669, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2883, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9080, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4694, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5904, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.366899490356445 cls_loss1: 9.288299560546875 cls_loss2: 4.90798807144165 ent_loss: 4.469396114349365 ne_loss: -4.590363502502441 align_loss: 4.605171203613281
Total loss: 25.04738998413086
[2024-05-02 19:56:00] 00069, contrastive_loss 6.36690, cls_loss1 9.28830, cls_loss2 4.90799, ent_loss 4.46940, ne_loss -4.59036, align_loss 4.60517, 
Training completed.
training step completed
 67% 70/104 [06:10<01:24,  2.47s/it]iteration number -->  70
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.6730769230769231
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.001893028846153846
learning rate from cosine_annealing_LR -->  0.001893028846153846
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:56:02] 00070, lr 0.00189, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 237,  445,  557,  ...,  833, 1073,  936], device='cuda:0')
Generated mixing coefficient (lambda): 0.700951709931014
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.338882446289062
Classification loss 2 during warmup: 4.71455717086792
Alignment loss during warmup: 4.60516881942749
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.520644664764404
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.589560508728027
Contrastive Loss: tensor(6.2875, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3389, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7146, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5206, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5896, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2875, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3389, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7146, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5206, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5896, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.287511825561523 cls_loss1: 9.338882446289062 cls_loss2: 4.71455717086792 ent_loss: 4.520644664764404 ne_loss: -4.589560508728027 align_loss: 4.60516881942749
Total loss: 24.877206802368164
[2024-05-02 19:56:02] 00070, contrastive_loss 6.28751, cls_loss1 9.33888, cls_loss2 4.71456, ent_loss 4.52064, ne_loss -4.58956, align_loss 4.60517, 
Training completed.
training step completed
 68% 71/104 [06:13<01:21,  2.47s/it]iteration number -->  71
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.6826923076923077
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0019200721153846152
learning rate from cosine_annealing_LR -->  0.0019200721153846152
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:56:04] 00071, lr 0.00192, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 597,  532,  307,  ...,  349, 1143,  363], device='cuda:0')
Generated mixing coefficient (lambda): 0.729170391254229
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.260300636291504
Classification loss 2 during warmup: 4.7064995765686035
Alignment loss during warmup: 4.605165958404541
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.524291038513184
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591440677642822
Contrastive Loss: tensor(6.4131, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2603, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7065, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5243, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5914, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4131, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2603, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7065, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5243, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5914, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.413093090057373 cls_loss1: 9.260300636291504 cls_loss2: 4.7064995765686035 ent_loss: 4.524291038513184 ne_loss: -4.591440677642822 align_loss: 4.605165958404541
Total loss: 24.917911529541016
[2024-05-02 19:56:05] 00071, contrastive_loss 6.41309, cls_loss1 9.26030, cls_loss2 4.70650, ent_loss 4.52429, ne_loss -4.59144, align_loss 4.60517, 
Training completed.
training step completed
 69% 72/104 [06:15<01:18,  2.47s/it]iteration number -->  72
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.6923076923076923
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0019471153846153844
learning rate from cosine_annealing_LR -->  0.0019471153846153844
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:56:07] 00072, lr 0.00195, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([378, 945, 652,  ..., 959, 834, 373], device='cuda:0')
Generated mixing coefficient (lambda): 0.6950706253342843
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.34308910369873
Classification loss 2 during warmup: 4.718169689178467
Alignment loss during warmup: 4.605170726776123
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.523746967315674
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.589326858520508
Contrastive Loss: tensor(6.3216, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3431, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7182, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5237, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5893, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3216, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3431, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7182, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5237, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5893, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.321586608886719 cls_loss1: 9.34308910369873 cls_loss2: 4.718169689178467 ent_loss: 4.523746967315674 ne_loss: -4.589326858520508 align_loss: 4.605170726776123
Total loss: 24.922435760498047
[2024-05-02 19:56:07] 00072, contrastive_loss 6.32159, cls_loss1 9.34309, cls_loss2 4.71817, ent_loss 4.52375, ne_loss -4.58933, align_loss 4.60517, 
Training completed.
training step completed
 70% 73/104 [06:18<01:16,  2.46s/it]iteration number -->  73
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.7019230769230769
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0019741586538461536
learning rate from cosine_annealing_LR -->  0.0019741586538461536
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:56:09] 00073, lr 0.00197, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([  49,  122, 1168,  ...,  527,  518,   87], device='cuda:0')
Generated mixing coefficient (lambda): 0.101917515904392
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.29182243347168
Classification loss 2 during warmup: 4.806026458740234
Alignment loss during warmup: 4.60516881942749
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.4930524826049805
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.590889930725098
Contrastive Loss: tensor(6.2644, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2918, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8060, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4931, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5909, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2644, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2918, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8060, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4931, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5909, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.264358997344971 cls_loss1: 9.29182243347168 cls_loss2: 4.806026458740234 ent_loss: 4.4930524826049805 ne_loss: -4.590889930725098 align_loss: 4.60516881942749
Total loss: 24.869539260864258
[2024-05-02 19:56:10] 00073, contrastive_loss 6.26436, cls_loss1 9.29182, cls_loss2 4.80603, ent_loss 4.49305, ne_loss -4.59089, align_loss 4.60517, 
Training completed.
training step completed
 71% 74/104 [06:20<01:13,  2.46s/it]iteration number -->  74
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.7115384615384616
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002001201923076923
learning rate from cosine_annealing_LR -->  0.002001201923076923
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:56:12] 00074, lr 0.00200, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 971, 1166, 1406,  ...,  564, 1058,  293], device='cuda:0')
Generated mixing coefficient (lambda): 0.8042195123870479
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.359687805175781
Classification loss 2 during warmup: 4.772107124328613
Alignment loss during warmup: 4.605171203613281
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.504776477813721
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.590568542480469
Contrastive Loss: tensor(6.3377, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3597, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7721, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5048, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5906, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3377, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3597, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7721, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5048, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5906, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.337703704833984 cls_loss1: 9.359687805175781 cls_loss2: 4.772107124328613 ent_loss: 4.504776477813721 ne_loss: -4.590568542480469 align_loss: 4.605171203613281
Total loss: 24.988876342773438
[2024-05-02 19:56:12] 00074, contrastive_loss 6.33770, cls_loss1 9.35969, cls_loss2 4.77211, ent_loss 4.50478, ne_loss -4.59057, align_loss 4.60517, 
Training completed.
training step completed
 72% 75/104 [06:23<01:11,  2.47s/it]iteration number -->  75
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.7211538461538461
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002028245192307692
learning rate from cosine_annealing_LR -->  0.002028245192307692
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:56:14] 00075, lr 0.00203, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 396,  627, 1373,  ...,  591, 1407, 1328], device='cuda:0')
Generated mixing coefficient (lambda): 0.6368364231777892
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.318666458129883
Classification loss 2 during warmup: 4.710157871246338
Alignment loss during warmup: 4.605168342590332
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.535542964935303
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.590509414672852
Contrastive Loss: tensor(6.3110, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3187, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7102, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5355, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5905, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3110, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3187, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7102, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5355, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5905, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.311013698577881 cls_loss1: 9.318666458129883 cls_loss2: 4.710157871246338 ent_loss: 4.535542964935303 ne_loss: -4.590509414672852 align_loss: 4.605168342590332
Total loss: 24.89004135131836
[2024-05-02 19:56:15] 00075, contrastive_loss 6.31101, cls_loss1 9.31867, cls_loss2 4.71016, ent_loss 4.53554, ne_loss -4.59051, align_loss 4.60517, 
Training completed.
training step completed
 73% 76/104 [06:25<01:09,  2.47s/it]iteration number -->  76
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.7307692307692307
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0020552884615384613
learning rate from cosine_annealing_LR -->  0.0020552884615384613
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:56:16] 00076, lr 0.00206, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1320,  563,  962,  ...,    2,  816,   44], device='cuda:0')
Generated mixing coefficient (lambda): 0.7126254766577547
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.333453178405762
Classification loss 2 during warmup: 4.72798490524292
Alignment loss during warmup: 4.605173110961914
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.52764892578125
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.590173244476318
Contrastive Loss: tensor(6.3193, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3335, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7280, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5276, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5902, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3193, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3335, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7280, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5276, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5902, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.319338321685791 cls_loss1: 9.333453178405762 cls_loss2: 4.72798490524292 ent_loss: 4.52764892578125 ne_loss: -4.590173244476318 align_loss: 4.605173110961914
Total loss: 24.923423767089844
[2024-05-02 19:56:17] 00076, contrastive_loss 6.31934, cls_loss1 9.33345, cls_loss2 4.72798, ent_loss 4.52765, ne_loss -4.59017, align_loss 4.60517, 
Training completed.
training step completed
 74% 77/104 [06:28<01:06,  2.47s/it]iteration number -->  77
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.7403846153846154
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0020823317307692305
learning rate from cosine_annealing_LR -->  0.0020823317307692305
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:56:19] 00077, lr 0.00208, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 299,  727,  629,  ...,  942, 1260,  534], device='cuda:0')
Generated mixing coefficient (lambda): 0.9844950878373382
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.278416633605957
Classification loss 2 during warmup: 4.865723609924316
Alignment loss during warmup: 4.605172634124756
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.488786220550537
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592757225036621
Contrastive Loss: tensor(6.2463, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2784, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8657, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4888, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5928, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2463, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2784, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8657, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4888, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5928, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.246307373046875 cls_loss1: 9.278416633605957 cls_loss2: 4.865723609924316 ent_loss: 4.488786220550537 ne_loss: -4.592757225036621 align_loss: 4.605172634124756
Total loss: 24.89164924621582
[2024-05-02 19:56:20] 00077, contrastive_loss 6.24631, cls_loss1 9.27842, cls_loss2 4.86572, ent_loss 4.48879, ne_loss -4.59276, align_loss 4.60517, 
Training completed.
training step completed
 75% 78/104 [06:30<01:04,  2.47s/it]iteration number -->  78
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.75
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0021093749999999997
learning rate from cosine_annealing_LR -->  0.0021093749999999997
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:56:21] 00078, lr 0.00211, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 650, 1234, 1028,  ...,  529, 1169,   80], device='cuda:0')
Generated mixing coefficient (lambda): 0.8398449502687155
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.254644393920898
Classification loss 2 during warmup: 4.767427444458008
Alignment loss during warmup: 4.605169296264648
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.5071892738342285
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591196537017822
Contrastive Loss: tensor(6.2471, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2546, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7674, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5072, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5912, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2471, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2546, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7674, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5072, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5912, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.247132301330566 cls_loss1: 9.254644393920898 cls_loss2: 4.767427444458008 ent_loss: 4.5071892738342285 ne_loss: -4.591196537017822 align_loss: 4.605169296264648
Total loss: 24.79036521911621
[2024-05-02 19:56:22] 00078, contrastive_loss 6.24713, cls_loss1 9.25464, cls_loss2 4.76743, ent_loss 4.50719, ne_loss -4.59120, align_loss 4.60517, 
Training completed.
training step completed
 76% 79/104 [06:32<01:01,  2.47s/it]iteration number -->  79
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.7596153846153846
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002136418269230769
learning rate from cosine_annealing_LR -->  0.002136418269230769
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:56:24] 00079, lr 0.00214, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 667,   40,  707,  ...,  464,  739, 1064], device='cuda:0')
Generated mixing coefficient (lambda): 0.4701967636709477
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.25312614440918
Classification loss 2 during warmup: 4.678402900695801
Alignment loss during warmup: 4.605171203613281
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.551132678985596
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592075347900391
Contrastive Loss: tensor(6.3927, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2531, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6784, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5511, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5921, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3927, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2531, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6784, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5511, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5921, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.392724514007568 cls_loss1: 9.25312614440918 cls_loss2: 4.678402900695801 ent_loss: 4.551132678985596 ne_loss: -4.592075347900391 align_loss: 4.605171203613281
Total loss: 24.88848114013672
[2024-05-02 19:56:25] 00079, contrastive_loss 6.39272, cls_loss1 9.25313, cls_loss2 4.67840, ent_loss 4.55113, ne_loss -4.59208, align_loss 4.60517, 
Training completed.
training step completed
 77% 80/104 [06:35<00:59,  2.47s/it]iteration number -->  80
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.7692307692307693
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0021634615384615386
learning rate from cosine_annealing_LR -->  0.0021634615384615386
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:56:26] 00080, lr 0.00216, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([  72, 1402,  994,  ..., 1214,  144,  587], device='cuda:0')
Generated mixing coefficient (lambda): 0.7325450256963191
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.290887832641602
Classification loss 2 during warmup: 4.745847225189209
Alignment loss during warmup: 4.605169773101807
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.532625675201416
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5921735763549805
Contrastive Loss: tensor(6.2185, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2909, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7458, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5326, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5922, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2185, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2909, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7458, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5326, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5922, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.218465805053711 cls_loss1: 9.290887832641602 cls_loss2: 4.745847225189209 ent_loss: 4.532625675201416 ne_loss: -4.5921735763549805 align_loss: 4.605169773101807
Total loss: 24.80082130432129
[2024-05-02 19:56:27] 00080, contrastive_loss 6.21847, cls_loss1 9.29089, cls_loss2 4.74585, ent_loss 4.53263, ne_loss -4.59217, align_loss 4.60517, 
Training completed.
training step completed
 78% 81/104 [06:37<00:56,  2.47s/it]iteration number -->  81
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.7788461538461539
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0021905048076923074
learning rate from cosine_annealing_LR -->  0.0021905048076923074
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:56:29] 00081, lr 0.00219, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 664,  548, 1144,  ..., 1105,  284,  101], device='cuda:0')
Generated mixing coefficient (lambda): 0.4300397948765166
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.286796569824219
Classification loss 2 during warmup: 4.65877103805542
Alignment loss during warmup: 4.605172157287598
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.553739547729492
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592614650726318
Contrastive Loss: tensor(6.1623, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2868, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6588, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5537, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5926, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1623, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2868, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6588, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5537, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5926, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.1622724533081055 cls_loss1: 9.286796569824219 cls_loss2: 4.65877103805542 ent_loss: 4.553739547729492 ne_loss: -4.592614650726318 align_loss: 4.605172157287598
Total loss: 24.674137115478516
[2024-05-02 19:56:30] 00081, contrastive_loss 6.16227, cls_loss1 9.28680, cls_loss2 4.65877, ent_loss 4.55374, ne_loss -4.59261, align_loss 4.60517, 
Training completed.
training step completed
 79% 82/104 [06:40<00:54,  2.48s/it]iteration number -->  82
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.7884615384615384
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0022175480769230766
learning rate from cosine_annealing_LR -->  0.0022175480769230766
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:56:31] 00082, lr 0.00222, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1409,  491, 1039,  ...,  866, 1229,  307], device='cuda:0')
Generated mixing coefficient (lambda): 0.6727742745306516
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.265722274780273
Classification loss 2 during warmup: 4.689525127410889
Alignment loss during warmup: 4.605170726776123
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.540048599243164
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591428756713867
Contrastive Loss: tensor(6.2322, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2657, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6895, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5400, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5914, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2322, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2657, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6895, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5400, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5914, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.2322468757629395 cls_loss1: 9.265722274780273 cls_loss2: 4.689525127410889 ent_loss: 4.540048599243164 ne_loss: -4.591428756713867 align_loss: 4.605170726776123
Total loss: 24.74128532409668
[2024-05-02 19:56:32] 00082, contrastive_loss 6.23225, cls_loss1 9.26572, cls_loss2 4.68953, ent_loss 4.54005, ne_loss -4.59143, align_loss 4.60517, 
Training completed.
training step completed
 80% 83/104 [06:42<00:51,  2.47s/it]iteration number -->  83
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.7980769230769231
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0022445913461538462
learning rate from cosine_annealing_LR -->  0.0022445913461538462
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:56:34] 00083, lr 0.00224, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 520,  514, 1353,  ...,  643,  996,  228], device='cuda:0')
Generated mixing coefficient (lambda): 0.15512189273940405
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.26772689819336
Classification loss 2 during warmup: 4.780887126922607
Alignment loss during warmup: 4.60516881942749
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.517270565032959
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.593128681182861
Contrastive Loss: tensor(6.2368, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2677, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7809, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5173, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5931, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2368, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2677, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7809, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5173, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5931, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.236818790435791 cls_loss1: 9.26772689819336 cls_loss2: 4.780887126922607 ent_loss: 4.517270565032959 ne_loss: -4.593128681182861 align_loss: 4.60516881942749
Total loss: 24.81474494934082
[2024-05-02 19:56:34] 00083, contrastive_loss 6.23682, cls_loss1 9.26773, cls_loss2 4.78089, ent_loss 4.51727, ne_loss -4.59313, align_loss 4.60517, 
Training completed.
training step completed
 81% 84/104 [06:45<00:49,  2.47s/it]iteration number -->  84
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.8076923076923077
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0022716346153846155
learning rate from cosine_annealing_LR -->  0.0022716346153846155
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:56:36] 00084, lr 0.00227, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 509,  850,  444,  ...,  302,  334, 1203], device='cuda:0')
Generated mixing coefficient (lambda): 0.3649355634481757
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.320789337158203
Classification loss 2 during warmup: 4.686124801635742
Alignment loss during warmup: 4.6051740646362305
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.551723003387451
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592648506164551
Contrastive Loss: tensor(6.2567, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3208, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6861, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5517, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5926, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2567, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3208, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6861, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5517, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5926, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.256739616394043 cls_loss1: 9.320789337158203 cls_loss2: 4.686124801635742 ent_loss: 4.551723003387451 ne_loss: -4.592648506164551 align_loss: 4.6051740646362305
Total loss: 24.827903747558594
[2024-05-02 19:56:37] 00084, contrastive_loss 6.25674, cls_loss1 9.32079, cls_loss2 4.68612, ent_loss 4.55172, ne_loss -4.59265, align_loss 4.60517, 
Training completed.
training step completed
 82% 85/104 [06:47<00:46,  2.46s/it]iteration number -->  85
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.8173076923076923
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0022986778846153843
learning rate from cosine_annealing_LR -->  0.0022986778846153843
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:56:39] 00085, lr 0.00230, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1273,  123,   61,  ...,  208,  482,  732], device='cuda:0')
Generated mixing coefficient (lambda): 0.4273428899864599
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.288719177246094
Classification loss 2 during warmup: 4.682971954345703
Alignment loss during warmup: 4.605169296264648
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.55433988571167
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.593457221984863
Contrastive Loss: tensor(6.2100, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2887, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6830, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5543, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5935, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2100, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2887, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6830, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5543, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5935, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.2100090980529785 cls_loss1: 9.288719177246094 cls_loss2: 4.682971954345703 ent_loss: 4.55433988571167 ne_loss: -4.593457221984863 align_loss: 4.605169296264648
Total loss: 24.747751235961914
[2024-05-02 19:56:39] 00085, contrastive_loss 6.21001, cls_loss1 9.28872, cls_loss2 4.68297, ent_loss 4.55434, ne_loss -4.59346, align_loss 4.60517, 
Training completed.
training step completed
 83% 86/104 [06:50<00:44,  2.46s/it]iteration number -->  86
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.8269230769230769
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0023257211538461535
learning rate from cosine_annealing_LR -->  0.0023257211538461535
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:56:41] 00086, lr 0.00233, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1305, 1268,   39,  ...,  213,  889, 1043], device='cuda:0')
Generated mixing coefficient (lambda): 0.18681202477878797
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.257347106933594
Classification loss 2 during warmup: 4.7329912185668945
Alignment loss during warmup: 4.605170249938965
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.527035236358643
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.593146324157715
Contrastive Loss: tensor(6.2759, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2573, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7330, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5270, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5931, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2759, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2573, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7330, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5270, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5931, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.275932788848877 cls_loss1: 9.257347106933594 cls_loss2: 4.7329912185668945 ent_loss: 4.527035236358643 ne_loss: -4.593146324157715 align_loss: 4.605170249938965
Total loss: 24.80533218383789
[2024-05-02 19:56:42] 00086, contrastive_loss 6.27593, cls_loss1 9.25735, cls_loss2 4.73299, ent_loss 4.52704, ne_loss -4.59315, align_loss 4.60517, 
Training completed.
training step completed
 84% 87/104 [06:52<00:41,  2.46s/it]iteration number -->  87
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.8365384615384616
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002352764423076923
learning rate from cosine_annealing_LR -->  0.002352764423076923
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:56:44] 00087, lr 0.00235, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([   8,  704,  451,  ..., 1402, 1369, 1330], device='cuda:0')
Generated mixing coefficient (lambda): 0.07541079723374793
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.274283409118652
Classification loss 2 during warmup: 4.776276588439941
Alignment loss during warmup: 4.605172634124756
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.517272472381592
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.594842910766602
Contrastive Loss: tensor(6.1250, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2743, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7763, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5173, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5948, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1250, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2743, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7763, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5173, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5948, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.124956130981445 cls_loss1: 9.274283409118652 cls_loss2: 4.776276588439941 ent_loss: 4.517272472381592 ne_loss: -4.594842910766602 align_loss: 4.605172634124756
Total loss: 24.7031192779541
[2024-05-02 19:56:44] 00087, contrastive_loss 6.12496, cls_loss1 9.27428, cls_loss2 4.77628, ent_loss 4.51727, ne_loss -4.59484, align_loss 4.60517, 
Training completed.
training step completed
 85% 88/104 [06:55<00:39,  2.46s/it]iteration number -->  88
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.8461538461538461
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0023798076923076924
learning rate from cosine_annealing_LR -->  0.0023798076923076924
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:56:46] 00088, lr 0.00238, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 757,  503, 1079,  ...,  531, 1159,   47], device='cuda:0')
Generated mixing coefficient (lambda): 0.4096777288648497
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.280923843383789
Classification loss 2 during warmup: 4.671081066131592
Alignment loss during warmup: 4.605168342590332
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.557990074157715
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.594108581542969
Contrastive Loss: tensor(6.1846, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2809, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6711, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5580, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5941, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1846, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2809, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6711, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5580, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5941, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.184579849243164 cls_loss1: 9.280923843383789 cls_loss2: 4.671081066131592 ent_loss: 4.557990074157715 ne_loss: -4.594108581542969 align_loss: 4.605168342590332
Total loss: 24.70563507080078
[2024-05-02 19:56:47] 00088, contrastive_loss 6.18458, cls_loss1 9.28092, cls_loss2 4.67108, ent_loss 4.55799, ne_loss -4.59411, align_loss 4.60517, 
Training completed.
training step completed
 86% 89/104 [06:57<00:36,  2.46s/it]iteration number -->  89
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.8557692307692307
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002406850961538461
learning rate from cosine_annealing_LR -->  0.002406850961538461
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:56:49] 00089, lr 0.00241, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 605,  122,   71,  ..., 1006,  326,   34], device='cuda:0')
Generated mixing coefficient (lambda): 0.6829206493611906
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.281947135925293
Classification loss 2 during warmup: 4.702399730682373
Alignment loss during warmup: 4.6051716804504395
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.549080848693848
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.594137191772461
Contrastive Loss: tensor(6.1795, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2819, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7024, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5491, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5941, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1795, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2819, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7024, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5491, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5941, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.179521560668945 cls_loss1: 9.281947135925293 cls_loss2: 4.702399730682373 ent_loss: 4.549080848693848 ne_loss: -4.594137191772461 align_loss: 4.6051716804504395
Total loss: 24.723981857299805
[2024-05-02 19:56:49] 00089, contrastive_loss 6.17952, cls_loss1 9.28195, cls_loss2 4.70240, ent_loss 4.54908, ne_loss -4.59414, align_loss 4.60517, 
Training completed.
training step completed
 87% 90/104 [07:00<00:34,  2.45s/it]iteration number -->  90
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.8653846153846154
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002433894230769231
learning rate from cosine_annealing_LR -->  0.002433894230769231
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:56:51] 00090, lr 0.00243, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([510, 236,  55,  ..., 801, 316,  36], device='cuda:0')
Generated mixing coefficient (lambda): 0.6530166864751794
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.253355026245117
Classification loss 2 during warmup: 4.695462226867676
Alignment loss during warmup: 4.605170249938965
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.553887844085693
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.594789505004883
Contrastive Loss: tensor(6.1018, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2534, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6955, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5539, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5948, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1018, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2534, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6955, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5539, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5948, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.101809501647949 cls_loss1: 9.253355026245117 cls_loss2: 4.695462226867676 ent_loss: 4.553887844085693 ne_loss: -4.594789505004883 align_loss: 4.605170249938965
Total loss: 24.61489486694336
[2024-05-02 19:56:52] 00090, contrastive_loss 6.10181, cls_loss1 9.25336, cls_loss2 4.69546, ent_loss 4.55389, ne_loss -4.59479, align_loss 4.60517, 
Training completed.
training step completed
 88% 91/104 [07:02<00:31,  2.45s/it]iteration number -->  91
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.875
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0024609374999999996
learning rate from cosine_annealing_LR -->  0.0024609374999999996
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:56:53] 00091, lr 0.00246, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 300,  389, 1433,  ...,   34,  260,  424], device='cuda:0')
Generated mixing coefficient (lambda): 0.9115656793040001
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.238555908203125
Classification loss 2 during warmup: 4.806188583374023
Alignment loss during warmup: 4.605168342590332
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.5203070640563965
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.594329357147217
Contrastive Loss: tensor(6.2032, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2386, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8062, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5203, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5943, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2032, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2386, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8062, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5203, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5943, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.203152179718018 cls_loss1: 9.238555908203125 cls_loss2: 4.806188583374023 ent_loss: 4.5203070640563965 ne_loss: -4.594329357147217 align_loss: 4.605168342590332
Total loss: 24.779041290283203
[2024-05-02 19:56:54] 00091, contrastive_loss 6.20315, cls_loss1 9.23856, cls_loss2 4.80619, ent_loss 4.52031, ne_loss -4.59433, align_loss 4.60517, 
Training completed.
training step completed
 88% 92/104 [07:04<00:29,  2.45s/it]iteration number -->  92
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.8846153846153846
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002487980769230769
learning rate from cosine_annealing_LR -->  0.002487980769230769
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:56:56] 00092, lr 0.00249, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1410, 1221, 1414,  ...,  727,  311, 1015], device='cuda:0')
Generated mixing coefficient (lambda): 0.9122255576441457
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.274102210998535
Classification loss 2 during warmup: 4.759685516357422
Alignment loss during warmup: 4.605169773101807
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.522791862487793
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.595489501953125
Contrastive Loss: tensor(6.1657, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2741, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7597, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5228, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5955, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1657, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2741, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7597, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5228, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5955, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.165665149688721 cls_loss1: 9.274102210998535 cls_loss2: 4.759685516357422 ent_loss: 4.522791862487793 ne_loss: -4.595489501953125 align_loss: 4.605169773101807
Total loss: 24.731924057006836
[2024-05-02 19:56:57] 00092, contrastive_loss 6.16567, cls_loss1 9.27410, cls_loss2 4.75969, ent_loss 4.52279, ne_loss -4.59549, align_loss 4.60517, 
Training completed.
training step completed
 89% 93/104 [07:07<00:27,  2.46s/it]iteration number -->  93
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.8942307692307693
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002515024038461538
learning rate from cosine_annealing_LR -->  0.002515024038461538
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:56:58] 00093, lr 0.00252, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 535,  817,  516,  ..., 1313,  144,  767], device='cuda:0')
Generated mixing coefficient (lambda): 0.6283996060676098
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.279345512390137
Classification loss 2 during warmup: 4.683797836303711
Alignment loss during warmup: 4.605171203613281
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.556234359741211
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.594364643096924
Contrastive Loss: tensor(6.1549, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2793, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6838, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5562, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5944, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1549, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2793, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6838, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5562, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5944, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.154898166656494 cls_loss1: 9.279345512390137 cls_loss2: 4.683797836303711 ent_loss: 4.556234359741211 ne_loss: -4.594364643096924 align_loss: 4.605171203613281
Total loss: 24.685083389282227
[2024-05-02 19:56:59] 00093, contrastive_loss 6.15490, cls_loss1 9.27935, cls_loss2 4.68380, ent_loss 4.55623, ne_loss -4.59436, align_loss 4.60517, 
Training completed.
training step completed
 90% 94/104 [07:09<00:24,  2.46s/it]iteration number -->  94
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.9038461538461539
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0025420673076923072
learning rate from cosine_annealing_LR -->  0.0025420673076923072
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:57:01] 00094, lr 0.00254, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 398, 1119,  763,  ...,  164,  900, 1050], device='cuda:0')
Generated mixing coefficient (lambda): 0.948067647826056
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.254079818725586
Classification loss 2 during warmup: 4.776062965393066
Alignment loss during warmup: 4.605166912078857
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.521986484527588
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.595705032348633
Contrastive Loss: tensor(6.1398, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2541, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7761, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5220, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5957, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1398, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2541, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7761, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5220, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5957, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.139837265014648 cls_loss1: 9.254079818725586 cls_loss2: 4.776062965393066 ent_loss: 4.521986484527588 ne_loss: -4.595705032348633 align_loss: 4.605166912078857
Total loss: 24.701427459716797
[2024-05-02 19:57:02] 00094, contrastive_loss 6.13984, cls_loss1 9.25408, cls_loss2 4.77606, ent_loss 4.52199, ne_loss -4.59571, align_loss 4.60517, 
Training completed.
training step completed
 91% 95/104 [07:12<00:22,  2.47s/it]iteration number -->  95
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.9134615384615384
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0025691105769230765
learning rate from cosine_annealing_LR -->  0.0025691105769230765
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:57:03] 00095, lr 0.00257, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 268, 1220, 1256,  ..., 1400,  377, 1030], device='cuda:0')
Generated mixing coefficient (lambda): 0.4576707892414795
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.247295379638672
Classification loss 2 during warmup: 4.65924072265625
Alignment loss during warmup: 4.605172157287598
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.566277027130127
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596209526062012
Contrastive Loss: tensor(6.0942, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2473, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6592, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5663, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5962, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.0942, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2473, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6592, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5663, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5962, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.094223499298096 cls_loss1: 9.247295379638672 cls_loss2: 4.65924072265625 ent_loss: 4.566277027130127 ne_loss: -4.596209526062012 align_loss: 4.605172157287598
Total loss: 24.57599639892578
[2024-05-02 19:57:04] 00095, contrastive_loss 6.09422, cls_loss1 9.24730, cls_loss2 4.65924, ent_loss 4.56628, ne_loss -4.59621, align_loss 4.60517, 
Training completed.
training step completed
 92% 96/104 [07:14<00:19,  2.46s/it]iteration number -->  96
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.9230769230769231
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002596153846153846
learning rate from cosine_annealing_LR -->  0.002596153846153846
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:57:06] 00096, lr 0.00260, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1221,  302,  910,  ..., 1311,   78,  870], device='cuda:0')
Generated mixing coefficient (lambda): 0.6829660400879685
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.266767501831055
Classification loss 2 during warmup: 4.701939105987549
Alignment loss during warmup: 4.605173587799072
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.554680347442627
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.595829963684082
Contrastive Loss: tensor(6.1332, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2668, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7019, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5547, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5958, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1332, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2668, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7019, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5547, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5958, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.13319206237793 cls_loss1: 9.266767501831055 cls_loss2: 4.701939105987549 ent_loss: 4.554680347442627 ne_loss: -4.595829963684082 align_loss: 4.605173587799072
Total loss: 24.665922164916992
[2024-05-02 19:57:06] 00096, contrastive_loss 6.13319, cls_loss1 9.26677, cls_loss2 4.70194, ent_loss 4.55468, ne_loss -4.59583, align_loss 4.60517, 
Training completed.
training step completed
 93% 97/104 [07:17<00:17,  2.46s/it]iteration number -->  97
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.9326923076923077
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002623197115384615
learning rate from cosine_annealing_LR -->  0.002623197115384615
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:57:08] 00097, lr 0.00262, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([  17, 1290,  338,  ..., 1221,  188,  254], device='cuda:0')
Generated mixing coefficient (lambda): 0.5992131394779854
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.275795936584473
Classification loss 2 during warmup: 4.668421268463135
Alignment loss during warmup: 4.605172157287598
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.562402248382568
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596017360687256
Contrastive Loss: tensor(6.2044, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2758, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6684, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5624, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5960, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2044, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2758, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6684, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5624, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5960, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.204402446746826 cls_loss1: 9.275795936584473 cls_loss2: 4.668421268463135 ent_loss: 4.562402248382568 ne_loss: -4.596017360687256 align_loss: 4.605172157287598
Total loss: 24.720176696777344
[2024-05-02 19:57:09] 00097, contrastive_loss 6.20440, cls_loss1 9.27580, cls_loss2 4.66842, ent_loss 4.56240, ne_loss -4.59602, align_loss 4.60517, 
Training completed.
training step completed
 94% 98/104 [07:19<00:14,  2.47s/it]iteration number -->  98
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.9423076923076923
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002650240384615384
learning rate from cosine_annealing_LR -->  0.002650240384615384
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:57:11] 00098, lr 0.00265, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 128,  221, 1336,  ...,   89,  792,  162], device='cuda:0')
Generated mixing coefficient (lambda): 0.8364876358261744
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.232763290405273
Classification loss 2 during warmup: 4.725906848907471
Alignment loss during warmup: 4.605169296264648
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.53566837310791
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.595660209655762
Contrastive Loss: tensor(6.2543, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2328, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7259, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5357, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5957, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2543, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2328, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7259, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5357, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5957, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.254281044006348 cls_loss1: 9.232763290405273 cls_loss2: 4.725906848907471 ent_loss: 4.53566837310791 ne_loss: -4.595660209655762 align_loss: 4.605169296264648
Total loss: 24.758127212524414
[2024-05-02 19:57:11] 00098, contrastive_loss 6.25428, cls_loss1 9.23276, cls_loss2 4.72591, ent_loss 4.53567, ne_loss -4.59566, align_loss 4.60517, 
Training completed.
training step completed
 95% 99/104 [07:22<00:12,  2.46s/it]iteration number -->  99
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.9519230769230769
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0026772836538461534
learning rate from cosine_annealing_LR -->  0.0026772836538461534
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:57:13] 00099, lr 0.00268, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 416,  889,  875,  ...,  716,  274, 1280], device='cuda:0')
Generated mixing coefficient (lambda): 0.12884143802376474
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.278459548950195
Classification loss 2 during warmup: 4.747321128845215
Alignment loss during warmup: 4.605170726776123
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.533713340759277
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.595829963684082
Contrastive Loss: tensor(6.0945, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2785, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7473, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5337, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5958, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.0945, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2785, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7473, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5337, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5958, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.094545841217041 cls_loss1: 9.278459548950195 cls_loss2: 4.747321128845215 ent_loss: 4.533713340759277 ne_loss: -4.595829963684082 align_loss: 4.605170726776123
Total loss: 24.66338348388672
[2024-05-02 19:57:14] 00099, contrastive_loss 6.09455, cls_loss1 9.27846, cls_loss2 4.74732, ent_loss 4.53371, ne_loss -4.59583, align_loss 4.60517, 
Training completed.
training step completed
 96% 100/104 [07:24<00:09,  2.47s/it]iteration number -->  100
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.9615384615384616
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002704326923076923
learning rate from cosine_annealing_LR -->  0.002704326923076923
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:57:16] 00100, lr 0.00270, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 170, 1015,  339,  ...,  391, 1209, 1037], device='cuda:0')
Generated mixing coefficient (lambda): 0.03356349005160294
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.253591537475586
Classification loss 2 during warmup: 4.796515464782715
Alignment loss during warmup: 4.6051716804504395
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.5228376388549805
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5967183113098145
Contrastive Loss: tensor(6.1321, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2536, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7965, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5228, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5967, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1321, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2536, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7965, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5228, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5967, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.132134437561035 cls_loss1: 9.253591537475586 cls_loss2: 4.796515464782715 ent_loss: 4.5228376388549805 ne_loss: -4.5967183113098145 align_loss: 4.6051716804504395
Total loss: 24.713531494140625
[2024-05-02 19:57:16] 00100, contrastive_loss 6.13213, cls_loss1 9.25359, cls_loss2 4.79652, ent_loss 4.52284, ne_loss -4.59672, align_loss 4.60517, 
Training completed.
training step completed
 97% 101/104 [07:27<00:07,  2.47s/it]iteration number -->  101
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.9711538461538461
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002731370192307692
learning rate from cosine_annealing_LR -->  0.002731370192307692
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:57:18] 00101, lr 0.00273, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1213, 1017,  261,  ...,  213,  858,  682], device='cuda:0')
Generated mixing coefficient (lambda): 0.4990604637557739
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.232975006103516
Classification loss 2 during warmup: 4.6475982666015625
Alignment loss during warmup: 4.605169296264648
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.56934928894043
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.597333908081055
Contrastive Loss: tensor(6.1521, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2330, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6476, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5693, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5973, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1521, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2330, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6476, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5693, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5973, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.152061462402344 cls_loss1: 9.232975006103516 cls_loss2: 4.6475982666015625 ent_loss: 4.56934928894043 ne_loss: -4.597333908081055 align_loss: 4.605169296264648
Total loss: 24.609819412231445
[2024-05-02 19:57:19] 00101, contrastive_loss 6.15206, cls_loss1 9.23298, cls_loss2 4.64760, ent_loss 4.56935, ne_loss -4.59733, align_loss 4.60517, 
Training completed.
training step completed
 98% 102/104 [07:29<00:04,  2.47s/it]iteration number -->  102
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.9807692307692307
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002758413461538461
learning rate from cosine_annealing_LR -->  0.002758413461538461
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:57:21] 00102, lr 0.00276, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 420,  170,  474,  ...,  973, 1298, 1051], device='cuda:0')
Generated mixing coefficient (lambda): 0.08101029774593642
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.235816955566406
Classification loss 2 during warmup: 4.74753475189209
Alignment loss during warmup: 4.6051716804504395
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.5309906005859375
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.59688138961792
Contrastive Loss: tensor(6.0525, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2358, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7475, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5310, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5969, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.0525, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2358, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7475, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5310, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5969, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.052520275115967 cls_loss1: 9.235816955566406 cls_loss2: 4.74753475189209 ent_loss: 4.5309906005859375 ne_loss: -4.59688138961792 align_loss: 4.6051716804504395
Total loss: 24.575151443481445
[2024-05-02 19:57:21] 00102, contrastive_loss 6.05252, cls_loss1 9.23582, cls_loss2 4.74753, ent_loss 4.53099, ne_loss -4.59688, align_loss 4.60517, 
Training completed.
training step completed
 99% 103/104 [07:32<00:02,  2.47s/it]iteration number -->  103
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.9903846153846154
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0027854567307692307
learning rate from cosine_annealing_LR -->  0.0027854567307692307
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:57:23] 00103, lr 0.00279, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1045,  227,  274,  ...,  233, 1432,  494], device='cuda:0')
Generated mixing coefficient (lambda): 0.5839731793418893
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.261669158935547
Classification loss 2 during warmup: 4.665116310119629
Alignment loss during warmup: 4.605166435241699
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.5687150955200195
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5973405838012695
Contrastive Loss: tensor(6.0644, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2617, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6651, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5687, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5973, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.0644, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2617, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6651, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5687, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5973, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.064392566680908 cls_loss1: 9.261669158935547 cls_loss2: 4.665116310119629 ent_loss: 4.5687150955200195 ne_loss: -4.5973405838012695 align_loss: 4.605166435241699
Total loss: 24.56772232055664
[2024-05-02 19:57:24] 00103, contrastive_loss 6.06439, cls_loss1 9.26167, cls_loss2 4.66512, ent_loss 4.56872, ne_loss -4.59734, align_loss 4.60517, 
Training completed.
training step completed
100% 104/104 [07:34<00:00,  2.47s/it]iteration number -->  104
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  1.0
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0028125
learning rate from cosine_annealing_LR -->  0.0028125
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:57:26] 00104, lr 0.00281, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1020, 1232,  212,  ..., 1332,  546,   96], device='cuda:0')
Generated mixing coefficient (lambda): 0.9489226498220377
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.250638961791992
Classification loss 2 during warmup: 4.7440314292907715
Alignment loss during warmup: 4.605168342590332
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.5319294929504395
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.597629547119141
Contrastive Loss: tensor(6.1170, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2506, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7440, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5319, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5976, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1170, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2506, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7440, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5319, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5976, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.1169891357421875 cls_loss1: 9.250638961791992 cls_loss2: 4.7440314292907715 ent_loss: 4.5319294929504395 ne_loss: -4.597629547119141 align_loss: 4.605168342590332
Total loss: 24.651126861572266
[2024-05-02 19:57:26] 00104, contrastive_loss 6.11699, cls_loss1 9.25064, cls_loss2 4.74403, ent_loss 4.53193, ne_loss -4.59763, align_loss 4.60517, 
Training completed.
training step completed
105it [07:37,  2.47s/it]             iteration number -->  105
current epoch -->  1
[2024-05-02 19:57:28] 00105, cur_epoch 1.00000, 
logger message recorded
apply_kmeans -->  True
checkpoints saved
Generating the psedo-labels
Obtained ground truth labels: 50000
Extracting features from the model...
Initialized features tensor shape: torch.Size([50000, 256])
Initialized all_labels tensor shape: torch.Size([50000])
Initialized cluster_labels tensor shape: torch.Size([50000, 100])

  0% 0/105 [00:00<?, ?it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  1% 1/105 [00:05<09:56,  5.73s/it]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  2% 2/105 [00:05<04:18,  2.51s/it]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  3% 3/105 [00:06<02:33,  1.50s/it]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  4% 4/105 [00:06<01:41,  1.01s/it]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  5% 5/105 [00:06<01:16,  1.31it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  6% 6/105 [00:07<00:58,  1.68it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  7% 7/105 [00:07<00:48,  2.03it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  8% 8/105 [00:07<00:45,  2.13it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  9% 9/105 [00:08<00:37,  2.56it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 10% 10/105 [00:08<00:31,  2.99it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 10% 11/105 [00:08<00:27,  3.37it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 11% 12/105 [00:08<00:26,  3.56it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 12% 13/105 [00:08<00:22,  4.03it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 13% 14/105 [00:08<00:18,  4.86it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 14% 15/105 [00:09<00:17,  5.18it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 15% 16/105 [00:09<00:14,  5.97it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 16% 17/105 [00:09<00:13,  6.30it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 18% 19/105 [00:09<00:13,  6.43it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 19% 20/105 [00:09<00:12,  6.69it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 20% 21/105 [00:10<00:13,  6.06it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 21% 22/105 [00:10<00:13,  6.07it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 22% 23/105 [00:10<00:14,  5.61it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 23% 24/105 [00:10<00:14,  5.54it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 24% 25/105 [00:10<00:15,  5.13it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 25% 26/105 [00:11<00:14,  5.31it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 26% 27/105 [00:11<00:13,  5.72it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 27% 28/105 [00:11<00:11,  6.46it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 28% 29/105 [00:11<00:11,  6.76it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 30% 31/105 [00:11<00:10,  7.36it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 30% 32/105 [00:11<00:09,  7.47it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 31% 33/105 [00:12<00:14,  4.96it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 33% 35/105 [00:12<00:10,  6.38it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 34% 36/105 [00:12<00:10,  6.57it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 35% 37/105 [00:12<00:09,  6.99it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 36% 38/105 [00:12<00:08,  7.54it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 37% 39/105 [00:12<00:09,  7.31it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 39% 41/105 [00:13<00:07,  8.22it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 41% 43/105 [00:13<00:06,  9.05it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 42% 44/105 [00:13<00:06,  8.93it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 44% 46/105 [00:13<00:06,  9.64it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 45% 47/105 [00:13<00:05,  9.70it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 46% 48/105 [00:13<00:05,  9.73it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 47% 49/105 [00:13<00:05,  9.78it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 48% 50/105 [00:13<00:05,  9.71it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 49% 51/105 [00:14<00:05,  9.77it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 50% 52/105 [00:14<00:05,  9.78it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 50% 53/105 [00:14<00:05,  9.82it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 51% 54/105 [00:14<00:05,  9.84it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 52% 55/105 [00:14<00:05,  9.86it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 53% 56/105 [00:14<00:04,  9.84it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 55% 58/105 [00:14<00:04,  9.88it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 56% 59/105 [00:14<00:04,  9.88it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 57% 60/105 [00:14<00:04,  9.79it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 59% 62/105 [00:15<00:04,  9.87it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 60% 63/105 [00:15<00:04,  9.82it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 61% 64/105 [00:15<00:04,  9.83it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 62% 65/105 [00:15<00:04,  9.86it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 63% 66/105 [00:15<00:03,  9.83it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 64% 67/105 [00:15<00:03,  9.85it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 65% 68/105 [00:15<00:03,  9.82it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 66% 69/105 [00:15<00:03,  9.85it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 67% 70/105 [00:15<00:03,  9.74it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 68% 71/105 [00:16<00:03,  9.75it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 69% 72/105 [00:16<00:03,  9.69it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 70% 73/105 [00:16<00:03,  9.76it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 70% 74/105 [00:16<00:03,  9.78it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 71% 75/105 [00:16<00:03,  9.77it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 72% 76/105 [00:16<00:02,  9.73it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 73% 77/105 [00:16<00:02,  9.78it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 74% 78/105 [00:16<00:02,  9.80it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 75% 79/105 [00:16<00:02,  9.81it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 76% 80/105 [00:17<00:02,  9.81it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 77% 81/105 [00:17<00:02,  9.83it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 78% 82/105 [00:17<00:02,  9.72it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 80% 84/105 [00:17<00:02,  9.69it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 82% 86/105 [00:17<00:01,  9.77it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 83% 87/105 [00:17<00:01,  9.75it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 84% 88/105 [00:17<00:01,  9.73it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 85% 89/105 [00:17<00:01,  9.75it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 86% 90/105 [00:18<00:01,  9.76it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 87% 91/105 [00:18<00:01,  9.67it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 88% 92/105 [00:18<00:01,  9.74it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 89% 93/105 [00:18<00:01,  9.62it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 90% 94/105 [00:18<00:01,  9.64it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 90% 95/105 [00:18<00:01,  9.70it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 91% 96/105 [00:18<00:00,  9.69it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 92% 97/105 [00:18<00:00,  9.70it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 93% 98/105 [00:18<00:00,  9.72it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 94% 99/105 [00:18<00:00,  9.71it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 95% 100/105 [00:19<00:00,  9.71it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 96% 101/105 [00:19<00:00,  9.72it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 97% 102/105 [00:19<00:00,  9.63it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 98% 103/105 [00:19<00:00,  9.41it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([80, 3, 32, 32]) labels shape: torch.Size([80])
Encoder output shape: torch.Size([80, 512])
Local cluster labels shape: torch.Size([80, 100])
Local features shape: torch.Size([80, 256])

100% 105/105 [00:19<00:00,  5.39it/s]

Updated features tensor shape after indexing: torch.Size([50000, 256])
Updated all_labels tensor shape after indexing: torch.Size([50000])
Updated cluster_labels tensor shape after indexing: torch.Size([50000, 100])
Final labels tensor shape: torch.Size([50000])
Feature extraction completed.
Extracted features and cluster labels from the memory loader.
Parameters of the config file: Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_49_41-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
Computed cluster centers after matrix multiplication: torch.Size([100, 256])
Computed confidence scores: torch.Size([50000])
Computed confidence, context assignments, and centers for noise detection.
Copied centers and context assignments to the model.
Obtained confidence, context assignments, features, and cluster labels from correct_labels function.
Copied confidence values to the tcl model.
data root ---->  /content/dataset/
dataset name ---------------->  cifar100
dataset type ---->  <class 'torchvision.datasets.cifar.CIFAR100'>
has_subfolder ---->  True
Files already downloaded and verified
dataset ---->  50000
Clean labels from the training dataset: 50000
Check if each label is clean: [False False False ... False False False]
save 0000105-0-context_assignments_hist.png to ./ckpt/2024_05_02_19_49_41-cifar100_90_prer18/save_images
Histogram of context assignments computed.
Training accuracy: tensor(0.0120, device='cuda:0')
Extracting features from the model...
Initialized features tensor shape: torch.Size([10000, 256])
Initialized all_labels tensor shape: torch.Size([10000])
Initialized cluster_labels tensor shape: torch.Size([10000, 100])

  0% 0/21 [00:00<?, ?it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  5% 1/21 [00:02<00:41,  2.08s/it]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 10% 2/21 [00:02<00:17,  1.07it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 14% 3/21 [00:02<00:10,  1.76it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 24% 5/21 [00:02<00:04,  3.31it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 29% 6/21 [00:02<00:03,  4.09it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 33% 7/21 [00:02<00:02,  4.91it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 43% 9/21 [00:02<00:01,  6.40it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 48% 10/21 [00:03<00:01,  7.01it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 52% 11/21 [00:03<00:01,  7.59it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 57% 12/21 [00:03<00:01,  8.02it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 62% 13/21 [00:03<00:00,  8.43it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 67% 14/21 [00:03<00:00,  8.76it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 71% 15/21 [00:03<00:00,  9.02it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 76% 16/21 [00:03<00:00,  9.28it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 81% 17/21 [00:03<00:00,  9.47it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 86% 18/21 [00:03<00:00,  9.55it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 95% 20/21 [00:04<00:00,  9.72it/s]Processed images shape: torch.Size([400, 3, 32, 32]) labels shape: torch.Size([400])
Encoder output shape: torch.Size([400, 512])
Local cluster labels shape: torch.Size([400, 100])
Local features shape: torch.Size([400, 256])

100% 21/21 [00:04<00:00,  5.04it/s]
Updated features tensor shape after indexing: torch.Size([10000, 256])
Updated all_labels tensor shape after indexing: torch.Size([10000])
Updated cluster_labels tensor shape after indexing: torch.Size([10000, 100])
Final labels tensor shape: torch.Size([10000])
Feature extraction completed.
Extracted features, test cluster labels, and test labels.
Test accuracy: tensor(0.0118, device='cuda:0')
KNN labels predicted for the test features: tensor([68, 38, 30,  ..., 51, 74, 83], device='cuda:0')
(tensor([ 1,  2,  6,  9, 10, 11, 15, 17, 19, 24, 25, 27, 28, 29, 31, 32, 34, 35,
        39, 40, 41, 43, 44, 45, 48, 50, 52, 54, 56, 59, 63, 65, 66, 67, 69, 72,
        74, 78, 80, 81, 85, 86, 91, 93, 94, 95, 96, 98], device='cuda:0'), tensor([667, 729,  36, 141,   2, 572, 218,   2, 645,   4, 428, 138,   2,   8,
        102, 453, 321,   5, 160,  84,   8,  33,   1, 663, 735, 149,   3,  19,
          4, 747, 359, 148, 304,   3,   3,  34,  23, 817,  39,   2, 284,   7,
         20,   3,   1, 114, 181, 579], device='cuda:0'))
KNN accuracy: tensor(0.1560, device='cuda:0')
Estimated noise ratio: 0.4783799946308136
Updated scale1 of tcl model: 0.4783799946308136
Noise accuracy: tensor(0.5324, device='cuda:0')
Context noise AUC: 0.5575956773169441
[2024-05-02 19:57:55] 00105, estimated_noise_ratio 0.47838, noise_accuracy 0.53240, context_noise_auc 0.55760, train_acc 0.01202, test_acc 0.01180, knn_acc 0.15600, 
Evaluated the tcl model using obtained features, confidence, cluster labels, and context assignments.
psedo labeling applied
 this function does nothing--------------------------
test performed
cuda cache emptied
current epoch after increment -->  2
105it [08:06,  4.67s/it]








Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 10.01250171661377
Classification loss 2 during warmup: 4.772898197174072
Alignment loss during warmup: 4.605165958404541
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.361083984375
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596325874328613
Contrastive Loss: tensor(6.8184, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(10.0125, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7729, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3611, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5963, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.8184, device='cuda:0', grad_fn=<DivBackward0>), tensor(10.0125, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7729, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3611, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5963, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.818423271179199 cls_loss1: 10.01250171661377 cls_loss2: 4.772898197174072 ent_loss: 4.361083984375 ne_loss: -4.596325874328613 align_loss: 4.605165958404541
Total loss: 25.9737491607666
[2024-05-02 19:37:51] 00009, contrastive_loss 6.81842, cls_loss1 10.01250, cls_loss2 4.77290, ent_loss 4.36108, ne_loss -4.59633, align_loss 4.60517, 
Training completed.
training step completed
 10% 10/104 [03:28<08:59,  5.74s/it]iteration number -->  10
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.09615384615384616
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0002704326923076923
learning rate from cosine_annealing_LR -->  0.0002704326923076923
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:37:53] 00010, lr 0.00027, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 424, 1168,  153,  ...,  723, 1187,  327], device='cuda:0')
Generated mixing coefficient (lambda): 0.5642477065646552
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 10.202177047729492
Classification loss 2 during warmup: 4.768327236175537
Alignment loss during warmup: 4.605165958404541
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.370948791503906
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596319198608398
Contrastive Loss: tensor(6.8515, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(10.2022, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7683, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3709, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5963, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.8515, device='cuda:0', grad_fn=<DivBackward0>), tensor(10.2022, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7683, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3709, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5963, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.851459503173828 cls_loss1: 10.202177047729492 cls_loss2: 4.768327236175537 ent_loss: 4.370948791503906 ne_loss: -4.596319198608398 align_loss: 4.605165958404541
Total loss: 26.201759338378906
[2024-05-02 19:37:53] 00010, contrastive_loss 6.85146, cls_loss1 10.20218, cls_loss2 4.76833, ent_loss 4.37095, ne_loss -4.59632, align_loss 4.60517, 
Training completed.
training step completed
 11% 11/104 [03:31<07:16,  4.69s/it]iteration number -->  11
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.10576923076923077
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.00029747596153846154
learning rate from cosine_annealing_LR -->  0.00029747596153846154
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:37:55] 00011, lr 0.00030, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 333,  796,  882,  ...,  955,  819, 1372], device='cuda:0')
Generated mixing coefficient (lambda): 0.7205520491410118
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.929706573486328
Classification loss 2 during warmup: 4.8468918800354
Alignment loss during warmup: 4.605172157287598
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.342614650726318
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5971574783325195
Contrastive Loss: tensor(6.8202, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.9297, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8469, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3426, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5972, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.8202, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.9297, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8469, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3426, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5972, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.820183277130127 cls_loss1: 9.929706573486328 cls_loss2: 4.8468918800354 ent_loss: 4.342614650726318 ne_loss: -4.5971574783325195 align_loss: 4.605172157287598
Total loss: 25.947410583496094
[2024-05-02 19:37:56] 00011, contrastive_loss 6.82018, cls_loss1 9.92971, cls_loss2 4.84689, ent_loss 4.34261, ne_loss -4.59716, align_loss 4.60517, 
Training completed.
training step completed
 12% 12/104 [03:33<06:07,  4.00s/it]iteration number -->  12
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.11538461538461539
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.00032451923076923077
learning rate from cosine_annealing_LR -->  0.00032451923076923077
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:37:57] 00012, lr 0.00032, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1091,  774,   97,  ...,  146,   69,  602], device='cuda:0')
Generated mixing coefficient (lambda): 0.6560952779579972
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 10.00097370147705
Classification loss 2 during warmup: 4.796307563781738
Alignment loss during warmup: 4.605177402496338
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.364989757537842
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.597012519836426
Contrastive Loss: tensor(6.8327, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(10.0010, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7963, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3650, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5970, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.8327, device='cuda:0', grad_fn=<DivBackward0>), tensor(10.0010, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7963, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3650, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5970, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.832683086395264 cls_loss1: 10.00097370147705 cls_loss2: 4.796307563781738 ent_loss: 4.364989757537842 ne_loss: -4.597012519836426 align_loss: 4.605177402496338
Total loss: 26.00311851501465
[2024-05-02 19:37:58] 00012, contrastive_loss 6.83268, cls_loss1 10.00097, cls_loss2 4.79631, ent_loss 4.36499, ne_loss -4.59701, align_loss 4.60518, 
Training completed.
training step completed
 12% 13/104 [03:36<05:21,  3.54s/it]iteration number -->  13
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.125
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0003515625
learning rate from cosine_annealing_LR -->  0.0003515625
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:38:00] 00013, lr 0.00035, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1301,  257, 1382,  ...,  119,  662, 1147], device='cuda:0')
Generated mixing coefficient (lambda): 0.5902086397190913
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 10.005017280578613
Classification loss 2 during warmup: 4.8153228759765625
Alignment loss during warmup: 4.605169773101807
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.376464366912842
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596035003662109
Contrastive Loss: tensor(6.8546, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(10.0050, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8153, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3765, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5960, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.8546, device='cuda:0', grad_fn=<DivBackward0>), tensor(10.0050, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8153, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3765, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5960, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.8546223640441895 cls_loss1: 10.005017280578613 cls_loss2: 4.8153228759765625 ent_loss: 4.376464366912842 ne_loss: -4.596035003662109 align_loss: 4.605169773101807
Total loss: 26.060562133789062
[2024-05-02 19:38:01] 00013, contrastive_loss 6.85462, cls_loss1 10.00502, cls_loss2 4.81532, ent_loss 4.37646, ne_loss -4.59604, align_loss 4.60517, 
Training completed.
training step completed
 13% 14/104 [03:38<04:49,  3.21s/it]iteration number -->  14
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.1346153846153846
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0003786057692307692
learning rate from cosine_annealing_LR -->  0.0003786057692307692
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:38:02] 00014, lr 0.00038, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1310,   77, 1244,  ...,  575, 1430,  577], device='cuda:0')
Generated mixing coefficient (lambda): 0.8260783257026236
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.998342514038086
Classification loss 2 during warmup: 4.875598430633545
Alignment loss during warmup: 4.605178356170654
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.346137523651123
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.597953796386719
Contrastive Loss: tensor(6.7355, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.9983, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8756, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3461, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5980, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.7355, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.9983, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8756, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3461, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5980, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.73548698425293 cls_loss1: 9.998342514038086 cls_loss2: 4.875598430633545 ent_loss: 4.346137523651123 ne_loss: -4.597953796386719 align_loss: 4.605178356170654
Total loss: 25.962791442871094
[2024-05-02 19:38:03] 00014, contrastive_loss 6.73549, cls_loss1 9.99834, cls_loss2 4.87560, ent_loss 4.34614, ne_loss -4.59795, align_loss 4.60518, 
Training completed.
training step completed
 14% 15/104 [03:40<04:25,  2.98s/it]iteration number -->  15
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.14423076923076922
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.00040564903846153843
learning rate from cosine_annealing_LR -->  0.00040564903846153843
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:38:05] 00015, lr 0.00041, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 955, 1395, 1150,  ...,   86, 1045,  290], device='cuda:0')
Generated mixing coefficient (lambda): 0.34774410463978506
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.908063888549805
Classification loss 2 during warmup: 4.776402950286865
Alignment loss during warmup: 4.605172634124756
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.373402118682861
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596142768859863
Contrastive Loss: tensor(6.7457, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.9081, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7764, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3734, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5961, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.7457, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.9081, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7764, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3734, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5961, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.745654106140137 cls_loss1: 9.908063888549805 cls_loss2: 4.776402950286865 ent_loss: 4.373402118682861 ne_loss: -4.596142768859863 align_loss: 4.605172634124756
Total loss: 25.81255531311035
[2024-05-02 19:38:06] 00015, contrastive_loss 6.74565, cls_loss1 9.90806, cls_loss2 4.77640, ent_loss 4.37340, ne_loss -4.59614, align_loss 4.60517, 
Training completed.
training step completed
 15% 16/104 [03:43<04:05,  2.79s/it]iteration number -->  16
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.15384615384615385
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0004326923076923077
learning rate from cosine_annealing_LR -->  0.0004326923076923077
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:38:07] 00016, lr 0.00043, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([464, 195, 885,  ..., 993, 888, 187], device='cuda:0')
Generated mixing coefficient (lambda): 0.8942360778803845
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.716272354125977
Classification loss 2 during warmup: 4.9516472816467285
Alignment loss during warmup: 4.605167865753174
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.34165620803833
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.598776340484619
Contrastive Loss: tensor(6.7555, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.7163, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9516, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3417, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5988, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.7555, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.7163, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9516, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3417, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5988, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.755472183227539 cls_loss1: 9.716272354125977 cls_loss2: 4.9516472816467285 ent_loss: 4.34165620803833 ne_loss: -4.598776340484619 align_loss: 4.605167865753174
Total loss: 25.771438598632812
[2024-05-02 19:38:08] 00016, contrastive_loss 6.75547, cls_loss1 9.71627, cls_loss2 4.95165, ent_loss 4.34166, ne_loss -4.59878, align_loss 4.60517, 
Training completed.
training step completed
 16% 17/104 [03:45<03:51,  2.66s/it]iteration number -->  17
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.16346153846153846
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.00045973557692307687
learning rate from cosine_annealing_LR -->  0.00045973557692307687
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:38:10] 00017, lr 0.00046, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1076,  719,  957,  ...,    7,  600,  183], device='cuda:0')
Generated mixing coefficient (lambda): 0.8971156837051465
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.754072189331055
Classification loss 2 during warmup: 4.899341583251953
Alignment loss during warmup: 4.605175018310547
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.348486423492432
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.598929405212402
Contrastive Loss: tensor(6.7118, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.7541, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8993, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3485, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5989, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.7118, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.7541, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8993, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3485, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5989, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.711774826049805 cls_loss1: 9.754072189331055 cls_loss2: 4.899341583251953 ent_loss: 4.348486423492432 ne_loss: -4.598929405212402 align_loss: 4.605175018310547
Total loss: 25.719921112060547
[2024-05-02 19:38:10] 00017, contrastive_loss 6.71177, cls_loss1 9.75407, cls_loss2 4.89934, ent_loss 4.34849, ne_loss -4.59893, align_loss 4.60518, 
Training completed.
training step completed
 17% 18/104 [03:47<03:40,  2.57s/it]iteration number -->  18
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.17307692307692307
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0004867788461538461
learning rate from cosine_annealing_LR -->  0.0004867788461538461
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:38:12] 00018, lr 0.00049, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 384,  527,  456,  ...,  311,  490, 1411], device='cuda:0')
Generated mixing coefficient (lambda): 0.9496592932052692
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.733078956604004
Classification loss 2 during warmup: 4.929637908935547
Alignment loss during warmup: 4.605166912078857
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.34468936920166
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.599300384521484
Contrastive Loss: tensor(6.6424, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.7331, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9296, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3447, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5993, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.6424, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.7331, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9296, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3447, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5993, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.642434597015381 cls_loss1: 9.733078956604004 cls_loss2: 4.929637908935547 ent_loss: 4.34468936920166 ne_loss: -4.599300384521484 align_loss: 4.605166912078857
Total loss: 25.65570831298828
[2024-05-02 19:38:13] 00018, contrastive_loss 6.64243, cls_loss1 9.73308, cls_loss2 4.92964, ent_loss 4.34469, ne_loss -4.59930, align_loss 4.60517, 
Training completed.
training step completed
 18% 19/104 [03:50<03:34,  2.52s/it]iteration number -->  19
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.18269230769230768
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0005138221153846153
learning rate from cosine_annealing_LR -->  0.0005138221153846153
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:38:14] 00019, lr 0.00051, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 135,  277, 1293,  ...,  386,  103,  551], device='cuda:0')
Generated mixing coefficient (lambda): 0.7017481135576186
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.981338500976562
Classification loss 2 during warmup: 4.809670925140381
Alignment loss during warmup: 4.6051716804504395
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.39208984375
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.595959663391113
Contrastive Loss: tensor(6.6843, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.9813, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8097, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3921, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5960, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.6843, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.9813, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8097, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3921, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5960, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.684315204620361 cls_loss1: 9.981338500976562 cls_loss2: 4.809670925140381 ent_loss: 4.39208984375 ne_loss: -4.595959663391113 align_loss: 4.6051716804504395
Total loss: 25.876625061035156
[2024-05-02 19:38:15] 00019, contrastive_loss 6.68432, cls_loss1 9.98134, cls_loss2 4.80967, ent_loss 4.39209, ne_loss -4.59596, align_loss 4.60517, 
Training completed.
training step completed
 19% 20/104 [03:52<03:30,  2.50s/it]iteration number -->  20
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.19230769230769232
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0005408653846153846
learning rate from cosine_annealing_LR -->  0.0005408653846153846
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:38:17] 00020, lr 0.00054, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1201,  357,  205,  ...,  451,  319,  934], device='cuda:0')
Generated mixing coefficient (lambda): 0.7138180268008237
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.85722541809082
Classification loss 2 during warmup: 4.771684169769287
Alignment loss during warmup: 4.60516357421875
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.3801093101501465
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.595166206359863
Contrastive Loss: tensor(6.6388, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.8572, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7717, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3801, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5952, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.6388, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.8572, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7717, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3801, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5952, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.638808250427246 cls_loss1: 9.85722541809082 cls_loss2: 4.771684169769287 ent_loss: 4.3801093101501465 ne_loss: -4.595166206359863 align_loss: 4.60516357421875
Total loss: 25.657825469970703
[2024-05-02 19:38:18] 00020, contrastive_loss 6.63881, cls_loss1 9.85723, cls_loss2 4.77168, ent_loss 4.38011, ne_loss -4.59517, align_loss 4.60516, 
Training completed.
training step completed
 20% 21/104 [03:55<03:26,  2.49s/it]iteration number -->  21
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.20192307692307693
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0005679086538461539
learning rate from cosine_annealing_LR -->  0.0005679086538461539
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:38:19] 00021, lr 0.00057, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([  60,  619,  153,  ..., 1412,  319, 1012], device='cuda:0')
Generated mixing coefficient (lambda): 0.43425720846941174
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.812929153442383
Classification loss 2 during warmup: 4.77321720123291
Alignment loss during warmup: 4.605171203613281
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.410955905914307
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.593671798706055
Contrastive Loss: tensor(6.6508, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.8129, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7732, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4110, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5937, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.6508, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.8129, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7732, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4110, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5937, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.650817394256592 cls_loss1: 9.812929153442383 cls_loss2: 4.77321720123291 ent_loss: 4.410955905914307 ne_loss: -4.593671798706055 align_loss: 4.605171203613281
Total loss: 25.6594181060791
[2024-05-02 19:38:20] 00021, contrastive_loss 6.65082, cls_loss1 9.81293, cls_loss2 4.77322, ent_loss 4.41096, ne_loss -4.59367, align_loss 4.60517, 
Training completed.
training step completed
 21% 22/104 [03:57<03:23,  2.48s/it]iteration number -->  22
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.21153846153846154
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0005949519230769231
learning rate from cosine_annealing_LR -->  0.0005949519230769231
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:38:22] 00022, lr 0.00059, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1230,   85, 1360,  ...,  272,  215,  780], device='cuda:0')
Generated mixing coefficient (lambda): 0.08478169877718927
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.67182731628418
Classification loss 2 during warmup: 4.929108142852783
Alignment loss during warmup: 4.6051716804504395
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.360296249389648
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596994876861572
Contrastive Loss: tensor(6.7086, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.6718, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9291, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3603, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5970, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.7086, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.6718, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9291, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3603, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5970, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.708585739135742 cls_loss1: 9.67182731628418 cls_loss2: 4.929108142852783 ent_loss: 4.360296249389648 ne_loss: -4.596994876861572 align_loss: 4.6051716804504395
Total loss: 25.677993774414062
[2024-05-02 19:38:22] 00022, contrastive_loss 6.70859, cls_loss1 9.67183, cls_loss2 4.92911, ent_loss 4.36030, ne_loss -4.59699, align_loss 4.60517, 
Training completed.
training step completed
 22% 23/104 [04:00<03:17,  2.44s/it]iteration number -->  23
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.22115384615384615
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0006219951923076922
learning rate from cosine_annealing_LR -->  0.0006219951923076922
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:38:24] 00023, lr 0.00062, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([159, 108, 249,  ..., 109, 774,  79], device='cuda:0')
Generated mixing coefficient (lambda): 0.6810187749184367
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.812131881713867
Classification loss 2 during warmup: 4.793696403503418
Alignment loss during warmup: 4.605171203613281
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.397387981414795
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5936384201049805
Contrastive Loss: tensor(6.6666, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.8121, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7937, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3974, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5936, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.6666, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.8121, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7937, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3974, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5936, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.66663122177124 cls_loss1: 9.812131881713867 cls_loss2: 4.793696403503418 ent_loss: 4.397387981414795 ne_loss: -4.5936384201049805 align_loss: 4.605171203613281
Total loss: 25.681381225585938
[2024-05-02 19:38:25] 00023, contrastive_loss 6.66663, cls_loss1 9.81213, cls_loss2 4.79370, ent_loss 4.39739, ne_loss -4.59364, align_loss 4.60517, 
Training completed.
training step completed
 23% 24/104 [04:02<03:12,  2.40s/it]iteration number -->  24
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.23076923076923078
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0006490384615384615
learning rate from cosine_annealing_LR -->  0.0006490384615384615
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:38:26] 00024, lr 0.00065, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1073,  951,  567,  ..., 1061,   67,  706], device='cuda:0')
Generated mixing coefficient (lambda): 0.847792852616476
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.704689025878906
Classification loss 2 during warmup: 4.898778438568115
Alignment loss during warmup: 4.605168342590332
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.375565052032471
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.595260143280029
Contrastive Loss: tensor(6.6683, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.7047, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8988, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3756, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5953, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.6683, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.7047, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8988, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3756, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5953, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.6683244705200195 cls_loss1: 9.704689025878906 cls_loss2: 4.898778438568115 ent_loss: 4.375565052032471 ne_loss: -4.595260143280029 align_loss: 4.605168342590332
Total loss: 25.657264709472656
[2024-05-02 19:38:27] 00024, contrastive_loss 6.66832, cls_loss1 9.70469, cls_loss2 4.89878, ent_loss 4.37557, ne_loss -4.59526, align_loss 4.60517, 
Training completed.
training step completed
 24% 25/104 [04:04<03:08,  2.38s/it]iteration number -->  25
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.2403846153846154
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0006760817307692308
learning rate from cosine_annealing_LR -->  0.0006760817307692308
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:38:29] 00025, lr 0.00068, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 441, 1138, 1166,  ...,  209,  274,   53], device='cuda:0')
Generated mixing coefficient (lambda): 0.3230990163566997
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.861140251159668
Classification loss 2 during warmup: 4.777268409729004
Alignment loss during warmup: 4.605166435241699
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.4169111251831055
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.593214988708496
Contrastive Loss: tensor(6.5590, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.8611, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7773, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4169, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5932, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.5590, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.8611, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7773, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4169, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5932, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.559042930603027 cls_loss1: 9.861140251159668 cls_loss2: 4.777268409729004 ent_loss: 4.4169111251831055 ne_loss: -4.593214988708496 align_loss: 4.605166435241699
Total loss: 25.62631607055664
[2024-05-02 19:38:29] 00025, contrastive_loss 6.55904, cls_loss1 9.86114, cls_loss2 4.77727, ent_loss 4.41691, ne_loss -4.59321, align_loss 4.60517, 
Training completed.
training step completed
 25% 26/104 [04:07<03:04,  2.37s/it]iteration number -->  26
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.25
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.000703125
learning rate from cosine_annealing_LR -->  0.000703125
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:38:31] 00026, lr 0.00070, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1297,  945, 1191,  ...,  777,  482,  332], device='cuda:0')
Generated mixing coefficient (lambda): 0.15539908865648952
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.775039672851562
Classification loss 2 during warmup: 4.910409450531006
Alignment loss during warmup: 4.60517692565918
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.399569511413574
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.595027446746826
Contrastive Loss: tensor(6.5725, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.7750, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9104, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3996, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5950, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.5725, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.7750, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9104, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3996, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5950, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.572536468505859 cls_loss1: 9.775039672851562 cls_loss2: 4.910409450531006 ent_loss: 4.399569511413574 ne_loss: -4.595027446746826 align_loss: 4.60517692565918
Total loss: 25.66770362854004
[2024-05-02 19:38:32] 00026, contrastive_loss 6.57254, cls_loss1 9.77504, cls_loss2 4.91041, ent_loss 4.39957, ne_loss -4.59503, align_loss 4.60518, 
Training completed.
training step completed
 26% 27/104 [04:09<03:01,  2.36s/it]iteration number -->  27
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.25961538461538464
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0007301682692307692
learning rate from cosine_annealing_LR -->  0.0007301682692307692
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:38:33] 00027, lr 0.00073, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([  25,  865,  875,  ...,  818, 1061, 1348], device='cuda:0')
Generated mixing coefficient (lambda): 0.6123733632361088
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.78704833984375
Classification loss 2 during warmup: 4.771223545074463
Alignment loss during warmup: 4.605175971984863
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.429267883300781
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5911641120910645
Contrastive Loss: tensor(6.5946, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.7870, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7712, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4293, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5912, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.5946, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.7870, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7712, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4293, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5912, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.594581604003906 cls_loss1: 9.78704833984375 cls_loss2: 4.771223545074463 ent_loss: 4.429267883300781 ne_loss: -4.5911641120910645 align_loss: 4.605175971984863
Total loss: 25.596134185791016
[2024-05-02 19:38:34] 00027, contrastive_loss 6.59458, cls_loss1 9.78705, cls_loss2 4.77122, ent_loss 4.42927, ne_loss -4.59116, align_loss 4.60518, 
Training completed.
training step completed
 27% 28/104 [04:11<03:01,  2.39s/it]iteration number -->  28
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.2692307692307692
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0007572115384615384
learning rate from cosine_annealing_LR -->  0.0007572115384615384
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:38:36] 00028, lr 0.00076, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 259,  712, 1012,  ...,  640,  507,  888], device='cuda:0')
Generated mixing coefficient (lambda): 0.966853169621924
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.597381591796875
Classification loss 2 during warmup: 4.91267728805542
Alignment loss during warmup: 4.605172157287598
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.3829345703125
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596585273742676
Contrastive Loss: tensor(6.6216, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.5974, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9127, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3829, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5966, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.6216, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.5974, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9127, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3829, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5966, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.621606826782227 cls_loss1: 9.597381591796875 cls_loss2: 4.91267728805542 ent_loss: 4.3829345703125 ne_loss: -4.596585273742676 align_loss: 4.605172157287598
Total loss: 25.52318572998047
[2024-05-02 19:38:37] 00028, contrastive_loss 6.62161, cls_loss1 9.59738, cls_loss2 4.91268, ent_loss 4.38293, ne_loss -4.59659, align_loss 4.60517, 
Training completed.
training step completed
 28% 29/104 [04:14<03:00,  2.41s/it]iteration number -->  29
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.27884615384615385
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0007842548076923076
learning rate from cosine_annealing_LR -->  0.0007842548076923076
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:38:38] 00029, lr 0.00078, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([119, 136, 703,  ..., 456,  88, 751], device='cuda:0')
Generated mixing coefficient (lambda): 0.994367470960539
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.614826202392578
Classification loss 2 during warmup: 4.965087890625
Alignment loss during warmup: 4.605166912078857
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.3701863288879395
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596406936645508
Contrastive Loss: tensor(6.5855, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.6148, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9651, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.3702, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5964, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.5855, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.6148, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9651, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.3702, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5964, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.585465908050537 cls_loss1: 9.614826202392578 cls_loss2: 4.965087890625 ent_loss: 4.3701863288879395 ne_loss: -4.596406936645508 align_loss: 4.605166912078857
Total loss: 25.544326782226562
[2024-05-02 19:38:39] 00029, contrastive_loss 6.58547, cls_loss1 9.61483, cls_loss2 4.96509, ent_loss 4.37019, ne_loss -4.59641, align_loss 4.60517, 
Training completed.
training step completed
 29% 30/104 [04:16<02:57,  2.40s/it]iteration number -->  30
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.28846153846153844
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0008112980769230769
learning rate from cosine_annealing_LR -->  0.0008112980769230769
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:38:41] 00030, lr 0.00081, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([  33,  825,  712,  ...,  311, 1132,  901], device='cuda:0')
Generated mixing coefficient (lambda): 0.7151287286340898
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.71316909790039
Classification loss 2 during warmup: 4.768132209777832
Alignment loss during warmup: 4.6051764488220215
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.444647789001465
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.59342098236084
Contrastive Loss: tensor(6.5175, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.7132, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7681, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4446, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5934, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.5175, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.7132, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7681, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4446, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5934, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.517505645751953 cls_loss1: 9.71316909790039 cls_loss2: 4.768132209777832 ent_loss: 4.444647789001465 ne_loss: -4.59342098236084 align_loss: 4.6051764488220215
Total loss: 25.455209732055664
[2024-05-02 19:38:41] 00030, contrastive_loss 6.51751, cls_loss1 9.71317, cls_loss2 4.76813, ent_loss 4.44465, ne_loss -4.59342, align_loss 4.60518, 
Training completed.
training step completed
 30% 31/104 [04:19<02:52,  2.37s/it]iteration number -->  31
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.2980769230769231
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0008383413461538461
learning rate from cosine_annealing_LR -->  0.0008383413461538461
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:38:43] 00031, lr 0.00084, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 146,  716,  498,  ..., 1174, 1432, 1321], device='cuda:0')
Generated mixing coefficient (lambda): 0.30155168369520463
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.717403411865234
Classification loss 2 during warmup: 4.7585883140563965
Alignment loss during warmup: 4.605167865753174
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.444869041442871
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.59221887588501
Contrastive Loss: tensor(6.5546, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.7174, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7586, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4449, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5922, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.5546, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.7174, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7586, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4449, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5922, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.554622650146484 cls_loss1: 9.717403411865234 cls_loss2: 4.7585883140563965 ent_loss: 4.444869041442871 ne_loss: -4.59221887588501 align_loss: 4.605167865753174
Total loss: 25.488431930541992
[2024-05-02 19:38:44] 00031, contrastive_loss 6.55462, cls_loss1 9.71740, cls_loss2 4.75859, ent_loss 4.44487, ne_loss -4.59222, align_loss 4.60517, 
Training completed.
training step completed
 31% 32/104 [04:21<02:49,  2.35s/it]iteration number -->  32
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.3076923076923077
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0008653846153846154
learning rate from cosine_annealing_LR -->  0.0008653846153846154
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:38:45] 00032, lr 0.00087, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 674,   49,  601,  ...,  183, 1181, 1028], device='cuda:0')
Generated mixing coefficient (lambda): 0.7016429835864404
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.631755828857422
Classification loss 2 during warmup: 4.784599781036377
Alignment loss during warmup: 4.605171203613281
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.449518203735352
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591695785522461
Contrastive Loss: tensor(6.5242, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.6318, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7846, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4495, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5917, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.5242, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.6318, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7846, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4495, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5917, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.524230003356934 cls_loss1: 9.631755828857422 cls_loss2: 4.784599781036377 ent_loss: 4.449518203735352 ne_loss: -4.591695785522461 align_loss: 4.605171203613281
Total loss: 25.403579711914062
[2024-05-02 19:38:46] 00032, contrastive_loss 6.52423, cls_loss1 9.63176, cls_loss2 4.78460, ent_loss 4.44952, ne_loss -4.59170, align_loss 4.60517, 
Training completed.
training step completed
 32% 33/104 [04:23<02:45,  2.34s/it]iteration number -->  33
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.3173076923076923
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0008924278846153844
learning rate from cosine_annealing_LR -->  0.0008924278846153844
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:38:48] 00033, lr 0.00089, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 389,  522, 1173,  ...,  544,   25,  128], device='cuda:0')
Generated mixing coefficient (lambda): 0.05983513758470556
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.58210563659668
Classification loss 2 during warmup: 4.9396748542785645
Alignment loss during warmup: 4.605169773101807
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.414516925811768
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.595743179321289
Contrastive Loss: tensor(6.5127, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.5821, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9397, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4145, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5957, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.5127, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.5821, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9397, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4145, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5957, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.512725353240967 cls_loss1: 9.58210563659668 cls_loss2: 4.9396748542785645 ent_loss: 4.414516925811768 ne_loss: -4.595743179321289 align_loss: 4.605169773101807
Total loss: 25.45844841003418
[2024-05-02 19:38:48] 00033, contrastive_loss 6.51273, cls_loss1 9.58211, cls_loss2 4.93967, ent_loss 4.41452, ne_loss -4.59574, align_loss 4.60517, 
Training completed.
training step completed
 33% 34/104 [04:26<02:45,  2.36s/it]iteration number -->  34
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.3269230769230769
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0009194711538461537
learning rate from cosine_annealing_LR -->  0.0009194711538461537
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:38:50] 00034, lr 0.00092, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1420,  284, 1424,  ..., 1045,   62,  807], device='cuda:0')
Generated mixing coefficient (lambda): 0.6947358508038756
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.59562873840332
Classification loss 2 during warmup: 4.766140460968018
Alignment loss during warmup: 4.6051716804504395
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.45319938659668
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591297149658203
Contrastive Loss: tensor(6.4787, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.5956, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7661, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4532, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5913, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4787, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.5956, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7661, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4532, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5913, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.478716850280762 cls_loss1: 9.59562873840332 cls_loss2: 4.766140460968018 ent_loss: 4.45319938659668 ne_loss: -4.591297149658203 align_loss: 4.6051716804504395
Total loss: 25.307558059692383
[2024-05-02 19:38:51] 00034, contrastive_loss 6.47872, cls_loss1 9.59563, cls_loss2 4.76614, ent_loss 4.45320, ne_loss -4.59130, align_loss 4.60517, 
Training completed.
training step completed
 34% 35/104 [04:28<02:44,  2.39s/it]iteration number -->  35
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.33653846153846156
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.000946514423076923
learning rate from cosine_annealing_LR -->  0.000946514423076923
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:38:53] 00035, lr 0.00095, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 819,  621, 1404,  ...,  464,  752, 1270], device='cuda:0')
Generated mixing coefficient (lambda): 0.3123275352017424
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.611434936523438
Classification loss 2 during warmup: 4.748083591461182
Alignment loss during warmup: 4.605169296264648
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.45943546295166
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591184616088867
Contrastive Loss: tensor(6.6070, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.6114, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7481, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4594, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5912, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.6070, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.6114, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7481, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4594, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5912, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.606997489929199 cls_loss1: 9.611434936523438 cls_loss2: 4.748083591461182 ent_loss: 4.45943546295166 ne_loss: -4.591184616088867 align_loss: 4.605169296264648
Total loss: 25.43993377685547
[2024-05-02 19:38:54] 00035, contrastive_loss 6.60700, cls_loss1 9.61143, cls_loss2 4.74808, ent_loss 4.45944, ne_loss -4.59118, align_loss 4.60517, 
Training completed.
training step completed
 35% 36/104 [04:31<02:47,  2.47s/it]iteration number -->  36
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.34615384615384615
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0009735576923076922
learning rate from cosine_annealing_LR -->  0.0009735576923076922
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:38:55] 00036, lr 0.00097, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 287,  147, 1340,  ...,  199,  171, 1274], device='cuda:0')
Generated mixing coefficient (lambda): 0.20884572042828553
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.528326034545898
Classification loss 2 during warmup: 4.796423435211182
Alignment loss during warmup: 4.605168342590332
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.448139667510986
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592999458312988
Contrastive Loss: tensor(6.5437, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.5283, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7964, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4481, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5930, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.5437, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.5283, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7964, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4481, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5930, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.543684959411621 cls_loss1: 9.528326034545898 cls_loss2: 4.796423435211182 ent_loss: 4.448139667510986 ne_loss: -4.592999458312988 align_loss: 4.605168342590332
Total loss: 25.32874298095703
[2024-05-02 19:38:56] 00036, contrastive_loss 6.54368, cls_loss1 9.52833, cls_loss2 4.79642, ent_loss 4.44814, ne_loss -4.59300, align_loss 4.60517, 
Training completed.
training step completed
 36% 37/104 [04:33<02:43,  2.43s/it]iteration number -->  37
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.3557692307692308
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0010006009615384614
learning rate from cosine_annealing_LR -->  0.0010006009615384614
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:38:57] 00037, lr 0.00100, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1125, 1245,  214,  ..., 1430,  999,  222], device='cuda:0')
Generated mixing coefficient (lambda): 0.1274093315039185
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.514106750488281
Classification loss 2 during warmup: 4.88893985748291
Alignment loss during warmup: 4.605168342590332
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.43831729888916
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.59412956237793
Contrastive Loss: tensor(6.4139, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.5141, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8889, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4383, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5941, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4139, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.5141, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8889, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4383, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5941, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.413915157318115 cls_loss1: 9.514106750488281 cls_loss2: 4.88893985748291 ent_loss: 4.43831729888916 ne_loss: -4.59412956237793 align_loss: 4.605168342590332
Total loss: 25.266319274902344
[2024-05-02 19:38:58] 00037, contrastive_loss 6.41392, cls_loss1 9.51411, cls_loss2 4.88894, ent_loss 4.43832, ne_loss -4.59413, align_loss 4.60517, 
Training completed.
training step completed
 37% 38/104 [04:35<02:38,  2.40s/it]iteration number -->  38
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.36538461538461536
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0010276442307692306
learning rate from cosine_annealing_LR -->  0.0010276442307692306
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:00] 00038, lr 0.00103, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1251, 1171, 1387,  ...,  760,  701,  210], device='cuda:0')
Generated mixing coefficient (lambda): 0.854798041445862
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.53604793548584
Classification loss 2 during warmup: 4.846578598022461
Alignment loss during warmup: 4.605172634124756
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.439639091491699
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592792987823486
Contrastive Loss: tensor(6.4848, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.5360, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8466, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4396, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5928, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4848, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.5360, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8466, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4396, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5928, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.484804630279541 cls_loss1: 9.53604793548584 cls_loss2: 4.846578598022461 ent_loss: 4.439639091491699 ne_loss: -4.592792987823486 align_loss: 4.605172634124756
Total loss: 25.3194522857666
[2024-05-02 19:39:01] 00038, contrastive_loss 6.48480, cls_loss1 9.53605, cls_loss2 4.84658, ent_loss 4.43964, ne_loss -4.59279, align_loss 4.60517, 
Training completed.
training step completed
 38% 39/104 [04:38<02:34,  2.38s/it]iteration number -->  39
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.375
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0010546874999999999
learning rate from cosine_annealing_LR -->  0.0010546874999999999
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:02] 00039, lr 0.00105, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 520, 1114,  217,  ...,  715,  846,  997], device='cuda:0')
Generated mixing coefficient (lambda): 0.03151555454460563
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.504000663757324
Classification loss 2 during warmup: 4.919370174407959
Alignment loss during warmup: 4.605169296264648
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.426732540130615
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.594703197479248
Contrastive Loss: tensor(6.5311, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.5040, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9194, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4267, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5947, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.5311, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.5040, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9194, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4267, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5947, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.531073570251465 cls_loss1: 9.504000663757324 cls_loss2: 4.919370174407959 ent_loss: 4.426732540130615 ne_loss: -4.594703197479248 align_loss: 4.605169296264648
Total loss: 25.391643524169922
[2024-05-02 19:39:03] 00039, contrastive_loss 6.53107, cls_loss1 9.50400, cls_loss2 4.91937, ent_loss 4.42673, ne_loss -4.59470, align_loss 4.60517, 
Training completed.
training step completed
 38% 40/104 [04:40<02:30,  2.36s/it]iteration number -->  40
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.38461538461538464
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0010817307692307693
learning rate from cosine_annealing_LR -->  0.0010817307692307693
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:04] 00040, lr 0.00108, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 272, 1340,   30,  ..., 1030,  654,  897], device='cuda:0')
Generated mixing coefficient (lambda): 0.8096430518004919
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.568534851074219
Classification loss 2 during warmup: 4.838690757751465
Alignment loss during warmup: 4.60517692565918
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.451603889465332
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5908660888671875
Contrastive Loss: tensor(6.5405, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.5685, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8387, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4516, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5909, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.5405, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.5685, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8387, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4516, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5909, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.540472030639648 cls_loss1: 9.568534851074219 cls_loss2: 4.838690757751465 ent_loss: 4.451603889465332 ne_loss: -4.5908660888671875 align_loss: 4.60517692565918
Total loss: 25.413610458374023
[2024-05-02 19:39:05] 00040, contrastive_loss 6.54047, cls_loss1 9.56853, cls_loss2 4.83869, ent_loss 4.45160, ne_loss -4.59087, align_loss 4.60518, 
Training completed.
training step completed
 39% 41/104 [04:42<02:30,  2.39s/it]iteration number -->  41
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.3942307692307692
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0011087740384615383
learning rate from cosine_annealing_LR -->  0.0011087740384615383
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:07] 00041, lr 0.00111, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1229,  804,  258,  ...,  801,  308,  968], device='cuda:0')
Generated mixing coefficient (lambda): 0.9055606749853524
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.46229362487793
Classification loss 2 during warmup: 4.881067752838135
Alignment loss during warmup: 4.605170726776123
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.430621147155762
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592532157897949
Contrastive Loss: tensor(6.4903, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.4623, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8811, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4306, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5925, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4903, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.4623, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8811, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4306, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5925, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.490340232849121 cls_loss1: 9.46229362487793 cls_loss2: 4.881067752838135 ent_loss: 4.430621147155762 ne_loss: -4.592532157897949 align_loss: 4.605170726776123
Total loss: 25.276962280273438
[2024-05-02 19:39:08] 00041, contrastive_loss 6.49034, cls_loss1 9.46229, cls_loss2 4.88107, ent_loss 4.43062, ne_loss -4.59253, align_loss 4.60517, 
Training completed.
training step completed
 40% 42/104 [04:45<02:28,  2.39s/it]iteration number -->  42
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.40384615384615385
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0011358173076923077
learning rate from cosine_annealing_LR -->  0.0011358173076923077
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:09] 00042, lr 0.00114, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 659,  786,  192,  ...,  621, 1018,  814], device='cuda:0')
Generated mixing coefficient (lambda): 0.9152391684362483
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.432186126708984
Classification loss 2 during warmup: 4.843931674957275
Alignment loss during warmup: 4.605170249938965
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.43896484375
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.593008995056152
Contrastive Loss: tensor(6.4234, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.4322, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8439, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4390, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5930, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4234, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.4322, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8439, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4390, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5930, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.423426151275635 cls_loss1: 9.432186126708984 cls_loss2: 4.843931674957275 ent_loss: 4.43896484375 ne_loss: -4.593008995056152 align_loss: 4.605170249938965
Total loss: 25.15066909790039
[2024-05-02 19:39:10] 00042, contrastive_loss 6.42343, cls_loss1 9.43219, cls_loss2 4.84393, ent_loss 4.43896, ne_loss -4.59301, align_loss 4.60517, 
Training completed.
training step completed
 41% 43/104 [04:47<02:24,  2.37s/it]iteration number -->  43
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.41346153846153844
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0011628605769230767
learning rate from cosine_annealing_LR -->  0.0011628605769230767
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:12] 00043, lr 0.00116, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 226, 1420,  722,  ...,  156,  572,  909], device='cuda:0')
Generated mixing coefficient (lambda): 0.9567554696582575
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.355255126953125
Classification loss 2 during warmup: 4.920459270477295
Alignment loss during warmup: 4.60516881942749
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.430728912353516
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.593039512634277
Contrastive Loss: tensor(6.4692, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3553, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9205, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4307, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5930, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4692, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3553, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9205, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4307, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5930, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.469237327575684 cls_loss1: 9.355255126953125 cls_loss2: 4.920459270477295 ent_loss: 4.430728912353516 ne_loss: -4.593039512634277 align_loss: 4.60516881942749
Total loss: 25.18781089782715
[2024-05-02 19:39:12] 00043, contrastive_loss 6.46924, cls_loss1 9.35526, cls_loss2 4.92046, ent_loss 4.43073, ne_loss -4.59304, align_loss 4.60517, 
Training completed.
training step completed
 42% 44/104 [04:49<02:20,  2.35s/it]iteration number -->  44
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.4230769230769231
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0011899038461538462
learning rate from cosine_annealing_LR -->  0.0011899038461538462
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:14] 00044, lr 0.00119, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1418,  754,  971,  ...,  718, 1242,  543], device='cuda:0')
Generated mixing coefficient (lambda): 0.20948855925138962
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.451824188232422
Classification loss 2 during warmup: 4.770298480987549
Alignment loss during warmup: 4.605166435241699
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.471983432769775
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.59102201461792
Contrastive Loss: tensor(6.4381, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.4518, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7703, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4720, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5910, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4381, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.4518, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7703, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4720, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5910, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.438090801239014 cls_loss1: 9.451824188232422 cls_loss2: 4.770298480987549 ent_loss: 4.471983432769775 ne_loss: -4.59102201461792 align_loss: 4.605166435241699
Total loss: 25.146339416503906
[2024-05-02 19:39:15] 00044, contrastive_loss 6.43809, cls_loss1 9.45182, cls_loss2 4.77030, ent_loss 4.47198, ne_loss -4.59102, align_loss 4.60517, 
Training completed.
training step completed
 43% 45/104 [04:52<02:17,  2.34s/it]iteration number -->  45
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.4326923076923077
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0012169471153846154
learning rate from cosine_annealing_LR -->  0.0012169471153846154
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:16] 00045, lr 0.00122, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 106,  982, 1011,  ...,   55,  296,  206], device='cuda:0')
Generated mixing coefficient (lambda): 0.7081504442623683
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.381904602050781
Classification loss 2 during warmup: 4.716943264007568
Alignment loss during warmup: 4.605170249938965
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.495741367340088
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591409683227539
Contrastive Loss: tensor(6.5071, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3819, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7169, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4957, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5914, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.5071, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3819, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7169, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4957, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5914, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.5071024894714355 cls_loss1: 9.381904602050781 cls_loss2: 4.716943264007568 ent_loss: 4.495741367340088 ne_loss: -4.591409683227539 align_loss: 4.605170249938965
Total loss: 25.11545181274414
[2024-05-02 19:39:17] 00045, contrastive_loss 6.50710, cls_loss1 9.38190, cls_loss2 4.71694, ent_loss 4.49574, ne_loss -4.59141, align_loss 4.60517, 
Training completed.
training step completed
 44% 46/104 [04:54<02:15,  2.34s/it]iteration number -->  46
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.4423076923076923
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0012439903846153844
learning rate from cosine_annealing_LR -->  0.0012439903846153844
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:19] 00046, lr 0.00124, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 960,  514,  681,  ...,  396, 1279, 1057], device='cuda:0')
Generated mixing coefficient (lambda): 0.6548990418241223
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.419468879699707
Classification loss 2 during warmup: 4.735354423522949
Alignment loss during warmup: 4.605169296264648
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.501074314117432
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.589571952819824
Contrastive Loss: tensor(6.4495, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.4195, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7354, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5011, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5896, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4495, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.4195, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7354, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5011, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5896, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.449485778808594 cls_loss1: 9.419468879699707 cls_loss2: 4.735354423522949 ent_loss: 4.501074314117432 ne_loss: -4.589571952819824 align_loss: 4.605169296264648
Total loss: 25.120981216430664
[2024-05-02 19:39:19] 00046, contrastive_loss 6.44949, cls_loss1 9.41947, cls_loss2 4.73535, ent_loss 4.50107, ne_loss -4.58957, align_loss 4.60517, 
Training completed.
training step completed
 45% 47/104 [04:56<02:13,  2.33s/it]iteration number -->  47
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.4519230769230769
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0012710336538461536
learning rate from cosine_annealing_LR -->  0.0012710336538461536
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:21] 00047, lr 0.00127, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 568, 1180,  177,  ..., 1247,  921,  502], device='cuda:0')
Generated mixing coefficient (lambda): 0.4720387407503516
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.489702224731445
Classification loss 2 during warmup: 4.69255256652832
Alignment loss during warmup: 4.605170249938965
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.5208892822265625
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592811107635498
Contrastive Loss: tensor(6.3428, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.4897, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6926, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5209, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5928, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3428, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.4897, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6926, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5209, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5928, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.342804431915283 cls_loss1: 9.489702224731445 cls_loss2: 4.69255256652832 ent_loss: 4.5208892822265625 ne_loss: -4.592811107635498 align_loss: 4.605170249938965
Total loss: 25.058307647705078
[2024-05-02 19:39:22] 00047, contrastive_loss 6.34280, cls_loss1 9.48970, cls_loss2 4.69255, ent_loss 4.52089, ne_loss -4.59281, align_loss 4.60517, 
Training completed.
training step completed
 46% 48/104 [04:59<02:10,  2.33s/it]iteration number -->  48
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.46153846153846156
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.001298076923076923
learning rate from cosine_annealing_LR -->  0.001298076923076923
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:23] 00048, lr 0.00130, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([176, 405, 996,  ..., 413, 253, 927], device='cuda:0')
Generated mixing coefficient (lambda): 0.1178386778555579
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.391082763671875
Classification loss 2 during warmup: 4.852146625518799
Alignment loss during warmup: 4.605173110961914
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.473959922790527
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.593141555786133
Contrastive Loss: tensor(6.4479, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3911, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8521, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4740, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5931, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4479, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3911, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8521, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4740, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5931, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.44791841506958 cls_loss1: 9.391082763671875 cls_loss2: 4.852146625518799 ent_loss: 4.473959922790527 ne_loss: -4.593141555786133 align_loss: 4.605173110961914
Total loss: 25.177139282226562
[2024-05-02 19:39:24] 00048, contrastive_loss 6.44792, cls_loss1 9.39108, cls_loss2 4.85215, ent_loss 4.47396, ne_loss -4.59314, align_loss 4.60517, 
Training completed.
training step completed
 47% 49/104 [05:01<02:08,  2.34s/it]iteration number -->  49
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.47115384615384615
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.001325120192307692
learning rate from cosine_annealing_LR -->  0.001325120192307692
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:26] 00049, lr 0.00133, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 551,  291, 1274,  ...,  972, 1357,   65], device='cuda:0')
Generated mixing coefficient (lambda): 0.6777574238898412
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.386290550231934
Classification loss 2 during warmup: 4.699426174163818
Alignment loss during warmup: 4.6051716804504395
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.516087532043457
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592510223388672
Contrastive Loss: tensor(6.4382, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3863, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6994, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5161, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5925, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4382, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3863, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6994, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5161, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5925, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.438206195831299 cls_loss1: 9.386290550231934 cls_loss2: 4.699426174163818 ent_loss: 4.516087532043457 ne_loss: -4.592510223388672 align_loss: 4.6051716804504395
Total loss: 25.052669525146484
[2024-05-02 19:39:26] 00049, contrastive_loss 6.43821, cls_loss1 9.38629, cls_loss2 4.69943, ent_loss 4.51609, ne_loss -4.59251, align_loss 4.60517, 
Training completed.
training step completed
 48% 50/104 [05:03<02:05,  2.33s/it]iteration number -->  50
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.4807692307692308
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0013521634615384615
learning rate from cosine_annealing_LR -->  0.0013521634615384615
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:28] 00050, lr 0.00135, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1296,  168,  710,  ...,  845,  682,  788], device='cuda:0')
Generated mixing coefficient (lambda): 0.26845609374919954
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.354448318481445
Classification loss 2 during warmup: 4.735942840576172
Alignment loss during warmup: 4.605166912078857
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.506223678588867
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592042922973633
Contrastive Loss: tensor(6.4106, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3544, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7359, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5062, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5920, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4106, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3544, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7359, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5062, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5920, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.410625457763672 cls_loss1: 9.354448318481445 cls_loss2: 4.735942840576172 ent_loss: 4.506223678588867 ne_loss: -4.592042922973633 align_loss: 4.605166912078857
Total loss: 25.02036476135254
[2024-05-02 19:39:29] 00050, contrastive_loss 6.41063, cls_loss1 9.35445, cls_loss2 4.73594, ent_loss 4.50622, ne_loss -4.59204, align_loss 4.60517, 
Training completed.
training step completed
 49% 51/104 [05:06<02:03,  2.32s/it]iteration number -->  51
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.49038461538461536
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0013792067307692305
learning rate from cosine_annealing_LR -->  0.0013792067307692305
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:30] 00051, lr 0.00138, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 632, 1438,    0,  ..., 1302, 1288,  133], device='cuda:0')
Generated mixing coefficient (lambda): 0.7157575432612416
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.43093490600586
Classification loss 2 during warmup: 4.706221580505371
Alignment loss during warmup: 4.605169296264648
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.50734281539917
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5915045738220215
Contrastive Loss: tensor(6.5006, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.4309, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7062, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5073, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5915, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.5006, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.4309, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7062, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5073, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5915, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.500622272491455 cls_loss1: 9.43093490600586 cls_loss2: 4.706221580505371 ent_loss: 4.50734281539917 ne_loss: -4.5915045738220215 align_loss: 4.605169296264648
Total loss: 25.15878677368164
[2024-05-02 19:39:31] 00051, contrastive_loss 6.50062, cls_loss1 9.43093, cls_loss2 4.70622, ent_loss 4.50734, ne_loss -4.59150, align_loss 4.60517, 
Training completed.
training step completed
 50% 52/104 [05:08<02:00,  2.32s/it]iteration number -->  52
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.5
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.00140625
learning rate from cosine_annealing_LR -->  0.00140625
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:32] 00052, lr 0.00141, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([  14,  277, 1365,  ...,  904, 1162,  695], device='cuda:0')
Generated mixing coefficient (lambda): 0.026508617968777782
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.412747383117676
Classification loss 2 during warmup: 4.8584370613098145
Alignment loss during warmup: 4.605168342590332
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.454739093780518
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592643737792969
Contrastive Loss: tensor(6.4602, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.4127, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8584, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4547, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5926, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4602, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.4127, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8584, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4547, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5926, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.460188865661621 cls_loss1: 9.412747383117676 cls_loss2: 4.8584370613098145 ent_loss: 4.454739093780518 ne_loss: -4.592643737792969 align_loss: 4.605168342590332
Total loss: 25.19863510131836
[2024-05-02 19:39:33] 00052, contrastive_loss 6.46019, cls_loss1 9.41275, cls_loss2 4.85844, ent_loss 4.45474, ne_loss -4.59264, align_loss 4.60517, 
Training completed.
training step completed
 51% 53/104 [05:10<01:57,  2.31s/it]iteration number -->  53
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.5096153846153846
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.001433293269230769
learning rate from cosine_annealing_LR -->  0.001433293269230769
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:35] 00053, lr 0.00143, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 287,    1,  272,  ..., 1409,  152,  273], device='cuda:0')
Generated mixing coefficient (lambda): 0.45490515074934373
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.572460174560547
Classification loss 2 during warmup: 4.695159435272217
Alignment loss during warmup: 4.605170249938965
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.52872371673584
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591631889343262
Contrastive Loss: tensor(6.3892, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.5725, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6952, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5287, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5916, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3892, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.5725, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6952, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5287, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5916, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.389166355133057 cls_loss1: 9.572460174560547 cls_loss2: 4.695159435272217 ent_loss: 4.52872371673584 ne_loss: -4.591631889343262 align_loss: 4.605170249938965
Total loss: 25.199047088623047
[2024-05-02 19:39:35] 00053, contrastive_loss 6.38917, cls_loss1 9.57246, cls_loss2 4.69516, ent_loss 4.52872, ne_loss -4.59163, align_loss 4.60517, 
Training completed.
training step completed
 52% 54/104 [05:13<01:55,  2.31s/it]iteration number -->  54
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.5192307692307693
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0014603365384615384
learning rate from cosine_annealing_LR -->  0.0014603365384615384
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:37] 00054, lr 0.00146, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1334,  614,  681,  ...,  451,  732,  748], device='cuda:0')
Generated mixing coefficient (lambda): 0.7613414807918842
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.41887378692627
Classification loss 2 during warmup: 4.729923725128174
Alignment loss during warmup: 4.605170249938965
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.508540153503418
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592240333557129
Contrastive Loss: tensor(6.3977, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.4189, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7299, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5085, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5922, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3977, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.4189, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7299, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5085, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5922, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.397749423980713 cls_loss1: 9.41887378692627 cls_loss2: 4.729923725128174 ent_loss: 4.508540153503418 ne_loss: -4.592240333557129 align_loss: 4.605170249938965
Total loss: 25.06801986694336
[2024-05-02 19:39:38] 00054, contrastive_loss 6.39775, cls_loss1 9.41887, cls_loss2 4.72992, ent_loss 4.50854, ne_loss -4.59224, align_loss 4.60517, 
Training completed.
training step completed
 53% 55/104 [05:15<01:53,  2.32s/it]iteration number -->  55
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.5288461538461539
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0014873798076923076
learning rate from cosine_annealing_LR -->  0.0014873798076923076
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:39] 00055, lr 0.00149, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1289,  764, 1366,  ...,  512,   58, 1189], device='cuda:0')
Generated mixing coefficient (lambda): 0.5434771005421039
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.384588241577148
Classification loss 2 during warmup: 4.698757171630859
Alignment loss during warmup: 4.605170249938965
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.523120403289795
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5892510414123535
Contrastive Loss: tensor(6.4172, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3846, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6988, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5231, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5893, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4172, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3846, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6988, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5231, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5893, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.417171478271484 cls_loss1: 9.384588241577148 cls_loss2: 4.698757171630859 ent_loss: 4.523120403289795 ne_loss: -4.5892510414123535 align_loss: 4.605170249938965
Total loss: 25.03955841064453
[2024-05-02 19:39:40] 00055, contrastive_loss 6.41717, cls_loss1 9.38459, cls_loss2 4.69876, ent_loss 4.52312, ne_loss -4.58925, align_loss 4.60517, 
Training completed.
training step completed
 54% 56/104 [05:17<01:51,  2.33s/it]iteration number -->  56
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.5384615384615384
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0015144230769230768
learning rate from cosine_annealing_LR -->  0.0015144230769230768
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:42] 00056, lr 0.00151, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 337,  964,  688,  ...,  143, 1373, 1413], device='cuda:0')
Generated mixing coefficient (lambda): 0.952814040170235
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.357924461364746
Classification loss 2 during warmup: 4.872313976287842
Alignment loss during warmup: 4.605171203613281
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.467656135559082
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591967582702637
Contrastive Loss: tensor(6.3885, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3579, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8723, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4677, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5920, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3885, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3579, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8723, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4677, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5920, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.388488292694092 cls_loss1: 9.357924461364746 cls_loss2: 4.872313976287842 ent_loss: 4.467656135559082 ne_loss: -4.591967582702637 align_loss: 4.605171203613281
Total loss: 25.099586486816406
[2024-05-02 19:39:42] 00056, contrastive_loss 6.38849, cls_loss1 9.35792, cls_loss2 4.87231, ent_loss 4.46766, ne_loss -4.59197, align_loss 4.60517, 
Training completed.
training step completed
 55% 57/104 [05:20<01:48,  2.32s/it]iteration number -->  57
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.5480769230769231
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0015414663461538463
learning rate from cosine_annealing_LR -->  0.0015414663461538463
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:44] 00057, lr 0.00154, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1010, 1087,  335,  ..., 1299, 1217, 1393], device='cuda:0')
Generated mixing coefficient (lambda): 0.32819466244828815
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.429180145263672
Classification loss 2 during warmup: 4.6973724365234375
Alignment loss during warmup: 4.605170249938965
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.525506496429443
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592377662658691
Contrastive Loss: tensor(6.4002, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.4292, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6974, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5255, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5924, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4002, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.4292, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6974, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5255, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5924, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.400224208831787 cls_loss1: 9.429180145263672 cls_loss2: 4.6973724365234375 ent_loss: 4.525506496429443 ne_loss: -4.592377662658691 align_loss: 4.605170249938965
Total loss: 25.065078735351562
[2024-05-02 19:39:45] 00057, contrastive_loss 6.40022, cls_loss1 9.42918, cls_loss2 4.69737, ent_loss 4.52551, ne_loss -4.59238, align_loss 4.60517, 
Training completed.
training step completed
 56% 58/104 [05:22<01:46,  2.31s/it]iteration number -->  58
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.5576923076923077
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0015685096153846153
learning rate from cosine_annealing_LR -->  0.0015685096153846153
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:46] 00058, lr 0.00157, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1233,  209,  251,  ..., 1352,  672,  102], device='cuda:0')
Generated mixing coefficient (lambda): 0.4466392512993226
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.382247924804688
Classification loss 2 during warmup: 4.679831504821777
Alignment loss during warmup: 4.6051716804504395
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.53822135925293
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592686653137207
Contrastive Loss: tensor(6.3741, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3822, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6798, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5382, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5927, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3741, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3822, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6798, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5382, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5927, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.374059200286865 cls_loss1: 9.382247924804688 cls_loss2: 4.679831504821777 ent_loss: 4.53822135925293 ne_loss: -4.592686653137207 align_loss: 4.6051716804504395
Total loss: 24.98684310913086
[2024-05-02 19:39:47] 00058, contrastive_loss 6.37406, cls_loss1 9.38225, cls_loss2 4.67983, ent_loss 4.53822, ne_loss -4.59269, align_loss 4.60517, 
Training completed.
training step completed
 57% 59/104 [05:24<01:44,  2.31s/it]iteration number -->  59
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.5673076923076923
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0015955528846153845
learning rate from cosine_annealing_LR -->  0.0015955528846153845
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:49] 00059, lr 0.00160, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1345,   94,   82,  ..., 1417,  185,  433], device='cuda:0')
Generated mixing coefficient (lambda): 0.3211759764739396
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.31692123413086
Classification loss 2 during warmup: 4.700168132781982
Alignment loss during warmup: 4.605169773101807
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.526695728302002
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591055393218994
Contrastive Loss: tensor(6.3893, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3169, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7002, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5267, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5911, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3893, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3169, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7002, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5267, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5911, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.3893351554870605 cls_loss1: 9.31692123413086 cls_loss2: 4.700168132781982 ent_loss: 4.526695728302002 ne_loss: -4.591055393218994 align_loss: 4.605169773101807
Total loss: 24.947235107421875
[2024-05-02 19:39:49] 00059, contrastive_loss 6.38934, cls_loss1 9.31692, cls_loss2 4.70017, ent_loss 4.52670, ne_loss -4.59106, align_loss 4.60517, 
Training completed.
training step completed
 58% 60/104 [05:27<01:41,  2.31s/it]iteration number -->  60
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.5769230769230769
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0016225961538461537
learning rate from cosine_annealing_LR -->  0.0016225961538461537
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:51] 00060, lr 0.00162, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1132, 1404,  182,  ..., 1239,  356, 1031], device='cuda:0')
Generated mixing coefficient (lambda): 0.8802390895271353
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.34625244140625
Classification loss 2 during warmup: 4.826021671295166
Alignment loss during warmup: 4.605172634124756
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.486035346984863
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.590682029724121
Contrastive Loss: tensor(6.3655, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3463, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8260, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4860, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5907, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3655, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3463, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8260, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4860, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5907, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.365458011627197 cls_loss1: 9.34625244140625 cls_loss2: 4.826021671295166 ent_loss: 4.486035346984863 ne_loss: -4.590682029724121 align_loss: 4.605172634124756
Total loss: 25.03825569152832
[2024-05-02 19:39:52] 00060, contrastive_loss 6.36546, cls_loss1 9.34625, cls_loss2 4.82602, ent_loss 4.48604, ne_loss -4.59068, align_loss 4.60517, 
Training completed.
training step completed
 59% 61/104 [05:29<01:39,  2.31s/it]iteration number -->  61
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.5865384615384616
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0016496394230769232
learning rate from cosine_annealing_LR -->  0.0016496394230769232
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:53] 00061, lr 0.00165, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 755, 1200,  228,  ..., 1319, 1404, 1184], device='cuda:0')
Generated mixing coefficient (lambda): 0.796645356851158
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.353622436523438
Classification loss 2 during warmup: 4.7892889976501465
Alignment loss during warmup: 4.605172634124756
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.503788471221924
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592058181762695
Contrastive Loss: tensor(6.2983, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3536, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7893, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5038, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5921, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2983, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3536, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7893, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5038, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5921, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.298309326171875 cls_loss1: 9.353622436523438 cls_loss2: 4.7892889976501465 ent_loss: 4.503788471221924 ne_loss: -4.592058181762695 align_loss: 4.605172634124756
Total loss: 24.9581241607666
[2024-05-02 19:39:54] 00061, contrastive_loss 6.29831, cls_loss1 9.35362, cls_loss2 4.78929, ent_loss 4.50379, ne_loss -4.59206, align_loss 4.60517, 
Training completed.
training step completed
 60% 62/104 [05:31<01:37,  2.31s/it]iteration number -->  62
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.5961538461538461
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0016766826923076922
learning rate from cosine_annealing_LR -->  0.0016766826923076922
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:56] 00062, lr 0.00168, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1172, 1392, 1109,  ...,  185,  821,  419], device='cuda:0')
Generated mixing coefficient (lambda): 0.7552029448787363
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.492557525634766
Classification loss 2 during warmup: 4.7436676025390625
Alignment loss during warmup: 4.605170249938965
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.514846324920654
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592228889465332
Contrastive Loss: tensor(6.3069, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.4926, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7437, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5148, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5922, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3069, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.4926, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7437, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5148, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5922, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.306931018829346 cls_loss1: 9.492557525634766 cls_loss2: 4.7436676025390625 ent_loss: 4.514846324920654 ne_loss: -4.592228889465332 align_loss: 4.605170249938965
Total loss: 25.070945739746094
[2024-05-02 19:39:56] 00062, contrastive_loss 6.30693, cls_loss1 9.49256, cls_loss2 4.74367, ent_loss 4.51485, ne_loss -4.59223, align_loss 4.60517, 
Training completed.
training step completed
 61% 63/104 [05:33<01:34,  2.31s/it]iteration number -->  63
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.6057692307692307
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0017037259615384614
learning rate from cosine_annealing_LR -->  0.0017037259615384614
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:39:58] 00063, lr 0.00170, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([751, 924, 323,  ...,  98, 169,  62], device='cuda:0')
Generated mixing coefficient (lambda): 0.22886211841076604
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.347431182861328
Classification loss 2 during warmup: 4.744492053985596
Alignment loss during warmup: 4.605170249938965
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.512420177459717
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.590672016143799
Contrastive Loss: tensor(6.3298, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3474, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7445, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5124, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5907, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3298, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3474, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7445, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5124, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5907, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.329827785491943 cls_loss1: 9.347431182861328 cls_loss2: 4.744492053985596 ent_loss: 4.512420177459717 ne_loss: -4.590672016143799 align_loss: 4.605170249938965
Total loss: 24.94866943359375
[2024-05-02 19:39:59] 00063, contrastive_loss 6.32983, cls_loss1 9.34743, cls_loss2 4.74449, ent_loss 4.51242, ne_loss -4.59067, align_loss 4.60517, 
Training completed.
training step completed
 62% 64/104 [05:36<01:32,  2.31s/it]iteration number -->  64
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.6153846153846154
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0017307692307692308
learning rate from cosine_annealing_LR -->  0.0017307692307692308
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:00] 00064, lr 0.00173, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([732, 553, 793,  ..., 745, 519, 642], device='cuda:0')
Generated mixing coefficient (lambda): 0.915490962154327
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.347320556640625
Classification loss 2 during warmup: 4.837247371673584
Alignment loss during warmup: 4.605169773101807
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.485203266143799
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592250823974609
Contrastive Loss: tensor(6.3389, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3473, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8372, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4852, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5923, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3389, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3473, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8372, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4852, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5923, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.338894367218018 cls_loss1: 9.347320556640625 cls_loss2: 4.837247371673584 ent_loss: 4.485203266143799 ne_loss: -4.592250823974609 align_loss: 4.605169773101807
Total loss: 25.021583557128906
[2024-05-02 19:40:01] 00064, contrastive_loss 6.33889, cls_loss1 9.34732, cls_loss2 4.83725, ent_loss 4.48520, ne_loss -4.59225, align_loss 4.60517, 
Training completed.
training step completed
 62% 65/104 [05:38<01:29,  2.30s/it]iteration number -->  65
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.625
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0017578125
learning rate from cosine_annealing_LR -->  0.0017578125
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:02] 00065, lr 0.00176, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1334, 1256, 1279,  ..., 1367,  945,  780], device='cuda:0')
Generated mixing coefficient (lambda): 0.70598867872455
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.32078742980957
Classification loss 2 during warmup: 4.718581199645996
Alignment loss during warmup: 4.605171203613281
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.524194240570068
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.590729713439941
Contrastive Loss: tensor(6.3313, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3208, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7186, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5242, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5907, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3313, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3208, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7186, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5242, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5907, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.331254482269287 cls_loss1: 9.32078742980957 cls_loss2: 4.718581199645996 ent_loss: 4.524194240570068 ne_loss: -4.590729713439941 align_loss: 4.605171203613281
Total loss: 24.909259796142578
[2024-05-02 19:40:03] 00065, contrastive_loss 6.33125, cls_loss1 9.32079, cls_loss2 4.71858, ent_loss 4.52419, ne_loss -4.59073, align_loss 4.60517, 
Training completed.
training step completed
 63% 66/104 [05:40<01:27,  2.30s/it]iteration number -->  66
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.6346153846153846
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0017848557692307688
learning rate from cosine_annealing_LR -->  0.0017848557692307688
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:05] 00066, lr 0.00178, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1246, 1011,  901,  ..., 1211,  228, 1428], device='cuda:0')
Generated mixing coefficient (lambda): 0.017368226871032337
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.272530555725098
Classification loss 2 during warmup: 4.820005416870117
Alignment loss during warmup: 4.605170249938965
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.477026462554932
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592323303222656
Contrastive Loss: tensor(6.3352, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2725, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8200, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4770, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5923, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3352, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2725, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8200, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4770, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5923, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.3351569175720215 cls_loss1: 9.272530555725098 cls_loss2: 4.820005416870117 ent_loss: 4.477026462554932 ne_loss: -4.592323303222656 align_loss: 4.605170249938965
Total loss: 24.917564392089844
[2024-05-02 19:40:05] 00066, contrastive_loss 6.33516, cls_loss1 9.27253, cls_loss2 4.82001, ent_loss 4.47703, ne_loss -4.59232, align_loss 4.60517, 
Training completed.
training step completed
 64% 67/104 [05:43<01:25,  2.30s/it]iteration number -->  67
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.6442307692307693
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0018118990384615383
learning rate from cosine_annealing_LR -->  0.0018118990384615383
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:07] 00067, lr 0.00181, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1266,  409, 1113,  ...,   73,  610,  652], device='cuda:0')
Generated mixing coefficient (lambda): 0.3245214107212053
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.283103942871094
Classification loss 2 during warmup: 4.728846549987793
Alignment loss during warmup: 4.605167388916016
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.523654937744141
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.59005069732666
Contrastive Loss: tensor(6.3548, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2831, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7288, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5237, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5901, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3548, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2831, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7288, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5237, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5901, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.354791641235352 cls_loss1: 9.283103942871094 cls_loss2: 4.728846549987793 ent_loss: 4.523654937744141 ne_loss: -4.59005069732666 align_loss: 4.605167388916016
Total loss: 24.905513763427734
[2024-05-02 19:40:08] 00067, contrastive_loss 6.35479, cls_loss1 9.28310, cls_loss2 4.72885, ent_loss 4.52365, ne_loss -4.59005, align_loss 4.60517, 
Training completed.
training step completed
 65% 68/104 [05:45<01:22,  2.30s/it]iteration number -->  68
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.6538461538461539
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0018389423076923075
learning rate from cosine_annealing_LR -->  0.0018389423076923075
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:09] 00068, lr 0.00184, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 234,  331,  826,  ..., 1353,  408, 1285], device='cuda:0')
Generated mixing coefficient (lambda): 0.1163705612685398
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.353584289550781
Classification loss 2 during warmup: 4.805179595947266
Alignment loss during warmup: 4.605168342590332
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.496905326843262
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5929436683654785
Contrastive Loss: tensor(6.3679, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3536, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8052, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4969, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5929, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3679, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3536, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8052, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4969, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5929, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.367932319641113 cls_loss1: 9.353584289550781 cls_loss2: 4.805179595947266 ent_loss: 4.496905326843262 ne_loss: -4.5929436683654785 align_loss: 4.605168342590332
Total loss: 25.03582763671875
[2024-05-02 19:40:10] 00068, contrastive_loss 6.36793, cls_loss1 9.35358, cls_loss2 4.80518, ent_loss 4.49691, ne_loss -4.59294, align_loss 4.60517, 
Training completed.
training step completed
 66% 69/104 [05:47<01:20,  2.31s/it]iteration number -->  69
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.6634615384615384
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0018659855769230765
learning rate from cosine_annealing_LR -->  0.0018659855769230765
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:12] 00069, lr 0.00187, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1380,  293, 1034,  ...,   58,  244,   71], device='cuda:0')
Generated mixing coefficient (lambda): 0.9790261867049342
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.301531791687012
Classification loss 2 during warmup: 4.906248569488525
Alignment loss during warmup: 4.6051716804504395
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.4732770919799805
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591068744659424
Contrastive Loss: tensor(6.3498, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3015, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.9062, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4733, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5911, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3498, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3015, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.9062, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4733, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5911, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.349758148193359 cls_loss1: 9.301531791687012 cls_loss2: 4.906248569488525 ent_loss: 4.4732770919799805 ne_loss: -4.591068744659424 align_loss: 4.6051716804504395
Total loss: 25.044918060302734
[2024-05-02 19:40:12] 00069, contrastive_loss 6.34976, cls_loss1 9.30153, cls_loss2 4.90625, ent_loss 4.47328, ne_loss -4.59107, align_loss 4.60517, 
Training completed.
training step completed
 67% 70/104 [05:50<01:18,  2.31s/it]iteration number -->  70
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.6730769230769231
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.001893028846153846
learning rate from cosine_annealing_LR -->  0.001893028846153846
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:14] 00070, lr 0.00189, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 237,  445,  557,  ...,  833, 1073,  936], device='cuda:0')
Generated mixing coefficient (lambda): 0.700951709931014
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.331022262573242
Classification loss 2 during warmup: 4.703790664672852
Alignment loss during warmup: 4.605168342590332
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.525844573974609
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.590743064880371
Contrastive Loss: tensor(6.2834, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3310, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7038, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5258, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5907, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2834, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3310, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7038, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5258, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5907, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.283406734466553 cls_loss1: 9.331022262573242 cls_loss2: 4.703790664672852 ent_loss: 4.525844573974609 ne_loss: -4.590743064880371 align_loss: 4.605168342590332
Total loss: 24.858489990234375
[2024-05-02 19:40:15] 00070, contrastive_loss 6.28341, cls_loss1 9.33102, cls_loss2 4.70379, ent_loss 4.52584, ne_loss -4.59074, align_loss 4.60517, 
Training completed.
training step completed
 68% 71/104 [05:52<01:16,  2.31s/it]iteration number -->  71
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.6826923076923077
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0019200721153846152
learning rate from cosine_annealing_LR -->  0.0019200721153846152
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:16] 00071, lr 0.00192, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 597,  532,  307,  ...,  349, 1143,  363], device='cuda:0')
Generated mixing coefficient (lambda): 0.729170391254229
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.279298782348633
Classification loss 2 during warmup: 4.700259685516357
Alignment loss during warmup: 4.605165481567383
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.527199745178223
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591811656951904
Contrastive Loss: tensor(6.4278, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2793, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7003, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5272, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5918, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4278, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2793, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7003, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5272, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5918, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.427846431732178 cls_loss1: 9.279298782348633 cls_loss2: 4.700259685516357 ent_loss: 4.527199745178223 ne_loss: -4.591811656951904 align_loss: 4.605165481567383
Total loss: 24.94795799255371
[2024-05-02 19:40:17] 00071, contrastive_loss 6.42785, cls_loss1 9.27930, cls_loss2 4.70026, ent_loss 4.52720, ne_loss -4.59181, align_loss 4.60517, 
Training completed.
training step completed
 69% 72/104 [05:54<01:13,  2.30s/it]iteration number -->  72
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.6923076923076923
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0019471153846153844
learning rate from cosine_annealing_LR -->  0.0019471153846153844
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:19] 00072, lr 0.00195, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([378, 945, 652,  ..., 959, 834, 373], device='cuda:0')
Generated mixing coefficient (lambda): 0.6950706253342843
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.382255554199219
Classification loss 2 during warmup: 4.718008518218994
Alignment loss during warmup: 4.605170249938965
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.524470329284668
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5896830558776855
Contrastive Loss: tensor(6.3240, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3823, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7180, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5245, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5897, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3240, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3823, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7180, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5245, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5897, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.323977470397949 cls_loss1: 9.382255554199219 cls_loss2: 4.718008518218994 ent_loss: 4.524470329284668 ne_loss: -4.5896830558776855 align_loss: 4.605170249938965
Total loss: 24.96419906616211
[2024-05-02 19:40:19] 00072, contrastive_loss 6.32398, cls_loss1 9.38226, cls_loss2 4.71801, ent_loss 4.52447, ne_loss -4.58968, align_loss 4.60517, 
Training completed.
training step completed
 70% 73/104 [05:56<01:11,  2.30s/it]iteration number -->  73
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.7019230769230769
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0019741586538461536
learning rate from cosine_annealing_LR -->  0.0019741586538461536
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:21] 00073, lr 0.00197, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([  49,  122, 1168,  ...,  527,  518,   87], device='cuda:0')
Generated mixing coefficient (lambda): 0.101917515904392
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.297834396362305
Classification loss 2 during warmup: 4.792886734008789
Alignment loss during warmup: 4.605169296264648
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.494840621948242
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.59160041809082
Contrastive Loss: tensor(6.2653, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2978, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7929, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4948, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5916, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2653, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2978, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7929, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4948, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5916, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.265336990356445 cls_loss1: 9.297834396362305 cls_loss2: 4.792886734008789 ent_loss: 4.494840621948242 ne_loss: -4.59160041809082 align_loss: 4.605169296264648
Total loss: 24.86446762084961
[2024-05-02 19:40:22] 00073, contrastive_loss 6.26534, cls_loss1 9.29783, cls_loss2 4.79289, ent_loss 4.49484, ne_loss -4.59160, align_loss 4.60517, 
Training completed.
training step completed
 71% 74/104 [05:59<01:09,  2.30s/it]iteration number -->  74
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.7115384615384616
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002001201923076923
learning rate from cosine_annealing_LR -->  0.002001201923076923
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:23] 00074, lr 0.00200, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 971, 1166, 1406,  ...,  564, 1058,  293], device='cuda:0')
Generated mixing coefficient (lambda): 0.8042195123870479
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.399730682373047
Classification loss 2 during warmup: 4.7653326988220215
Alignment loss during warmup: 4.6051716804504395
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.50568962097168
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591251373291016
Contrastive Loss: tensor(6.3136, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3997, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7653, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5057, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5913, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3136, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3997, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7653, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5057, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5913, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.3135986328125 cls_loss1: 9.399730682373047 cls_loss2: 4.7653326988220215 ent_loss: 4.50568962097168 ne_loss: -4.591251373291016 align_loss: 4.6051716804504395
Total loss: 24.998271942138672
[2024-05-02 19:40:24] 00074, contrastive_loss 6.31360, cls_loss1 9.39973, cls_loss2 4.76533, ent_loss 4.50569, ne_loss -4.59125, align_loss 4.60517, 
Training completed.
training step completed
 72% 75/104 [06:01<01:06,  2.31s/it]iteration number -->  75
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.7211538461538461
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002028245192307692
learning rate from cosine_annealing_LR -->  0.002028245192307692
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:26] 00075, lr 0.00203, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 396,  627, 1373,  ...,  591, 1407, 1328], device='cuda:0')
Generated mixing coefficient (lambda): 0.6368364231777892
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.320724487304688
Classification loss 2 during warmup: 4.704396724700928
Alignment loss during warmup: 4.605167865753174
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.539397716522217
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591296672821045
Contrastive Loss: tensor(6.3128, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3207, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7044, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5394, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5913, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3128, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3207, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7044, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5394, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5913, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.312811374664307 cls_loss1: 9.320724487304688 cls_loss2: 4.704396724700928 ent_loss: 4.539397716522217 ne_loss: -4.591296672821045 align_loss: 4.605167865753174
Total loss: 24.89120101928711
[2024-05-02 19:40:26] 00075, contrastive_loss 6.31281, cls_loss1 9.32072, cls_loss2 4.70440, ent_loss 4.53940, ne_loss -4.59130, align_loss 4.60517, 
Training completed.
training step completed
 73% 76/104 [06:03<01:04,  2.31s/it]iteration number -->  76
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.7307692307692307
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0020552884615384613
learning rate from cosine_annealing_LR -->  0.0020552884615384613
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:28] 00076, lr 0.00206, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1320,  563,  962,  ...,    2,  816,   44], device='cuda:0')
Generated mixing coefficient (lambda): 0.7126254766577547
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.346749305725098
Classification loss 2 during warmup: 4.729190826416016
Alignment loss during warmup: 4.605174541473389
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.528382778167725
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5903167724609375
Contrastive Loss: tensor(6.3078, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3467, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7292, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5284, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5903, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.3078, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3467, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7292, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5284, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5903, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.307819843292236 cls_loss1: 9.346749305725098 cls_loss2: 4.729190826416016 ent_loss: 4.528382778167725 ne_loss: -4.5903167724609375 align_loss: 4.605174541473389
Total loss: 24.927001953125
[2024-05-02 19:40:29] 00076, contrastive_loss 6.30782, cls_loss1 9.34675, cls_loss2 4.72919, ent_loss 4.52838, ne_loss -4.59032, align_loss 4.60517, 
Training completed.
training step completed
 74% 77/104 [06:06<01:02,  2.32s/it]iteration number -->  77
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.7403846153846154
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0020823317307692305
learning rate from cosine_annealing_LR -->  0.0020823317307692305
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:30] 00077, lr 0.00208, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 299,  727,  629,  ...,  942, 1260,  534], device='cuda:0')
Generated mixing coefficient (lambda): 0.9844950878373382
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.271059036254883
Classification loss 2 during warmup: 4.868391036987305
Alignment loss during warmup: 4.605173110961914
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.488569259643555
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592877388000488
Contrastive Loss: tensor(6.2518, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2711, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8684, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.4886, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5929, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2518, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2711, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8684, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.4886, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5929, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.251760959625244 cls_loss1: 9.271059036254883 cls_loss2: 4.868391036987305 ent_loss: 4.488569259643555 ne_loss: -4.592877388000488 align_loss: 4.605173110961914
Total loss: 24.89207649230957
[2024-05-02 19:40:31] 00077, contrastive_loss 6.25176, cls_loss1 9.27106, cls_loss2 4.86839, ent_loss 4.48857, ne_loss -4.59288, align_loss 4.60517, 
Training completed.
training step completed
 75% 78/104 [06:08<01:00,  2.31s/it]iteration number -->  78
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.75
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0021093749999999997
learning rate from cosine_annealing_LR -->  0.0021093749999999997
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:32] 00078, lr 0.00211, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 650, 1234, 1028,  ...,  529, 1169,   80], device='cuda:0')
Generated mixing coefficient (lambda): 0.8398449502687155
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.26161003112793
Classification loss 2 during warmup: 4.766005039215088
Alignment loss during warmup: 4.605169296264648
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.507560729980469
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5912394523620605
Contrastive Loss: tensor(6.2501, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2616, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7660, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5076, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5912, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2501, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2616, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7660, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5076, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5912, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.25006103515625 cls_loss1: 9.26161003112793 cls_loss2: 4.766005039215088 ent_loss: 4.507560729980469 ne_loss: -4.5912394523620605 align_loss: 4.605169296264648
Total loss: 24.799165725708008
[2024-05-02 19:40:33] 00078, contrastive_loss 6.25006, cls_loss1 9.26161, cls_loss2 4.76601, ent_loss 4.50756, ne_loss -4.59124, align_loss 4.60517, 
Training completed.
training step completed
 76% 79/104 [06:10<00:57,  2.31s/it]iteration number -->  79
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.7596153846153846
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002136418269230769
learning rate from cosine_annealing_LR -->  0.002136418269230769
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:35] 00079, lr 0.00214, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 667,   40,  707,  ...,  464,  739, 1064], device='cuda:0')
Generated mixing coefficient (lambda): 0.4701967636709477
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.26133918762207
Classification loss 2 during warmup: 4.6785993576049805
Alignment loss during warmup: 4.6051716804504395
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.551505088806152
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592042922973633
Contrastive Loss: tensor(6.4010, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2613, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6786, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5515, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5920, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.4010, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2613, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6786, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5515, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5920, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.400986671447754 cls_loss1: 9.26133918762207 cls_loss2: 4.6785993576049805 ent_loss: 4.551505088806152 ne_loss: -4.592042922973633 align_loss: 4.6051716804504395
Total loss: 24.90555763244629
[2024-05-02 19:40:35] 00079, contrastive_loss 6.40099, cls_loss1 9.26134, cls_loss2 4.67860, ent_loss 4.55151, ne_loss -4.59204, align_loss 4.60517, 
Training completed.
training step completed
 77% 80/104 [06:13<00:55,  2.30s/it]iteration number -->  80
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.7692307692307693
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0021634615384615386
learning rate from cosine_annealing_LR -->  0.0021634615384615386
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:37] 00080, lr 0.00216, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([  72, 1402,  994,  ..., 1214,  144,  587], device='cuda:0')
Generated mixing coefficient (lambda): 0.7325450256963191
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.284754753112793
Classification loss 2 during warmup: 4.7378950119018555
Alignment loss during warmup: 4.605170249938965
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.5317912101745605
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592206954956055
Contrastive Loss: tensor(6.2306, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2848, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7379, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5318, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5922, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2306, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2848, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7379, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5318, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5922, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.230587959289551 cls_loss1: 9.284754753112793 cls_loss2: 4.7378950119018555 ent_loss: 4.5317912101745605 ne_loss: -4.592206954956055 align_loss: 4.605170249938965
Total loss: 24.797992706298828
[2024-05-02 19:40:38] 00080, contrastive_loss 6.23059, cls_loss1 9.28475, cls_loss2 4.73790, ent_loss 4.53179, ne_loss -4.59221, align_loss 4.60517, 
Training completed.
training step completed
 78% 81/104 [06:15<00:52,  2.30s/it]iteration number -->  81
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.7788461538461539
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0021905048076923074
learning rate from cosine_annealing_LR -->  0.0021905048076923074
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:39] 00081, lr 0.00219, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 664,  548, 1144,  ..., 1105,  284,  101], device='cuda:0')
Generated mixing coefficient (lambda): 0.4300397948765166
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.294429779052734
Classification loss 2 during warmup: 4.66092586517334
Alignment loss during warmup: 4.6051716804504395
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.553505897521973
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592630386352539
Contrastive Loss: tensor(6.1637, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2944, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6609, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5535, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5926, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1637, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2944, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6609, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5535, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5926, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.1636643409729 cls_loss1: 9.294429779052734 cls_loss2: 4.66092586517334 ent_loss: 4.553505897521973 ne_loss: -4.592630386352539 align_loss: 4.6051716804504395
Total loss: 24.685068130493164
[2024-05-02 19:40:40] 00081, contrastive_loss 6.16366, cls_loss1 9.29443, cls_loss2 4.66093, ent_loss 4.55351, ne_loss -4.59263, align_loss 4.60517, 
Training completed.
training step completed
 79% 82/104 [06:17<00:50,  2.30s/it]iteration number -->  82
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.7884615384615384
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0022175480769230766
learning rate from cosine_annealing_LR -->  0.0022175480769230766
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:42] 00082, lr 0.00222, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1409,  491, 1039,  ...,  866, 1229,  307], device='cuda:0')
Generated mixing coefficient (lambda): 0.6727742745306516
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.293371200561523
Classification loss 2 during warmup: 4.68835973739624
Alignment loss during warmup: 4.605170726776123
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.5403852462768555
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.591565132141113
Contrastive Loss: tensor(6.2377, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2934, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6884, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5404, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5916, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2377, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2934, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6884, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5404, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5916, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.237684726715088 cls_loss1: 9.293371200561523 cls_loss2: 4.68835973739624 ent_loss: 4.5403852462768555 ne_loss: -4.591565132141113 align_loss: 4.605170726776123
Total loss: 24.773406982421875
[2024-05-02 19:40:42] 00082, contrastive_loss 6.23768, cls_loss1 9.29337, cls_loss2 4.68836, ent_loss 4.54039, ne_loss -4.59157, align_loss 4.60517, 
Training completed.
training step completed
 80% 83/104 [06:20<00:48,  2.31s/it]iteration number -->  83
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.7980769230769231
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0022445913461538462
learning rate from cosine_annealing_LR -->  0.0022445913461538462
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:44] 00083, lr 0.00224, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 520,  514, 1353,  ...,  643,  996,  228], device='cuda:0')
Generated mixing coefficient (lambda): 0.15512189273940405
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.267593383789062
Classification loss 2 during warmup: 4.782779216766357
Alignment loss during warmup: 4.605168342590332
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.518749237060547
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.593266487121582
Contrastive Loss: tensor(6.2716, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2676, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7828, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5187, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5933, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2716, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2676, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7828, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5187, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5933, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.271640777587891 cls_loss1: 9.267593383789062 cls_loss2: 4.782779216766357 ent_loss: 4.518749237060547 ne_loss: -4.593266487121582 align_loss: 4.605168342590332
Total loss: 24.852664947509766
[2024-05-02 19:40:45] 00083, contrastive_loss 6.27164, cls_loss1 9.26759, cls_loss2 4.78278, ent_loss 4.51875, ne_loss -4.59327, align_loss 4.60517, 
Training completed.
training step completed
 81% 84/104 [06:22<00:46,  2.31s/it]iteration number -->  84
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.8076923076923077
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0022716346153846155
learning rate from cosine_annealing_LR -->  0.0022716346153846155
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:46] 00084, lr 0.00227, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 509,  850,  444,  ...,  302,  334, 1203], device='cuda:0')
Generated mixing coefficient (lambda): 0.3649355634481757
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.320576667785645
Classification loss 2 during warmup: 4.684490203857422
Alignment loss during warmup: 4.6051740646362305
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.550875663757324
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.592559337615967
Contrastive Loss: tensor(6.2574, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.3206, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6845, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5509, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5926, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2574, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.3206, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6845, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5509, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5926, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.257431983947754 cls_loss1: 9.320576667785645 cls_loss2: 4.684490203857422 ent_loss: 4.550875663757324 ne_loss: -4.592559337615967 align_loss: 4.6051740646362305
Total loss: 24.82598876953125
[2024-05-02 19:40:47] 00084, contrastive_loss 6.25743, cls_loss1 9.32058, cls_loss2 4.68449, ent_loss 4.55088, ne_loss -4.59256, align_loss 4.60517, 
Training completed.
training step completed
 82% 85/104 [06:24<00:43,  2.31s/it]iteration number -->  85
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.8173076923076923
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0022986778846153843
learning rate from cosine_annealing_LR -->  0.0022986778846153843
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:49] 00085, lr 0.00230, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1273,  123,   61,  ...,  208,  482,  732], device='cuda:0')
Generated mixing coefficient (lambda): 0.4273428899864599
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.271303176879883
Classification loss 2 during warmup: 4.6837687492370605
Alignment loss during warmup: 4.605169773101807
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.554718494415283
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.593380928039551
Contrastive Loss: tensor(6.2467, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2713, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6838, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5547, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5934, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2467, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2713, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6838, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5547, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5934, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.246675968170166 cls_loss1: 9.271303176879883 cls_loss2: 4.6837687492370605 ent_loss: 4.554718494415283 ne_loss: -4.593380928039551 align_loss: 4.605169773101807
Total loss: 24.76825523376465
[2024-05-02 19:40:49] 00085, contrastive_loss 6.24668, cls_loss1 9.27130, cls_loss2 4.68377, ent_loss 4.55472, ne_loss -4.59338, align_loss 4.60517, 
Training completed.
training step completed
 83% 86/104 [06:26<00:41,  2.31s/it]iteration number -->  86
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.8269230769230769
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0023257211538461535
learning rate from cosine_annealing_LR -->  0.0023257211538461535
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:51] 00086, lr 0.00233, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1305, 1268,   39,  ...,  213,  889, 1043], device='cuda:0')
Generated mixing coefficient (lambda): 0.18681202477878797
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.26021957397461
Classification loss 2 during warmup: 4.738352298736572
Alignment loss during warmup: 4.605169773101807
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.525834083557129
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.593075752258301
Contrastive Loss: tensor(6.2912, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2602, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7384, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5258, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5931, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2912, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2602, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7384, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5258, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5931, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.291165351867676 cls_loss1: 9.26021957397461 cls_loss2: 4.738352298736572 ent_loss: 4.525834083557129 ne_loss: -4.593075752258301 align_loss: 4.605169773101807
Total loss: 24.827665328979492
[2024-05-02 19:40:52] 00086, contrastive_loss 6.29117, cls_loss1 9.26022, cls_loss2 4.73835, ent_loss 4.52583, ne_loss -4.59308, align_loss 4.60517, 
Training completed.
training step completed
 84% 87/104 [06:29<00:39,  2.30s/it]iteration number -->  87
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.8365384615384616
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002352764423076923
learning rate from cosine_annealing_LR -->  0.002352764423076923
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:53] 00087, lr 0.00235, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([   8,  704,  451,  ..., 1402, 1369, 1330], device='cuda:0')
Generated mixing coefficient (lambda): 0.07541079723374793
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.289443016052246
Classification loss 2 during warmup: 4.777853488922119
Alignment loss during warmup: 4.605172634124756
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.51556396484375
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5948028564453125
Contrastive Loss: tensor(6.1422, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2894, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7779, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5156, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5948, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1422, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2894, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7779, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5156, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5948, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.142177104949951 cls_loss1: 9.289443016052246 cls_loss2: 4.777853488922119 ent_loss: 4.51556396484375 ne_loss: -4.5948028564453125 align_loss: 4.605172634124756
Total loss: 24.735408782958984
[2024-05-02 19:40:54] 00087, contrastive_loss 6.14218, cls_loss1 9.28944, cls_loss2 4.77785, ent_loss 4.51556, ne_loss -4.59480, align_loss 4.60517, 
Training completed.
training step completed
 85% 88/104 [06:31<00:36,  2.30s/it]iteration number -->  88
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.8461538461538461
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0023798076923076924
learning rate from cosine_annealing_LR -->  0.0023798076923076924
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:55] 00088, lr 0.00238, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 757,  503, 1079,  ...,  531, 1159,   47], device='cuda:0')
Generated mixing coefficient (lambda): 0.4096777288648497
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.288171768188477
Classification loss 2 during warmup: 4.6718363761901855
Alignment loss during warmup: 4.605169296264648
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.556461811065674
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5940260887146
Contrastive Loss: tensor(6.2011, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2882, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6718, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5565, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5940, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2011, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2882, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6718, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5565, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5940, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.201137065887451 cls_loss1: 9.288171768188477 cls_loss2: 4.6718363761901855 ent_loss: 4.556461811065674 ne_loss: -4.5940260887146 align_loss: 4.605169296264648
Total loss: 24.728750228881836
[2024-05-02 19:40:56] 00088, contrastive_loss 6.20114, cls_loss1 9.28817, cls_loss2 4.67184, ent_loss 4.55646, ne_loss -4.59403, align_loss 4.60517, 
Training completed.
training step completed
 86% 89/104 [06:33<00:34,  2.30s/it]iteration number -->  89
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.8557692307692307
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002406850961538461
learning rate from cosine_annealing_LR -->  0.002406850961538461
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:40:58] 00089, lr 0.00241, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 605,  122,   71,  ..., 1006,  326,   34], device='cuda:0')
Generated mixing coefficient (lambda): 0.6829206493611906
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.283378601074219
Classification loss 2 during warmup: 4.701562404632568
Alignment loss during warmup: 4.605170726776123
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.549590110778809
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.594161033630371
Contrastive Loss: tensor(6.1897, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2834, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7016, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5496, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5942, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1897, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2834, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7016, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5496, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5942, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.1896562576293945 cls_loss1: 9.283378601074219 cls_loss2: 4.701562404632568 ent_loss: 4.549590110778809 ne_loss: -4.594161033630371 align_loss: 4.605170726776123
Total loss: 24.735198974609375
[2024-05-02 19:40:59] 00089, contrastive_loss 6.18966, cls_loss1 9.28338, cls_loss2 4.70156, ent_loss 4.54959, ne_loss -4.59416, align_loss 4.60517, 
Training completed.
training step completed
 87% 90/104 [06:36<00:32,  2.31s/it]iteration number -->  90
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.8653846153846154
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002433894230769231
learning rate from cosine_annealing_LR -->  0.002433894230769231
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:41:00] 00090, lr 0.00243, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([510, 236,  55,  ..., 801, 316,  36], device='cuda:0')
Generated mixing coefficient (lambda): 0.6530166864751794
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.245000839233398
Classification loss 2 during warmup: 4.696932792663574
Alignment loss during warmup: 4.605170249938965
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.553042888641357
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.59477424621582
Contrastive Loss: tensor(6.1121, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2450, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6969, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5530, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5948, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1121, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2450, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6969, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5530, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5948, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.112051010131836 cls_loss1: 9.245000839233398 cls_loss2: 4.696932792663574 ent_loss: 4.553042888641357 ne_loss: -4.59477424621582 align_loss: 4.605170249938965
Total loss: 24.61742401123047
[2024-05-02 19:41:01] 00090, contrastive_loss 6.11205, cls_loss1 9.24500, cls_loss2 4.69693, ent_loss 4.55304, ne_loss -4.59477, align_loss 4.60517, 
Training completed.
training step completed
 88% 91/104 [06:38<00:30,  2.31s/it]iteration number -->  91
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.875
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0024609374999999996
learning rate from cosine_annealing_LR -->  0.0024609374999999996
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:41:02] 00091, lr 0.00246, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 300,  389, 1433,  ...,   34,  260,  424], device='cuda:0')
Generated mixing coefficient (lambda): 0.9115656793040001
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.260242462158203
Classification loss 2 during warmup: 4.813239097595215
Alignment loss during warmup: 4.605167865753174
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.519159317016602
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.594457626342773
Contrastive Loss: tensor(6.1960, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2602, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8132, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5192, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5945, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1960, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2602, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8132, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5192, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5945, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.1960272789001465 cls_loss1: 9.260242462158203 cls_loss2: 4.813239097595215 ent_loss: 4.519159317016602 ne_loss: -4.594457626342773 align_loss: 4.605167865753174
Total loss: 24.79937744140625
[2024-05-02 19:41:03] 00091, contrastive_loss 6.19603, cls_loss1 9.26024, cls_loss2 4.81324, ent_loss 4.51916, ne_loss -4.59446, align_loss 4.60517, 
Training completed.
training step completed
 88% 92/104 [06:40<00:27,  2.30s/it]iteration number -->  92
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.8846153846153846
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002487980769230769
learning rate from cosine_annealing_LR -->  0.002487980769230769
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:41:05] 00092, lr 0.00249, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1410, 1221, 1414,  ...,  727,  311, 1015], device='cuda:0')
Generated mixing coefficient (lambda): 0.9122255576441457
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.276865005493164
Classification loss 2 during warmup: 4.761770725250244
Alignment loss during warmup: 4.605169296264648
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.520881652832031
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.595555305480957
Contrastive Loss: tensor(6.1560, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2769, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7618, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5209, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5956, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1560, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2769, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7618, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5209, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5956, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.15596342086792 cls_loss1: 9.276865005493164 cls_loss2: 4.761770725250244 ent_loss: 4.520881652832031 ne_loss: -4.595555305480957 align_loss: 4.605169296264648
Total loss: 24.725095748901367
[2024-05-02 19:41:05] 00092, contrastive_loss 6.15596, cls_loss1 9.27687, cls_loss2 4.76177, ent_loss 4.52088, ne_loss -4.59556, align_loss 4.60517, 
Training completed.
training step completed
 89% 93/104 [06:43<00:25,  2.30s/it]iteration number -->  93
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.8942307692307693
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002515024038461538
learning rate from cosine_annealing_LR -->  0.002515024038461538
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:41:07] 00093, lr 0.00252, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 535,  817,  516,  ..., 1313,  144,  767], device='cuda:0')
Generated mixing coefficient (lambda): 0.6283996060676098
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.269227981567383
Classification loss 2 during warmup: 4.689297676086426
Alignment loss during warmup: 4.605170726776123
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.554299354553223
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.594461441040039
Contrastive Loss: tensor(6.1332, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2692, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6893, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5543, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5945, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1332, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2692, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6893, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5543, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5945, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.133186340332031 cls_loss1: 9.269227981567383 cls_loss2: 4.689297676086426 ent_loss: 4.554299354553223 ne_loss: -4.594461441040039 align_loss: 4.605170726776123
Total loss: 24.656721115112305
[2024-05-02 19:41:08] 00093, contrastive_loss 6.13319, cls_loss1 9.26923, cls_loss2 4.68930, ent_loss 4.55430, ne_loss -4.59446, align_loss 4.60517, 
Training completed.
training step completed
 90% 94/104 [06:45<00:23,  2.30s/it]iteration number -->  94
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.9038461538461539
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0025420673076923072
learning rate from cosine_annealing_LR -->  0.0025420673076923072
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:41:09] 00094, lr 0.00254, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 398, 1119,  763,  ...,  164,  900, 1050], device='cuda:0')
Generated mixing coefficient (lambda): 0.948067647826056
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.237808227539062
Classification loss 2 during warmup: 4.780718803405762
Alignment loss during warmup: 4.605167865753174
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.519411087036133
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.595893859863281
Contrastive Loss: tensor(6.1678, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2378, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7807, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5194, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5959, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1678, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2378, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7807, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5194, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5959, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.167769908905029 cls_loss1: 9.237808227539062 cls_loss2: 4.780718803405762 ent_loss: 4.519411087036133 ne_loss: -4.595893859863281 align_loss: 4.605167865753174
Total loss: 24.714981079101562
[2024-05-02 19:41:10] 00094, contrastive_loss 6.16777, cls_loss1 9.23781, cls_loss2 4.78072, ent_loss 4.51941, ne_loss -4.59589, align_loss 4.60517, 
Training completed.
training step completed
 91% 95/104 [06:47<00:20,  2.30s/it]iteration number -->  95
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.9134615384615384
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0025691105769230765
learning rate from cosine_annealing_LR -->  0.0025691105769230765
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:41:12] 00095, lr 0.00257, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 268, 1220, 1256,  ..., 1400,  377, 1030], device='cuda:0')
Generated mixing coefficient (lambda): 0.4576707892414795
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.261192321777344
Classification loss 2 during warmup: 4.6628241539001465
Alignment loss during warmup: 4.605172634124756
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.565722942352295
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596401691436768
Contrastive Loss: tensor(6.1037, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2612, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6628, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5657, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5964, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1037, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2612, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6628, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5657, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5964, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.103687286376953 cls_loss1: 9.261192321777344 cls_loss2: 4.6628241539001465 ent_loss: 4.565722942352295 ne_loss: -4.596401691436768 align_loss: 4.605172634124756
Total loss: 24.60219955444336
[2024-05-02 19:41:12] 00095, contrastive_loss 6.10369, cls_loss1 9.26119, cls_loss2 4.66282, ent_loss 4.56572, ne_loss -4.59640, align_loss 4.60517, 
Training completed.
training step completed
 92% 96/104 [06:50<00:18,  2.30s/it]iteration number -->  96
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.9230769230769231
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002596153846153846
learning rate from cosine_annealing_LR -->  0.002596153846153846
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:41:14] 00096, lr 0.00260, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1221,  302,  910,  ..., 1311,   78,  870], device='cuda:0')
Generated mixing coefficient (lambda): 0.6829660400879685
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.252874374389648
Classification loss 2 during warmup: 4.700655937194824
Alignment loss during warmup: 4.6051740646362305
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.5550537109375
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596024036407471
Contrastive Loss: tensor(6.1352, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2529, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7007, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5551, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5960, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1352, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2529, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7007, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5551, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5960, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.1352057456970215 cls_loss1: 9.252874374389648 cls_loss2: 4.700655937194824 ent_loss: 4.5550537109375 ne_loss: -4.596024036407471 align_loss: 4.6051740646362305
Total loss: 24.652938842773438
[2024-05-02 19:41:15] 00096, contrastive_loss 6.13521, cls_loss1 9.25287, cls_loss2 4.70066, ent_loss 4.55505, ne_loss -4.59602, align_loss 4.60517, 
Training completed.
training step completed
 93% 97/104 [06:52<00:16,  2.31s/it]iteration number -->  97
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.9326923076923077
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002623197115384615
learning rate from cosine_annealing_LR -->  0.002623197115384615
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:41:16] 00097, lr 0.00262, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([  17, 1290,  338,  ..., 1221,  188,  254], device='cuda:0')
Generated mixing coefficient (lambda): 0.5992131394779854
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.273250579833984
Classification loss 2 during warmup: 4.673167705535889
Alignment loss during warmup: 4.605173110961914
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.561434268951416
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5961432456970215
Contrastive Loss: tensor(6.2289, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2733, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6732, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5614, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5961, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2289, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2733, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6732, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5614, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5961, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.228915214538574 cls_loss1: 9.273250579833984 cls_loss2: 4.673167705535889 ent_loss: 4.561434268951416 ne_loss: -4.5961432456970215 align_loss: 4.605173110961914
Total loss: 24.745798110961914
[2024-05-02 19:41:17] 00097, contrastive_loss 6.22892, cls_loss1 9.27325, cls_loss2 4.67317, ent_loss 4.56143, ne_loss -4.59614, align_loss 4.60517, 
Training completed.
training step completed
 94% 98/104 [06:54<00:13,  2.31s/it]iteration number -->  98
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.9423076923076923
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002650240384615384
learning rate from cosine_annealing_LR -->  0.002650240384615384
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:41:19] 00098, lr 0.00265, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 128,  221, 1336,  ...,   89,  792,  162], device='cuda:0')
Generated mixing coefficient (lambda): 0.8364876358261744
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.234123229980469
Classification loss 2 during warmup: 4.734137535095215
Alignment loss during warmup: 4.605169773101807
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.534264087677002
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5958452224731445
Contrastive Loss: tensor(6.2734, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2341, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7341, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5343, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5958, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.2734, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2341, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7341, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5343, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5958, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.273364067077637 cls_loss1: 9.234123229980469 cls_loss2: 4.734137535095215 ent_loss: 4.534264087677002 ne_loss: -4.5958452224731445 align_loss: 4.605169773101807
Total loss: 24.78521156311035
[2024-05-02 19:41:19] 00098, contrastive_loss 6.27336, cls_loss1 9.23412, cls_loss2 4.73414, ent_loss 4.53426, ne_loss -4.59585, align_loss 4.60517, 
Training completed.
training step completed
 95% 99/104 [06:56<00:11,  2.31s/it]iteration number -->  99
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.9519230769230769
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0026772836538461534
learning rate from cosine_annealing_LR -->  0.0026772836538461534
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:41:21] 00099, lr 0.00268, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 416,  889,  875,  ...,  716,  274, 1280], device='cuda:0')
Generated mixing coefficient (lambda): 0.12884143802376474
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.274408340454102
Classification loss 2 during warmup: 4.75235652923584
Alignment loss during warmup: 4.605170726776123
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.534214973449707
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.596092224121094
Contrastive Loss: tensor(6.1164, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2744, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7524, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5342, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5961, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1164, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2744, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7524, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5342, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5961, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.116368770599365 cls_loss1: 9.274408340454102 cls_loss2: 4.75235652923584 ent_loss: 4.534214973449707 ne_loss: -4.596092224121094 align_loss: 4.605170726776123
Total loss: 24.68642807006836
[2024-05-02 19:41:22] 00099, contrastive_loss 6.11637, cls_loss1 9.27441, cls_loss2 4.75236, ent_loss 4.53421, ne_loss -4.59609, align_loss 4.60517, 
Training completed.
training step completed
 96% 100/104 [06:59<00:09,  2.30s/it]iteration number -->  100
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.9615384615384616
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002704326923076923
learning rate from cosine_annealing_LR -->  0.002704326923076923
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:41:23] 00100, lr 0.00270, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 170, 1015,  339,  ...,  391, 1209, 1037], device='cuda:0')
Generated mixing coefficient (lambda): 0.03356349005160294
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.246278762817383
Classification loss 2 during warmup: 4.8038177490234375
Alignment loss during warmup: 4.605173110961914
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.519205570220947
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.5968427658081055
Contrastive Loss: tensor(6.1334, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2463, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.8038, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5192, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5968, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1334, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2463, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.8038, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5192, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5968, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.133427619934082 cls_loss1: 9.246278762817383 cls_loss2: 4.8038177490234375 ent_loss: 4.519205570220947 ne_loss: -4.5968427658081055 align_loss: 4.605173110961914
Total loss: 24.711061477661133
[2024-05-02 19:41:24] 00100, contrastive_loss 6.13343, cls_loss1 9.24628, cls_loss2 4.80382, ent_loss 4.51921, ne_loss -4.59684, align_loss 4.60517, 
Training completed.
training step completed
 97% 101/104 [07:01<00:06,  2.30s/it]iteration number -->  101
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.9711538461538461
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002731370192307692
learning rate from cosine_annealing_LR -->  0.002731370192307692
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:41:25] 00101, lr 0.00273, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1213, 1017,  261,  ...,  213,  858,  682], device='cuda:0')
Generated mixing coefficient (lambda): 0.4990604637557739
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.237276077270508
Classification loss 2 during warmup: 4.652256488800049
Alignment loss during warmup: 4.605169296264648
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.569210052490234
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.597706317901611
Contrastive Loss: tensor(6.1891, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2373, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6523, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5692, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5977, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1891, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2373, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6523, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5692, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5977, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.189103603363037 cls_loss1: 9.237276077270508 cls_loss2: 4.652256488800049 ent_loss: 4.569210052490234 ne_loss: -4.597706317901611 align_loss: 4.605169296264648
Total loss: 24.655309677124023
[2024-05-02 19:41:26] 00101, contrastive_loss 6.18910, cls_loss1 9.23728, cls_loss2 4.65226, ent_loss 4.56921, ne_loss -4.59771, align_loss 4.60517, 
Training completed.
training step completed
 98% 102/104 [07:03<00:04,  2.30s/it]iteration number -->  102
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.9807692307692307
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.002758413461538461
learning rate from cosine_annealing_LR -->  0.002758413461538461
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:41:28] 00102, lr 0.00276, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([ 420,  170,  474,  ...,  973, 1298, 1051], device='cuda:0')
Generated mixing coefficient (lambda): 0.08101029774593642
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.237043380737305
Classification loss 2 during warmup: 4.750677108764648
Alignment loss during warmup: 4.6051716804504395
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.530644416809082
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.597036361694336
Contrastive Loss: tensor(6.0534, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2370, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7507, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5306, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5970, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.0534, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2370, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7507, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5306, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5970, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.0534467697143555 cls_loss1: 9.237043380737305 cls_loss2: 4.750677108764648 ent_loss: 4.530644416809082 ne_loss: -4.597036361694336 align_loss: 4.6051716804504395
Total loss: 24.579946517944336
[2024-05-02 19:41:29] 00102, contrastive_loss 6.05345, cls_loss1 9.23704, cls_loss2 4.75068, ent_loss 4.53064, ne_loss -4.59704, align_loss 4.60517, 
Training completed.
training step completed
 99% 103/104 [07:06<00:02,  2.30s/it]iteration number -->  103
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  0.9903846153846154
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0027854567307692307
learning rate from cosine_annealing_LR -->  0.0027854567307692307
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:41:30] 00103, lr 0.00279, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1045,  227,  274,  ...,  233, 1432,  494], device='cuda:0')
Generated mixing coefficient (lambda): 0.5839731793418893
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.247779846191406
Classification loss 2 during warmup: 4.667135715484619
Alignment loss during warmup: 4.605165958404541
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.568373203277588
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.597588062286377
Contrastive Loss: tensor(6.0547, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2478, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.6671, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5684, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5976, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.0547, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2478, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.6671, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5684, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5976, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.054654598236084 cls_loss1: 9.247779846191406 cls_loss2: 4.667135715484619 ent_loss: 4.568373203277588 ne_loss: -4.597588062286377 align_loss: 4.605165958404541
Total loss: 24.545520782470703
[2024-05-02 19:41:31] 00103, contrastive_loss 6.05465, cls_loss1 9.24778, cls_loss2 4.66714, ent_loss 4.56837, ne_loss -4.59759, align_loss 4.60517, 
Training completed.
training step completed
100% 104/104 [07:08<00:00,  2.31s/it]iteration number -->  104
opt in adjust_learning_rate -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
opt in cosine_annealing_LR -->  Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
epoch -->  1.0
max_lr -->  0.056249999999999994
min_lr -->  0.0005625
lr (warmup) -->  0.0028125
learning rate from cosine_annealing_LR -->  0.0028125
learning rate adjusted for optimizer.param_groups[0]
[2024-05-02 19:41:32] 00104, lr 0.00281, 
learning rate logged.
learning rate adjusted
Training TCL model...
Is warmup: True
Batch size: 1440
Random indices for shuffling: tensor([1020, 1232,  212,  ..., 1332,  546,   96], device='cuda:0')
Generated mixing coefficient (lambda): 0.9489226498220377
Expanded mixing coefficient (lambda): torch.Size([1440, 1, 1, 1])
Pseudo labels shape: torch.Size([480])
Confidences shape: torch.Size([480, 1])
One-hot encoded targets shape: torch.Size([480, 100])
Softmax probabilities shapes - w_prob: torch.Size([480, 100]) q_prob1: torch.Size([480, 100]) q_prob2: torch.Size([480, 100])
Targets corrected shapes - targets_corrected1: torch.Size([480, 100]) targets_corrected2: torch.Size([480, 100])
Targets mix corrected shape: torch.Size([1440, 100])
Mixed noise targets shape: torch.Size([1440, 100])
Alignment logits shape: torch.Size([1440, 100])
Classification loss 1 during warmup: 9.256317138671875
Classification loss 2 during warmup: 4.756235122680664
Alignment loss during warmup: 4.605167865753174
Softmax probabilities shape: torch.Size([2400, 100])
Entropy loss: 4.5312347412109375
Mean probability shape: torch.Size([100])
Negative entropy loss: -4.597704887390137
Contrastive Loss: tensor(6.1038, device='cuda:0', grad_fn=<DivBackward0>)
Classification Loss 1: tensor(9.2563, device='cuda:0', grad_fn=<AddBackward0>)
Classification Loss 2: tensor(4.7562, device='cuda:0', grad_fn=<NegBackward0>)
Entropy Loss: tensor(4.5312, device='cuda:0', grad_fn=<NegBackward0>)
Negative Entropy Loss: tensor(-4.5977, device='cuda:0', grad_fn=<SumBackward0>)
Alignment Loss: tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>)
Outputs after forward pass: (tensor(6.1038, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2563, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.7562, device='cuda:0', grad_fn=<NegBackward0>), tensor(4.5312, device='cuda:0', grad_fn=<NegBackward0>), tensor(-4.5977, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.6052, device='cuda:0', grad_fn=<NegBackward0>))
Losses - contrastive_loss: 6.1037774085998535 cls_loss1: 9.256317138671875 cls_loss2: 4.756235122680664 ent_loss: 4.5312347412109375 ne_loss: -4.597704887390137 align_loss: 4.605167865753174
Total loss: 24.655025482177734
[2024-05-02 19:41:33] 00104, contrastive_loss 6.10378, cls_loss1 9.25632, cls_loss2 4.75624, ent_loss 4.53123, ne_loss -4.59770, align_loss 4.60517, 
Training completed.
training step completed
105it [07:10,  2.31s/it]             iteration number -->  105
current epoch -->  1
[2024-05-02 19:41:35] 00105, cur_epoch 1.00000, 
logger message recorded
apply_kmeans -->  True
checkpoints saved
Generating the psedo-labels
Obtained ground truth labels: 50000
Extracting features from the model...
Initialized features tensor shape: torch.Size([50000, 256])
Initialized all_labels tensor shape: torch.Size([50000])
Initialized cluster_labels tensor shape: torch.Size([50000, 100])

  0% 0/105 [00:00<?, ?it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  1% 1/105 [00:03<06:51,  3.96s/it]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  2% 2/105 [00:04<02:54,  1.69s/it]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  3% 3/105 [00:04<01:45,  1.03s/it]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  4% 4/105 [00:04<01:14,  1.36it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  5% 5/105 [00:04<00:58,  1.72it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  6% 6/105 [00:05<00:48,  2.06it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  7% 7/105 [00:05<00:41,  2.39it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  8% 8/105 [00:05<00:35,  2.70it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  9% 9/105 [00:06<00:33,  2.91it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 10% 10/105 [00:06<00:32,  2.94it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 10% 11/105 [00:06<00:30,  3.06it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 11% 12/105 [00:06<00:30,  3.04it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 12% 13/105 [00:07<00:31,  2.96it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 13% 14/105 [00:07<00:30,  3.03it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 14% 15/105 [00:07<00:27,  3.31it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 15% 16/105 [00:08<00:23,  3.74it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 16% 17/105 [00:08<00:22,  3.94it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 17% 18/105 [00:08<00:20,  4.20it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 18% 19/105 [00:08<00:20,  4.21it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 19% 20/105 [00:09<00:20,  4.13it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 20% 21/105 [00:09<00:21,  3.98it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 21% 22/105 [00:09<00:22,  3.69it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 22% 23/105 [00:09<00:21,  3.84it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 23% 24/105 [00:10<00:22,  3.62it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 24% 25/105 [00:10<00:21,  3.71it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 25% 26/105 [00:10<00:23,  3.30it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 26% 27/105 [00:11<00:23,  3.29it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 27% 28/105 [00:11<00:25,  3.02it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 28% 29/105 [00:11<00:25,  3.02it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 29% 30/105 [00:12<00:23,  3.19it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 30% 31/105 [00:12<00:21,  3.43it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 30% 32/105 [00:12<00:19,  3.69it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 31% 33/105 [00:13<00:25,  2.88it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 32% 34/105 [00:13<00:20,  3.46it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 33% 35/105 [00:13<00:18,  3.77it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 34% 36/105 [00:13<00:15,  4.41it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 35% 37/105 [00:13<00:14,  4.61it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 36% 38/105 [00:13<00:12,  5.17it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 37% 39/105 [00:14<00:12,  5.23it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 38% 40/105 [00:14<00:11,  5.91it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 39% 41/105 [00:14<00:10,  6.14it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 40% 42/105 [00:14<00:10,  5.76it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 41% 43/105 [00:14<00:10,  5.89it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 42% 44/105 [00:14<00:09,  6.55it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 44% 46/105 [00:15<00:08,  7.29it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 45% 47/105 [00:15<00:07,  7.54it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 46% 48/105 [00:15<00:07,  7.50it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 47% 49/105 [00:15<00:07,  8.00it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 48% 50/105 [00:15<00:06,  8.28it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])
Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 50% 52/105 [00:15<00:06,  8.68it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 50% 53/105 [00:15<00:05,  8.69it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 51% 54/105 [00:15<00:05,  8.66it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 52% 55/105 [00:16<00:05,  8.68it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 53% 56/105 [00:16<00:05,  8.44it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 54% 57/105 [00:16<00:05,  8.55it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 55% 58/105 [00:16<00:05,  8.48it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 56% 59/105 [00:16<00:05,  8.49it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 57% 60/105 [00:16<00:05,  8.44it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 58% 61/105 [00:16<00:05,  8.44it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 59% 62/105 [00:16<00:05,  8.55it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 60% 63/105 [00:17<00:04,  8.57it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 61% 64/105 [00:17<00:04,  8.57it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 62% 65/105 [00:17<00:04,  8.51it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 63% 66/105 [00:17<00:04,  8.54it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 64% 67/105 [00:17<00:04,  8.39it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 65% 68/105 [00:17<00:04,  8.39it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 66% 69/105 [00:17<00:04,  8.37it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 67% 70/105 [00:17<00:04,  8.42it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 68% 71/105 [00:17<00:04,  8.47it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 69% 72/105 [00:18<00:03,  8.49it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 70% 73/105 [00:18<00:03,  8.49it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 70% 74/105 [00:18<00:03,  8.46it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 71% 75/105 [00:18<00:03,  8.41it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 72% 76/105 [00:18<00:03,  8.37it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 73% 77/105 [00:18<00:03,  8.34it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 74% 78/105 [00:18<00:03,  8.39it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 75% 79/105 [00:18<00:03,  8.38it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 76% 80/105 [00:19<00:02,  8.45it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 77% 81/105 [00:19<00:02,  8.49it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 78% 82/105 [00:19<00:02,  8.43it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 79% 83/105 [00:19<00:02,  8.42it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 80% 84/105 [00:19<00:02,  8.36it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 81% 85/105 [00:19<00:02,  8.32it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 82% 86/105 [00:19<00:02,  8.35it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 83% 87/105 [00:19<00:02,  8.34it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 84% 88/105 [00:20<00:02,  8.31it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 85% 89/105 [00:20<00:01,  8.38it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 86% 90/105 [00:20<00:01,  8.39it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 87% 91/105 [00:20<00:01,  8.42it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 88% 92/105 [00:20<00:01,  8.37it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 89% 93/105 [00:20<00:01,  8.40it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 90% 94/105 [00:20<00:01,  8.33it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 90% 95/105 [00:20<00:01,  8.31it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 91% 96/105 [00:20<00:01,  8.34it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 92% 97/105 [00:21<00:00,  8.30it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 93% 98/105 [00:21<00:00,  8.38it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 94% 99/105 [00:21<00:00,  8.37it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 95% 100/105 [00:21<00:00,  8.29it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 96% 101/105 [00:21<00:00,  8.37it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 97% 102/105 [00:21<00:00,  8.37it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 98% 103/105 [00:21<00:00,  8.38it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 99% 104/105 [00:21<00:00,  8.37it/s]Processed images shape: torch.Size([80, 3, 32, 32]) labels shape: torch.Size([80])
Encoder output shape: torch.Size([80, 512])
Local cluster labels shape: torch.Size([80, 100])
Local features shape: torch.Size([80, 256])
100% 105/105 [00:21<00:00,  4.79it/s]
Updated features tensor shape after indexing: torch.Size([50000, 256])
Updated all_labels tensor shape after indexing: torch.Size([50000])
Updated cluster_labels tensor shape after indexing: torch.Size([50000, 100])
Final labels tensor shape: torch.Size([50000])
Feature extraction completed.
Extracted features and cluster labels from the memory loader.
Parameters of the config file: Namespace(save_freq=1, test_freq=50, wandb=False, project_name='noise_label', entity='zzhuang', run_name='2024_05_02_19_34_20-cifar100_90_prer18', num_workers=32, resume_epoch=0, resume_name=None, local_rank=0, seed=0, eval_metric=['nmi', 'acc', 'ari'], weight_decay=0.001, momentum=0.9, amp=False, encoder_name='bigresnet18_preact', batch_size=480, epochs=1, learning_rate=0.056249999999999994, learning_eta_min=0.01, lr_decay_gamma=0.1, lr_decay_milestone=[60, 80], step_lr=False, acc_grd_step=1, warmup_epochs=20, dist=False, num_devices=1, whole_dataset=False, pin_memory=False, dataset='cifar100', data_folder='/content/dataset/', label_file='/content/drive/MyDrive/TCL-master/models/tcl/data/sym_noise_cifar100_90.npy', img_size=32, num_cluster=100, test_resized_crop=False, resized_crop_scale=0.2, model_name='tcl', use_gaussian_blur=False, save_checkpoints=True, feat_dim=256, data_resample=False, reassign=1, sep_gmm=True, temp=0.25, scale1=None, scale2=1.0, cls_loss_weight=1.0, align_loss_weight=1.0, ent_loss_weight=1.0, ne_loss_weight=1.0, mixup_alpha=1.0, arch='simclr', syncbn=True)
Computed cluster centers after matrix multiplication: torch.Size([100, 256])
Computed confidence scores: torch.Size([50000])
Computed confidence, context assignments, and centers for noise detection.
Copied centers and context assignments to the model.
Obtained confidence, context assignments, features, and cluster labels from correct_labels function.
Copied confidence values to the tcl model.
data root ---->  /content/dataset/
dataset name ---------------->  cifar100
dataset type ---->  <class 'torchvision.datasets.cifar.CIFAR100'>
has_subfolder ---->  True
Files already downloaded and verified
dataset ---->  50000
Clean labels from the training dataset: 50000
Check if each label is clean: [False False False ... False False False]
save 0000105-0-context_assignments_hist.png to ./ckpt/2024_05_02_19_34_20-cifar100_90_prer18/save_images
Histogram of context assignments computed.
Training accuracy: tensor(0.0138, device='cuda:0')
Extracting features from the model...
Initialized features tensor shape: torch.Size([10000, 256])
Initialized all_labels tensor shape: torch.Size([10000])
Initialized cluster_labels tensor shape: torch.Size([10000, 100])

  0% 0/21 [00:00<?, ?it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

  5% 1/21 [00:02<00:42,  2.15s/it]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 10% 2/21 [00:02<00:18,  1.05it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 14% 3/21 [00:02<00:10,  1.75it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 19% 4/21 [00:02<00:06,  2.55it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 24% 5/21 [00:02<00:04,  3.41it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 29% 6/21 [00:02<00:03,  4.32it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 33% 7/21 [00:02<00:02,  5.13it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 38% 8/21 [00:02<00:02,  5.91it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 43% 9/21 [00:03<00:01,  6.52it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 48% 10/21 [00:03<00:01,  6.75it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 52% 11/21 [00:03<00:01,  7.12it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 57% 12/21 [00:03<00:01,  7.48it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 62% 13/21 [00:03<00:01,  7.78it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 67% 14/21 [00:03<00:00,  7.97it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 71% 15/21 [00:03<00:00,  8.13it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 76% 16/21 [00:03<00:00,  8.24it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 81% 17/21 [00:04<00:00,  8.33it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 86% 18/21 [00:04<00:00,  8.18it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 90% 19/21 [00:04<00:00,  8.11it/s]Processed images shape: torch.Size([480, 3, 32, 32]) labels shape: torch.Size([480])
Encoder output shape: torch.Size([480, 512])
Local cluster labels shape: torch.Size([480, 100])
Local features shape: torch.Size([480, 256])

 95% 20/21 [00:04<00:00,  8.07it/s]Processed images shape: torch.Size([400, 3, 32, 32]) labels shape: torch.Size([400])
Encoder output shape: torch.Size([400, 512])
Local cluster labels shape: torch.Size([400, 100])
Local features shape: torch.Size([400, 256])

100% 21/21 [00:04<00:00,  4.62it/s]
Updated features tensor shape after indexing: torch.Size([10000, 256])
Updated all_labels tensor shape after indexing: torch.Size([10000])
Updated cluster_labels tensor shape after indexing: torch.Size([10000, 100])
Final labels tensor shape: torch.Size([10000])
Feature extraction completed.
Extracted features, test cluster labels, and test labels.
Test accuracy: tensor(0.0143, device='cuda:0')
KNN labels predicted for the test features: tensor([68, 38, 95,  ..., 51,  8, 92], device='cuda:0')
(tensor([ 1,  2,  5,  6,  9, 10, 11, 15, 16, 17, 19, 24, 25, 27, 28, 29, 30, 31,
        32, 34, 39, 40, 41, 43, 45, 48, 50, 52, 54, 55, 59, 63, 65, 66, 67, 72,
        73, 74, 75, 78, 80, 84, 85, 86, 93, 94, 95, 96, 98], device='cuda:0'), tensor([ 265,  116,   25,  102,  120,   21,  450,  224,    1,    3,  635,    1,
         941,   47,    2,   61,    2,  219,  299,  307,  566,   54,   38,   13,
        1200,  161,  163,  127,    5,    3, 1011,  510,  285,  227,    5,   33,
           1,    5,    1,  344,    8,    4,  292,    1,    1,    1,  342,  205,
         553], device='cuda:0'))
KNN accuracy: tensor(0.1568, device='cuda:0')
Estimated noise ratio: 0.4820999801158905
Updated scale1 of tcl model: 0.4820999801158905
Noise accuracy: tensor(0.5318, device='cuda:0')
Context noise AUC: 0.5598157824882339
[2024-05-02 19:42:04] 00105, estimated_noise_ratio 0.48210, noise_accuracy 0.53176, context_noise_auc 0.55982, train_acc 0.01378, test_acc 0.01430, knn_acc 0.15680, 
Evaluated the tcl model using obtained features, confidence, cluster labels, and context assignments.
psedo labeling applied
 this function does nothing--------------------------
test performed
cuda cache emptied
current epoch after increment -->  2
105it [07:42,  4.45s/it]
